{
  "entity_to_id": {
    "--enable_v1_converter": 0,
    "--keras_model_file": 1,
    "--output_file": 2,
    "--output_file, --saved_model_dir, --keras_model_file, --enable_v1_converter": 3,
    "--saved_model_dir": 4,
    "-D WITH_QT=OFF 禁用了 Qt5 支持": 5,
    ".tflite": 6,
    "/dev": 7,
    "/opt/python": 8,
    "/sys/class/gpio目录": 9,
    "/usr/bin/python": 10,
    "0.2s的响应时间": 11,
    "100": 12,
    "128个神经元和relu激活函数": 13,
    "128核 NVIDIA Maxwell 架构的 GPU": 14,
    "14个引脚用于其他功能": 15,
    "155层网络": 16,
    "2.0.0": 17,
    "2.3.0": 18,
    "26个引脚可以用作数字输入或输出": 19,
    "3个Conv2D和2个MaxPooling2D层": 20,
    "40 针 GPIO 扩展接口": 21,
    "40个 GPIO 引脚": 22,
    "4GB 的内存": 23,
    "5个节点": 24,
    "5个节点的输出层": 25,
    "6": 26,
    "64位四核的 ARM Cortex-A57 CPU": 27,
    "800万像素、感光芯片为索尼 IMX219，静态图片分辨率为3280 × 2464、支持1080p30, 720p60以及640 × 480p90视频录像": 28,
    "<html> <body> <h4>TFJS example<hr/></h4> <div id=\"micro-out-div\">TensorFlow.js Test</div> <script src=\"./index.js\"> </script> </body> </html>": 29,
    "<script>标签": 30,
    "@tensorflow/tfjs": 31,
    "@tensorflow/tfjs-vis": 32,
    "AC8265": 33,
    "API 函数": 34,
    "ARM Cortex-A57 CPU、NVIDIA Maxwell GPU": 35,
    "Adam": 36,
    "Adam优化器": 37,
    "Android": 38,
    "Android Studio": 39,
    "Android 应用": 40,
    "Android 开发人员": 41,
    "Android 开发者": 42,
    "Android 神经网络 API": 43,
    "Android 设备": 44,
    "Android、iOS 和 Linux": 45,
    "Android环境部署": 46,
    "Android部署": 47,
    "B02版本有两路": 48,
    "BCM编号方式": 49,
    "C": 50,
    "C#": 51,
    "C++": 52,
    "C++ 和 Python 提供的 TensorFlow Lite API": 53,
    "CNN": 54,
    "CSI 摄像头": 55,
    "CSI 相机接口": 56,
    "CSI摄像头": 57,
    "CSI端口": 58,
    "ClassifierFloatMobileNet类": 59,
    "Classifier类": 60,
    "CocoaPods": 61,
    "CocoaPods for Swift or Objective-C": 62,
    "Conv2D": 63,
    "Conv2D, MaxPooling2D, Flatten, Dense": 64,
    "Core API": 65,
    "DOCTYPE声明、html、head和body元素": 66,
    "DeepLearning.js": 67,
    "Dense": 68,
    "Display Port 接口": 69,
    "Dropout": 70,
    "Etcher": 71,
    "Face Recognition": 72,
    "FlatBuffers": 73,
    "FlatBuffers 格式": 74,
    "GPIO": 75,
    "GPIO.cleanup()方法": 76,
    "GPIO.setmod()": 77,
    "GPIO.setup()": 78,
    "GPIO.setup()方法": 79,
    "GPIO.setup(18, GPIO.IN, pull_up_down=GPIO.PUD_UP)": 80,
    "GPIO21": 81,
    "GPIO库": 82,
    "GPIO引脚": 83,
    "GPIO引脚编号模式的一种": 84,
    "GPIO端口文件": 85,
    "GPU": 86,
    "GPU 委托": 87,
    "GPU代理": 88,
    "GStreamer": 89,
    "GStreamer管道参数": 90,
    "GlobalAveragePooling2D": 91,
    "Google": 92,
    "Google Arts & Culture": 93,
    "Google Assistant": 94,
    "Google Edge TPU Coral Dev Board": 95,
    "Google Photos": 96,
    "Google Photos、Google Assistant、Uber、Airbnb、网易、爱奇艺、WPS 等": 97,
    "Google 内部用于计算机视觉场景的解决方案": 98,
    "Gradle 同步": 99,
    "HDMI 接口": 100,
    "HIGH电平": 101,
    "HTML 文件": 102,
    "HTML文件、JS文件和配置文件": 103,
    "HTML文档": 104,
    "HTML文档类型声明、html标签、head标签和body标签": 105,
    "Haar 特征分类器": 106,
    "Haar 特征的 cascade 分类器": 107,
    "Haar特征的cascade分类器": 108,
    "Hello AI World": 109,
    "IMAGE_WIDTH, IMAGE_HEIGHT, testData, testxs, labels, preds": 110,
    "ImageDataGenerator": 111,
    "ImageProcessor": 112,
    "Intel Neural Compute Stick 2": 113,
    "IoT领域": 114,
    "JCenter Bintray 的 TFLite AAR": 115,
    "Java": 116,
    "Java 或 C++ API 执行 TensorFlow Lite 推理": 117,
    "JavaScript": 118,
    "JavaScript 语言版本的扩展": 119,
    "JetBot": 120,
    "JetPack SDK": 121,
    "Jetson": 122,
    "Jetson Nano": 123,
    "Jetson Nano 开发板": 124,
    "Jetson Nano开发板": 125,
    "Jetson 项目社区": 126,
    "Jupyter Lab": 127,
    "Jupyter Notebook": 128,
    "Jupyter Notebook 的全面升级": 129,
    "Jupyter Notebook、文本编辑器、终端": 130,
    "Jupyter Notebook、文本编辑器、终端以及各种个性化组件": 131,
    "Jupyter lab": 132,
    "JupyterLab": 133,
    "Keras H5": 134,
    "Keras Model 和 SavedModel": 135,
    "Keras模型": 136,
    "Keras的模型定义方式": 137,
    "LED": 138,
    "LED灯": 139,
    "LED的控制引脚": 140,
    "LOW电平": 141,
    "Layers API": 142,
    "Layers API和Core API": 143,
    "Linux": 144,
    "Linux 中所有设备文件或特殊文件的存储位置": 145,
    "Linux 开发环境": 146,
    "Live Caption功能": 147,
    "MCU": 148,
    "MIPI": 149,
    "MIPI CSI-2": 150,
    "MIPI 联盟发起的为移动应用处理器制定的开放标准": 151,
    "MNIST数据集": 152,
    "MNIST项目": 153,
    "MaxPooling2D": 154,
    "Micro USB 接口": 155,
    "Micro-USB 接口": 156,
    "MnistData": 157,
    "MobileNet": 158,
    "MobileNet V2": 159,
    "MobileNet V2模型": 160,
    "MobileNet 模型": 161,
    "MobileNetV2": 162,
    "MobileNets_v2": 163,
    "NVIDIA Jetson Nano": 164,
    "NVIDIA Jetson Nano 开发板": 165,
    "NVIDIA Jetson 开发者专区": 166,
    "NVIDIA Jetson 论坛": 167,
    "Node.js 或浏览器环境": 168,
    "Object C": 169,
    "OpenCV": 170,
    "OpenCV 编译": 171,
    "OperatorCode": 172,
    "Parcel, WebPack 或 Rollup": 173,
    "PoseNet模型": 174,
    "PoseNet示例应用程序": 175,
    "Python": 176,
    "Python 2.7": 177,
    "Python API 和命令行工具": 178,
    "Python GPIO引脚": 179,
    "Python 代码片段": 180,
    "Python 官网": 181,
    "Python 源码包": 182,
    "Python包管理": 183,
    "Python开发环境": 184,
    "Python模块编译失败": 185,
    "Quantization-aware training": 186,
    "RPI.GPIO库": 187,
    "RandomFlip": 188,
    "RandomRotation": 189,
    "Raspberry Camera V2": 190,
    "Raspberry Pi 4": 191,
    "Raspberry Pi、Arducam 等常见的相机模块": 192,
    "SD Memory Card Formatter": 193,
    "SavedModel": 194,
    "SavedModel 和 Keras Sequential": 195,
    "SavedModel 和 Keras Sequential 两种模型导出方法和格式": 196,
    "Sequential模型": 197,
    "SparseCategoricalCrossentropy": 198,
    "Swift": 199,
    "Swift 和 Objective-C 编写的原生 iOS 库": 200,
    "TF Mobile": 201,
    "TFLite": 202,
    "TFLite API": 203,
    "TFLite model": 204,
    "TFLite 文件格式": 205,
    "TFLite 模型": 206,
    "TFLite 模型文件格式": 207,
    "TFLite 模型文件格式，支持内存高效读取": 208,
    "TFLite 模型转换过程": 209,
    "TFLite 解释器": 210,
    "TFLite 解释执行器": 211,
    "TFLite 转换器": 212,
    "TFLiteConverter.from_concrete_functions()": 213,
    "TFLiteConverter.from_keras_model()": 214,
    "TFLiteConverter.from_saved_model()": 215,
    "TFLite模型": 216,
    "TFLite模型转换器": 217,
    "TFLite解释器": 218,
    "TFLite解释器和GPU代理": 219,
    "TFLite转换": 220,
    "TFMini": 221,
    "TensorBoard": 222,
    "TensorFLow-vis": 223,
    "TensorFlow": 224,
    "TensorFlow 2.x 模型": 225,
    "TensorFlow GPU 版本": 226,
    "TensorFlow Hub": 227,
    "TensorFlow Hub 上可搜索到的模型": 228,
    "TensorFlow Lite": 229,
    "TensorFlow Lite AAR": 230,
    "TensorFlow Lite API": 231,
    "TensorFlow Lite 工作流程": 232,
    "TensorFlow Lite 开发工作流程": 233,
    "TensorFlow Lite 推理": 234,
    "TensorFlow Lite 支持库": 235,
    "TensorFlow Lite 模型": 236,
    "TensorFlow Lite 模型文件": 237,
    "TensorFlow Lite 算子库": 238,
    "TensorFlow Lite 解释器": 239,
    "TensorFlow Lite 解释器(Interpreter)": 240,
    "TensorFlow Lite 解释器、TensorFlow Lite 转换器、算子库、硬件加速代理": 241,
    "TensorFlow Lite 转换器": 242,
    "TensorFlow Lite 转换器(Converter)": 243,
    "TensorFlow Lite 转换器命令行工具": 244,
    "TensorFlow Lite推理过程": 245,
    "TensorFlow Lite支持库": 246,
    "TensorFlow Lite模型": 247,
    "TensorFlow Lite的工作流程": 248,
    "TensorFlow Lite的简称": 249,
    "TensorFlow Lite解释器": 250,
    "TensorFlow 安装": 251,
    "TensorFlow 模型": 252,
    "TensorFlow 模型导出": 253,
    "TensorFlow 版本": 254,
    "TensorFlow 的 JavaScript 版本": 255,
    "TensorFlow.js": 256,
    "TensorFlow.js 模块": 257,
    "TensorFlow.js 进行浏览器可视化的一组实用工具库": 258,
    "TensorFlow.js中的中心数据单元，是一维或多维数组": 259,
    "TensorFlow团队开发的产品": 260,
    "Tensorflow.js": 261,
    "Tensorflow.js模型": 262,
    "Text，Image，Video 和 Publishers 等类别": 263,
    "Total params: 2,626,821, Trainable params: 368,837, Non-trainable params: 2,257,984": 264,
    "USB": 265,
    "USB摄像头": 266,
    "VNC": 267,
    "VNC Viewer": 268,
    "VNC 服务": 269,
    "WPS": 270,
    "Web Server for Chrome": 271,
    "Web 开发新手": 272,
    "XML 文件，该文件中会描述人体各个部位的 Haar 特征值": 273,
    "aaptOptions": 274,
    "activation": 275,
    "add_event_detect()函数": 276,
    "allocate_tensors()": 277,
    "android/start 目录下的项目模板和 finish 目录下的完整代码": 278,
    "base_model": 279,
    "batchSize": 280,
    "batchSize设置为512": 281,
    "batch_size": 282,
    "batch_size用于设置每批图像数量": 283,
    "build.gradle": 284,
    "buildscript": 285,
    "callback": 286,
    "callbacks设置为fitCallbacks": 287,
    "categorical_crossentropy": 288,
    "cdn.jsdelivr.net": 289,
    "class_names": 290,
    "compare_faces": 291,
    "console.log语句": 292,
    "conv2d, dropout, global_average_pooling2d, dense": 293,
    "convertToTensor函数": 294,
    "converter.convert": 295,
    "converter.optimizations=[tf.lite.Optimize.DEFAULT]": 296,
    "createModel()": 297,
    "daisy, dandelion, roses, sunflowers, tulips": 298,
    "data.js": 299,
    "data.js文件": 300,
    "dense层": 301,
    "dependencies": 302,
    "dispose": 303,
    "dispose和tf.tidy两种内存管理方法": 304,
    "doPrediction函数": 305,
    "epochs": 306,
    "epochs设置为10": 307,
    "face_cascade.detectMultiScale": 308,
    "face_distance": 309,
    "face_encodings": 310,
    "face_landmarks": 311,
    "face_locations": 312,
    "filters": 313,
    "fine_tune_at": 314,
    "flatten层": 315,
    "flow_from_directory": 316,
    "flower_classification": 317,
    "from_saved_model(), from_keras_model(), from_concrete_functions()": 318,
    "functional模型": 319,
    "get_input_details()": 320,
    "get_output_details()": 321,
    "get_tensor()": 322,
    "hog模型在CPU上运行更快但不太准确，cnn模型更准确但需要GPU加速": 323,
    "hub.KerasLayer": 324,
    "iOS": 325,
    "iOS 开发人员": 326,
    "iOS 开发者": 327,
    "import * as tf from '@tensorflow/tfjs' console.log(tf.version.tfjs) const shape = [2, 3]; // 2 rows, 3 columns const a = tf.tensor": 328,
    "import语句": 329,
    "include_top=False": 330,
    "index": 331,
    "index.html": 332,
    "index.html、index.js和data.js文件": 333,
    "index.js": 334,
    "inputShape": 335,
    "inputShape, kernelSize, filters, strides, activation, kernelInitializer": 336,
    "inputShape为[1]，units为1，useBias为true": 337,
    "interpreter": 338,
    "interpreter.get_tensor()": 339,
    "invoke()": 340,
    "jtop 命令": 341,
    "jupyter": 342,
    "jupyter_notebook_config.py": 343,
    "kernelInitializer": 344,
    "kernelSize": 345,
    "labels.txt": 346,
    "lambda x, y: (normalization_layer(x), y)": 347,
    "layers.Flatten()": 348,
    "load_image_file": 349,
    "loss='categorical_crossentropy'": 350,
    "make install命令": 351,
    "make命令": 352,
    "metrics=['accuracy']": 353,
    "microSD 卡": 354,
    "microSD 卡插槽": 355,
    "minNeighbors": 356,
    "mnist项目": 357,
    "mobilenetv2_1.00_224": 358,
    "model": 359,
    "model.compile": 360,
    "model.fit": 361,
    "model.fit()": 362,
    "model.predict()": 363,
    "model.tflite": 364,
    "model.tflite和label.txt": 365,
    "model.trainable = False": 366,
    "modelSummary": 367,
    "model和保存目录路径": 368,
    "nextTestBatch": 369,
    "nextTrainBatch": 370,
    "nextTrainBatch（batchSize）和 nextTestBatch（batchSize）方法": 371,
    "noCompress \"tflite\"": 372,
    "normalization_layer": 373,
    "np.argmax()": 374,
    "num_classes个输出": 375,
    "number_of_times_to_upsample": 376,
    "optimizer='sgd', loss='mean_squared_error'": 377,
    "optimizer=tf.keras.optimizers.Adam(1e-5)": 378,
    "org.tensorflow 库": 379,
    "output_details": 380,
    "package.json": 381,
    "pip": 382,
    "pip install": 383,
    "pip3": 384,
    "poolSize": 385,
    "poolSize, strides": 386,
    "post-training quantization": 387,
    "predict()": 388,
    "print(\"button pressed!\")": 389,
    "python": 390,
    "recognizeImage方法": 391,
    "repositories和dependencies": 392,
    "resize原始图像到模型输入大小": 393,
    "saved_model_dir": 394,
    "schema.fbs": 395,
    "sequential模型": 396,
    "sequential模型和functional模型": 397,
    "set_tensor()": 398,
    "showAccuracy函数": 399,
    "showConfusion函数": 400,
    "shuffle": 401,
    "shuffle用于控制批处理顺序": 402,
    "shuffle设置为true": 403,
    "strides": 404,
    "target_size参数": 405,
    "target_size用于设置图像大小": 406,
    "tensor()": 407,
    "tensorflow-lite": 408,
    "tensor创建代码": 409,
    "tf.keras model": 410,
    "tf.keras.Sequential": 411,
    "tf.keras.layers.Dense": 412,
    "tf.keras.losses.SparseCategoricalCrossentropy": 413,
    "tf.keras.models.Sequential": 414,
    "tf.keras.optimizers.Adam": 415,
    "tf.linspace()": 416,
    "tf.lite.TFLiteConverter": 417,
    "tf.lite.TFLiteConverter.from_keras_model": 418,
    "tf.lite.TFLiteConverter.from_saved_model": 419,
    "tf.losses.meanSquaredError": 420,
    "tf.model()": 421,
    "tf.nn.softmax()": 422,
    "tf.saved_model.save": 423,
    "tf.sequential": 424,
    "tf.sequential()": 425,
    "tf.sequential()和tf.model()两种创建模型的方式": 426,
    "tf.tensor": 427,
    "tf.tensor2d": 428,
    "tf.tidy": 429,
    "tf.tidy()": 430,
    "tf.train.adam()": 431,
    "tfjs-examples/mnist": 432,
    "tfliteModel和tfliteOptions": 433,
    "tflite_convert": 434,
    "tflite_convert --keras_model_file=/tmp/mobilenet_keras_model.h5 --output_file=/tmp/mobilenet.tflite": 435,
    "tflite_convert --saved_model_dir=/tmp/mobilenet_saved_model --output_file=/tmp/mobilenet.tflite": 436,
    "tfvis.render.scatterplot": 437,
    "time.sleep()方法": 438,
    "train_generator": 439,
    "train_generator, steps_per_epoch=len(train_generator), epochs=epochs, validation_data=val_generator, validation_steps=len(val_generator)": 440,
    "ui.js": 441,
    "units=1": 442,
    "units=1, input_shape=[1]": 443,
    "units=16, activation='relu'": 444,
    "units为1，useBias为true": 445,
    "useBias": 446,
    "val_ds": 447,
    "val_ds.map": 448,
    "val_generator": 449,
    "validationData设置为[testXs, testYs]": 450,
    "vino": 451,
    "wait_for_edge()函数": 452,
    "wait_for_edge()函数和add_event_detect()函数": 453,
    "x—images, y—labels": 454,
    "yarn": 455,
    "一个使用数据流图进行数值计算的开源软件库": 456,
    "一个多媒体框架，用于后端处理任务，如格式修改、显示驱动程序协调和数据处理": 457,
    "一个强大、简单、易上手的人脸识别开源项目": 458,
    "一个端到端的机器学习开源框架": 459,
    "一个缩减版的 TensorFlow，简化了算子集，也缩小了运行库": 460,
    "一个轻量、快速、兼容度高的专门针对移动式应用场景的深度学习工具": 461,
    "一切皆文件，对文件的读写操作相当于外设输入输出": 462,
    "一种数据结构，包含了在解决一个特定问题时，训练得到的机器学习网络的逻辑和知识": 463,
    "一种有效的物品检测方法": 464,
    "一种特定的矩阵用来呈现算法性能的可视化效果": 465,
    "一系列的函数，这些函数以一个或多个张量作为输入，并输出另一个张量": 466,
    "一系列的计算节点": 467,
    "一组帮助开发者在移动设备、嵌入式设备和IoT设备上运行TensorFlow模型的工具": 468,
    "一组数字引脚，可用于将树莓派连接到其他电子设备": 469,
    "上拉电阻": 470,
    "下载 OpenCV": 471,
    "下载和访问mnist数据集": 472,
    "下载解压": 473,
    "不支持 CUDA": 474,
    "不支持 CUDA 且版本是固定搭配的": 475,
    "不改变基础模型的各项参数变量": 476,
    "不清除内部函数的返回值": 477,
    "不需要原始模型构建代码": 478,
    "不需要原始模型构建代码就可以运行": 479,
    "不需要原有模型中最后的神经网络层": 480,
    "不需要序列化或可以创造自己的序列化方法": 481,
    "不需要改变模型": 482,
    "与 NVIDIA TensorRT 一起使用预训练模型进行实时图像分类和对象检测": 483,
    "与 TensorFlow 一起安装": 484,
    "与 TensorFlow 的核心算子库略有不同": 485,
    "与树莓派结合可以将项目与现实世界轻松的联系起来": 486,
    "串联在LED和电源之间限制电流保护LED和GPIO引脚": 487,
    "为LED提供电源": 488,
    "主要应用于游戏场景": 489,
    "主要应用于游戏场景，是为了高性能场景创建的序列化库": 490,
    "了解模型效率、调试超参数": 491,
    "二维卷积层": 492,
    "二进制文件小、延迟低、支持设备端机器学习推断": 493,
    "二进制文件很小": 494,
    "二进制文件的大小约为 1 MB（针对 32 位 ARM build）": 495,
    "交互式、富文本、多语言支持": 496,
    "交叉熵损失函数": 497,
    "人脸检测": 498,
    "人脸检测、检测面部特征点、给脸部编码、从编码中找出人的名字": 499,
    "人脸识别": 500,
    "仅用于开发的程序包": 501,
    "仅适用于卷积神经网络的一个子集": 502,
    "从 Keras Model 转换模型": 503,
    "从 MNIST 数据集中随机批量提取 MNIST 图像": 504,
    "从 SavedModel 转换模型": 505,
    "从GitHub克隆项目代码，进入项目目录，使用yarn构建和运行": 506,
    "从头自己编译": 507,
    "从指定目录生成训练数据批次": 508,
    "从指定目录生成验证数据批次": 509,
    "从测试集中返回一批图像及其标签": 510,
    "从训练集中返回一批随机图像及其标签": 511,
    "从该层开始进行微调的参数": 512,
    "以最小精度下降来训练网络": 513,
    "优化分类任务": 514,
    "优化模型": 515,
    "优化模型大小和性能": 516,
    "优化的 FlatBuffer 格式": 517,
    "优化的 FlatBuffer 格式，以 .tflite 为文件扩展名": 518,
    "优化的算子库": 519,
    "会使用 GPU 加速模型的运算，提高运算效率": 520,
    "会导致多次输出": 521,
    "位于其引脚排针上，可以通过跳线线连接到其他电路板或设备": 522,
    "低功耗、支持多种深度学习框架": 523,
    "作为优化器": 524,
    "作为优化器用于模型训练": 525,
    "作为判断训练结果的参数": 526,
    "作为损失函数": 527,
    "作为损失函数用于模型训练": 528,
    "作为模型优化算法": 529,
    "作为网页的入口文件": 530,
    "作为迁移学习的基础模型": 531,
    "使权重和激活值的 Post training 更简单": 532,
    "使模型的预测不受图像顺序的影响": 533,
    "使用 Python API 进行转换": 534,
    "使用 SavedModel 格式存储": 535,
    "使用 TFLite 模型转换器转换成 TFLite 文件格式(FlatBuffers 格式)": 536,
    "使用 TensorFlow Lite 解释器在设备端运行模型": 537,
    "使用 nano 或者 vi 编辑工具修改软件源的配置文件/etc/apt/sources.list": 538,
    "使用3×3的卷积核，并在输出上使用Relu激活函数": 539,
    "使用GPU来加速数学运算": 540,
    "使用TFLiteConverter转换模型": 541,
    "使用TensorFlow.js实现手势识别": 542,
    "使用VGG、ResNet等模型": 543,
    "使用tf.model() API创建非闭环的计算图": 544,
    "使用tf.saved_model.save保存模型": 545,
    "使用tf.saved_model.save函数": 546,
    "使用低学习率编译模型": 547,
    "使用低学习率重新编译模型": 548,
    "使用层构建模型": 549,
    "使用已编译好的库": 550,
    "使用模型从未见过的测试数据评估分类器准确性": 551,
    "使用模型优化工具包缩减模型大小并提高效率": 552,
    "使用测试数据集评估模型": 553,
    "使用深度可分离的卷积": 554,
    "使用自己的 TensorFlow 模型、在线查找模型或从 TensorFlow 预训练模型中选择": 555,
    "使用计算机视觉相关的模型，包括实时摄像机的使用": 556,
    "使用语言就是 Javascript，前端工程师不需要学习其他后端语言，降低入门门槛": 557,
    "使用预训练量化进行模型转换": 558,
    "使这些专用功能适应新数据集，而不是覆盖通用学习": 559,
    "保存为model.tflite文件": 560,
    "保存完整的TensorFlow程序包括权重和计算": 561,
    "保存完整的TensorFlow程序，包括权重值和计算": 562,
    "保存训练数据的类别标签": 563,
    "保持了很多通用性": 564,
    "保留原来大规模训练的优势": 565,
    "修改build.gradle配置": 566,
    "借助低级运算构建模型": 567,
    "做了移动设备相关的优化": 568,
    "允许解释器在设备的 GPU 上运行适当的运算符": 569,
    "全能 IDE": 570,
    "全能IDE": 571,
    "全连接 (Full Connected) 层": 572,
    "全连接(Full Connected)层": 573,
    "全连接网络": 574,
    "共享的内存缓冲区": 575,
    "关闭LED灯": 576,
    "具有shape属性定义数组形状": 577,
    "兼容度高": 578,
    "内存只有几十KB": 579,
    "内存回收问题突出": 580,
    "内存高效": 581,
    "内置的算子": 582,
    "写一段简单的测试代码": 583,
    "写入镜像": 584,
    "冻结前100层": 585,
    "冻结预训练模型并更新分类器的权重": 586,
    "准备训练集和验证集": 587,
    "准确度": 588,
    "减少内存碎片化": 589,
    "减少服务器的运算，提高服务器资源利用，增强客户端响应运算结果的速度": 590,
    "出门问问智能音箱": 591,
    "分类到5类": 592,
    "分类结果的概率": 593,
    "创建0~1之间平均分配的100个值": 594,
    "创建TFLite转换器实例": 595,
    "创建Tensor实例的主要构造函数": 596,
    "创建、训练和导出自定义 TensorFlow Lite 模型": 597,
    "创建包含多个 Dense 层的模型": 598,
    "创建管道，绑定视频流，逐帧提取和显示": 599,
    "创建解释器、分配张量": 600,
    "创新奇智": 601,
    "创新奇智智能读码机": 602,
    "创新奇智智能质检一体机": 603,
    "初始化TensorFlow Lite解释器": 604,
    "利用 ARM 的 NEON 指令集做了大量的优化": 605,
    "利用 SIMD 指令功能": 606,
    "利用在同一域中的较大数据集上训练的模型所学习的特征": 607,
    "前几层学习非常简单和通用的功能，这些功能可以推广到几乎所有类型的图像": 608,
    "剪刀石头布识别项目": 609,
    "功耗非常低": 610,
    "功能强大且易于使用": 611,
    "功能强大的编程语言，易于使用，易于阅读和编写": 612,
    "功能强大，交互式、富文本，还有丰富的插件、主题修改、多语言支持": 613,
    "加载 @tensorflow/tfjs 和 @tensorflow/tfjs-vis 的代码": 614,
    "加载 @tensorflow/tfjs 和 @tensorflow/tfjs-vis 的脚本": 615,
    "加载和运行TFLite模型": 616,
    "加载数据并准备进行训练, 定义模型结构, 训练模型并监视其性能, 评估模型": 617,
    "加载数据，定义模型，训练循环并指定UI元素": 618,
    "加载模型": 619,
    "加载模型、分配张量、设置输入张量、执行推理、读取输出张量": 620,
    "加载模型、转换数据、运行模型推理、解释输出": 621,
    "加载面孔照片": 622,
    "加速模型推理": 623,
    "动态显示训练的过程": 624,
    "包含JavaScript代码用于网页功能实现": 625,
    "包含MNIST数据集相关数据": 626,
    "包含完整的TensorFlow程序": 627,
    "千兆以太网端口": 628,
    "升级 Python 版本": 629,
    "单一芯片的小型计算机": 630,
    "单个图像的维度为[28,28,1]": 631,
    "卷积图像分类模型": 632,
    "卷积完成后应用于数据的激活函数": 633,
    "卷积层": 634,
    "卷积层与全连接层": 635,
    "卷积层输入": 636,
    "取消冻结模型的顶层": 637,
    "受限于GPU内存的大小": 638,
    "只提供了基本的转化功能": 639,
    "可从 github 下载源码": 640,
    "可以从同一网络上的另一台计算机控制 Jetson Nano 开发板": 641,
    "可以使用树莓派摄像头，IMX219模组800万像素": 642,
    "可以利用手机上的加速器，比如 GPU 或者 DSP": 643,
    "可以更换国内源如阿里、清华等": 644,
    "可以添加--no-cache-dir参数": 645,
    "可以直接使用cv2.videocapture打开": 646,
    "可以直接在浏览器中运行，无需进行安装，也无需借助后端运行": 647,
    "可以调用不同的硬件加速器比如GPU进行执行": 648,
    "可以通过软件编程进行控制": 649,
    "可以配置为输入或输出": 650,
    "可以配置图像加载的细节": 651,
    "可在手机、嵌入式 Linux 设备和微控制器等很多不同类型的硬件上运行": 652,
    "可能值得使用构建工具进行探索": 653,
    "可能存在过度拟合": 654,
    "可能导致模型过拟合": 655,
    "可视化模型训练的过程和结果": 656,
    "可视化模型预测结果和原始数据": 657,
    "可配置为输入或输出": 658,
    "同步项目依赖": 659,
    "启动Jupyter lab": 660,
    "启动图标": 661,
    "启用默认优化": 662,
    "命令行 TensorFlow Lite 转换器命令行工具": 663,
    "命令行与 Python API": 664,
    "命令行工具": 665,
    "命令行工具和 Python API": 666,
    "四引脚按键": 667,
    "四脚按键开关": 668,
    "回调函数": 669,
    "图像分类、物体检测、分割和语音处理等应用程序中并行运行多个神经网络": 670,
    "图像分类任务": 671,
    "图像和视频处理": 672,
    "图像识别": 673,
    "图像识别、文本处理、语音识别、AR 效果等": 674,
    "图像识别模型，可用于迁移学习": 675,
    "图像预处理": 676,
    "在 Android 与 iOS 平台上使用": 677,
    "在 Android 项目中使用 TensorFlow Lite": 678,
    "在 HTML 中直接引用 TensorFlow.js 发布的 NPM 包中已经打包安装好的 JavaScript 代码": 679,
    "在 MCU 上甚至可以小于 100KB": 680,
    "在 Node 环境进行运算的速度目前与 Python 速度不相上下": 681,
    "在 TensorFlow Lite 中使用": 682,
    "在18号引脚处设置上拉电阻": 683,
    "在不同设备上使用硬件加速": 684,
    "在图像中检测面部": 685,
    "在大规模数据处理上不如Python高效": 686,
    "在小型数据集上训练模型": 687,
    "在把模型嵌入到实际的二进制文件中时兼容性存在较大挑战": 688,
    "在有 GPU 加速的手机上运行 MobileNet 图像分类，模型运行速度可以提高": 689,
    "在每一个训练周期显示训练情况": 690,
    "在浏览器上开发模型": 691,
    "在浏览器中加载": 692,
    "在浏览器中训练模型": 693,
    "在浏览器环境中实现深度学习的功能": 694,
    "在生产环境中不需要": 695,
    "在移动端（mobile）、嵌入式（embeded）和物联网（IoT）设备上运行 TensorFlow 模型": 696,
    "在移动设备、嵌入式设备和 IoT 设备上运行 TensorFlow 模型": 697,
    "在移动设备上运行 TensorFlow": 698,
    "在移动设备和嵌入式设备上运行TensorFlow模型": 699,
    "在给定设备上实现性能、模型大小和准确性的理想平衡": 700,
    "在训练过程中不能用于训练": 701,
    "在设备端运行TFLite模型": 702,
    "在资源限制严重的移动和嵌入式设备上执行": 703,
    "在边缘设备上运行 TensorFlow 模型推理": 704,
    "在边缘设备上运行 TensorFlow 模型推理的官方框架": 705,
    "基于 TF Mobile 的经验": 706,
    "基于 WebGL 加速的开放源代码 JavaScript 机器学习库": 707,
    "基于一个流线型的架构，使用深度可分离的卷积": 708,
    "基于流线型架构的轻量级深层神经网络": 709,
    "基于浏览器运行已训练的模型": 710,
    "基于现有模型构建 Interpreter": 711,
    "基于现有的模型进行继续训练": 712,
    "基础模型的各项参数变量不会被新的训练修改数据": 713,
    "增加一个事件的检测函数": 714,
    "处理简单的数据": 715,
    "多个张量": 716,
    "多个连续层": 717,
    "大大降低了移动端及IoT设备端的深度学习技术门槛": 718,
    "大量易于学习的教程": 719,
    "头信息": 720,
    "如果仅使用支持常见图像分类模型（InceptionV3 和 MobileNet）所需的运算符，二进制文件的大小不到 300 KB": 721,
    "如果在预先训练的模型上添加一个随机初始化的分类器并尝试联合训练所有层，则梯度更新的幅度将太大，并且预训练模型将忘记它学到的东西": 722,
    "子图": 723,
    "子图、算子库和共享的内存缓冲区": 724,
    "子图本身的输入和输出": 725,
    "存储图像和标签数据": 726,
    "存储已安装软件包的名称和版本": 727,
    "存储模型权重": 728,
    "存放Python相关程序模块": 729,
    "存放训练好的模型供开发人员复用": 730,
    "学习空间不变的变换": 731,
    "安卓应用只需 1 兆左右的运行环境": 732,
    "安装 OpenCV": 733,
    "安装 Python 包依赖项": 734,
    "安装 TensorFlow": 735,
    "安装 python": 736,
    "安装Package": 737,
    "安装Python": 738,
    "安装依赖": 739,
    "安装依赖项": 740,
    "安装和升级 pip3": 741,
    "安装所需的系统包": 742,
    "完全基于 JavaScript 从头开发、训练和部署模型": 743,
    "完成分类": 744,
    "官方已经停止维护": 745,
    "官方推荐使用的无线网卡": 746,
    "官方集成到Python的工具": 747,
    "定义的神经元网络层与层之间的关系较为随意": 748,
    "定义需要监视的指标": 749,
    "定位图像中的人脸位置": 750,
    "实例化对象，调用add方法添加输入层和输出层": 751,
    "实例化预训练模型作为基础模型": 752,
    "实时识别照相机所拍摄的花卉": 753,
    "实现 headless 远程桌面访问 Jetson Nano": 754,
    "实现了一组优化的算子内核": 755,
    "实现人体姿势估计": 756,
    "实现花卉识别 app": 757,
    "实现识别花卉模型": 758,
    "室内避开障碍物": 759,
    "对 SIMD 指令功能特别有益": 760,
    "对val_ds中的每个元素应用lambda函数进行归一化处理": 761,
    "对图像数据进行归一化处理": 762,
    "对图像进行多少次上采样以查找人脸": 763,
    "对手写数字的图像进行分类": 764,
    "对数据降维": 765,
    "对模型的权重产生更一致且变化较小的渐变更新": 766,
    "对现有 CPU 平台的支持": 767,
    "对训练图像随机变换的方法来人为引入样本多样性": 768,
    "对随机目标函数执行一阶梯度优化的算法": 769,
    "导致每个时期的梯度更新数量较少": 770,
    "将 Keras 模型保存为 SavedModel 格式": 771,
    "将 NPM 模块转换为在线可以引用的免费服务": 772,
    "将 TensorFlow 模型格式转换为 TensorFlow Lite 的 Python API": 773,
    "将 TensorFlow 模型转换为 TFLite 专用格式": 774,
    "将 TensorFlow 模型转换为 TFLite 专用格式，通过解释器在设备端运行": 775,
    "将 TensorFlow 模型转换为 TFLite 格式": 776,
    "将 TensorFlow 模型转换为方便解释器使用的格式": 777,
    "将Keras模型转换为TFLite模型": 778,
    "将Python相关程序模块拷贝到/opt/python目录": 779,
    "将SavedModel转换为TFLite兼容格式": 780,
    "将SavedModel转换为TFLite模型": 781,
    "将SavedModel转换为TensorFlow Lite兼容格式": 782,
    "将TensorFlow模型转换为TFLite文件格式(FlatBuffers格式)": 783,
    "将TensorFlow模型转换为轻量级格式": 784,
    "将maven源google()和jcenter()替换为国内镜像": 785,
    "将三维张量展开到1维": 786,
    "将三维张量展开到1维以便传入Dense层": 787,
    "将人脸编码列表与候选编码进行比较": 788,
    "将前一层的输出平铺到一个向量中": 789,
    "将图片分类到1000类": 790,
    "将彩色图像转换为灰度图像，检测人脸，绘制边界矩形": 791,
    "将所有图像加载到一个模型需要的特定的大小": 792,
    "将数据转换为TensorFlow可读的张量格式": 793,
    "将模型加载到内存中": 794,
    "将模型嵌入到二进制文件中，这样就可以在设备上运行和部署模型": 795,
    "将模型文件和标签文件拷贝到assets目录": 796,
    "将模型显示在浏览器中": 797,
    "将特征转换为每个图像对应一个1280元素向量": 798,
    "将网络的每一层简单的叠在一起": 799,
    "将自定义模型转换为 TensorFlow Lite 格式": 800,
    "将输入数据转换成模型接收的形式或排布": 801,
    "将输入的 TensorFlow 模型生成 TensorFlow Lite 模型": 802,
    "将镜像写入 microSD 卡": 803,
    "将需要的层按顺序写在一个列表里，然后将列表作为sequential()函数的输入": 804,
    "嵌入式开发平台": 805,
    "工业物联智能设备的开发": 806,
    "已经训练好的分类器，其中包括面部，眼睛，微笑等": 807,
    "常开触点": 808,
    "常开触点和常闭触点": 809,
    "常见的移动/嵌入式平台": 810,
    "常闭触点": 811,
    "应用于输入数据的滑动窗口大小": 812,
    "应用于输入数据的滤波器窗口数量": 813,
    "底层 Core API 或最高级的 Layers API": 814,
    "廉价且周边设备多": 815,
    "延迟一秒钟": 816,
    "延迟低、二进制文件小": 817,
    "延迟较低": 818,
    "建议使用脚本代码": 819,
    "建议最小采用 64 GB UHS-1 卡": 820,
    "开关去抖": 821,
    "开关抖动": 822,
    "开发依赖": 823,
    "开发板": 824,
    "开发者套件": 825,
    "开源项目": 826,
    "开箱即用的开发库，开发者无需花精力去编写基础复杂的数学问题": 827,
    "引入优化以减小二进制文件的大小和提高性能": 828,
    "引用 Model 的内存缓冲区的一片区域，提高内存效率": 829,
    "张量": 830,
    "张量(Tensor)": 831,
    "张量形状是(image_height, image_width, color_channels)": 832,
    "归一化操作": 833,
    "当压力撤销时电路恢复原始状态": 834,
    "当压力施压时电路接通": 835,
    "形状为[null, 10]的张量": 836,
    "形状为[null, 28, 28, 1]的张量": 837,
    "形状是(224,224,3)": 838,
    "影响准确性": 839,
    "影响磁盘和内存占用以及运行效率": 840,
    "微调": 841,
    "微调少量顶层而不是整个 MobileNet 模型": 842,
    "微调结果": 843,
    "微调过程": 844,
    "微调预训练模型的顶层权重": 845,
    "必须与正在使用的 JetPack 版本一致": 846,
    "必须在开机前先装上去，系统才能识别": 847,
    "快速": 848,
    "快速启动": 849,
    "快速的启动并运行一组深度学习推理演示": 850,
    "忽略由于开关抖动引起的小于": 851,
    "恢复训练": 852,
    "手写数字识别": 853,
    "打乱数据顺序，创建特征向量和标签向量，转换为张量格式，进行归一化操作": 854,
    "打开现有 Android Studio 项目": 855,
    "打开项目图标": 856,
    "执行一个函数并清除所有创建的中间张量，释放它们的GPU内存": 857,
    "执行推理": 858,
    "执行推理并获取类别概率": 859,
    "执行最终的分类": 860,
    "执行模型推理": 861,
    "执行模型文件在输入数据上定义的运算符，输出推理结果": 862,
    "执行模型转换过程": 863,
    "批次大小": 864,
    "指定引脚编号系统": 865,
    "损失函数": 866,
    "接受 TFLite 模型": 867,
    "控制GPIO引脚": 868,
    "控制LED灯的亮暗": 869,
    "控制外部硬件设备": 870,
    "推理过程": 871,
    "推理速度提高了30%": 872,
    "描述构建和运行示例所需的依赖项": 873,
    "提供了一个简单的 API，用于从 Java、Swift、Objective-C、C++ 和 Python 运行 TensorFlow Lite 模型": 874,
    "提供了一些转换工具压缩模型，进行算子融合并生成代码": 875,
    "提供了各种库和工具，使编程更加方便": 876,
    "提供了大量方便的工具，例如权重初始化，模型序列化，训练监测，可迁移性和安全检查": 877,
    "提供低级的机器学习构建模块和高级的类似Keras的API": 878,
    "提供多种语言的 API": 879,
    "提供已经训练好且经过充分认证的模型": 880,
    "提供预训练模型用于图像分类、对象检测、姿势估计、文本恶意检测等": 881,
    "提供预训练模型，供开发者复用": 882,
    "提升移动设备上的性能": 883,
    "提问或分享项目": 884,
    "提高性能的方法是训练预训练模型的顶层的权重以及刚添加的分类器的训练": 885,
    "摄像头接口": 886,
    "支持 GPU 硬件加速": 887,
    "支持像素缩放和数据增强": 888,
    "支持多种编程语言包括Python、C++、Java、Swift和Javascript": 889,
    "支持多种视觉任务": 890,
    "支持大规模的模型训练和各种环境的部署": 891,
    "支持将文件映射到内存中直接读取和解释，不需要额外解析": 892,
    "支持微控制器(MCU)": 893,
    "支持微控制器(MCU)，模型大小可小至20 KB": 894,
    "支持服务器和移动端的部署": 895,
    "支持算子优化、量化等模型优化": 896,
    "支持算子优化和编译优化": 897,
    "支持自定义输入形状": 898,
    "支持设备端机器学习推断": 899,
    "支持设置目标图像大小和批次大小": 900,
    "支持量化": 901,
    "支持预装驱动程序的 RPi 相机，并且可以很容易地用作即插即用外围设备，不需要安装驱动程序": 902,
    "效果和服务器端十分接近": 903,
    "教育": 904,
    "数字引脚": 905,
    "数据图像的采集、模型的训练、参数的调整、模型文件生成、网页端部署": 906,
    "数据规范化和转换为张量类型": 907,
    "数据转换": 908,
    "数据转换、执行推理、解释输出": 909,
    "数据集": 910,
    "数据预处理": 911,
    "文字处理": 912,
    "易于设置和使用，并且与许多流行的配件兼容": 913,
    "是阻塞函数，会阻塞程序执行直到检测到一个边沿": 914,
    "智能质检一体机、智能读码机等工业物联智能设备": 915,
    "更新可视化元素": 916,
    "更新系统需要 root 权限": 917,
    "更谨慎地控制内存何时回收": 918,
    "更适合于边缘设备部署": 919,
    "最后的神经网络层": 920,
    "最大池化层": 921,
    "有一个名字": 922,
    "有两种模式：5W（低功耗模式；可以使用 USB 口供电）和 10W（必须使用 Power Jack 外接5V 电源供电）": 923,
    "有两种模式：BCM编号模式和物理引脚Broad编号模式": 924,
    "有助于避免因错误的样本而改向错误的方向": 925,
    "有经验的开发者": 926,
    "服装厂质检": 927,
    "未安装相应的依赖包": 928,
    "未满足的对等依赖 seedrandom@~": 929,
    "机器学习和计算机视觉应用，如物体检测、人脸识别、图像分割等视觉任务": 930,
    "机身只有 Ethernet 有线网络，不包括无线网卡": 931,
    "构建Tensorflow.js模型来识别手写数字": 932,
    "构建和运行mnist代码": 933,
    "构建和运行机器学习模型": 934,
    "构建小型移动机器人、人脸签到打卡、口罩识别、智能门锁、智能音箱等复杂 AI 系统": 935,
    "构建工具": 936,
    "构建神经网络": 937,
    "构建神经网络的全连接层": 938,
    "构成检测目标的相邻矩形的最小个数": 939,
    "查看开发板系统信息": 940,
    "标准算子": 941,
    "树莓派": 942,
    "树莓派系统": 943,
    "树莓派系统升级": 944,
    "树莓派编程": 945,
    "树莓派通用输入/输出接口（GPIO）": 946,
    "核心板可拆的设计": 947,
    "核心板的大小只有70 x 45 mm": 948,
    "核心运行时": 949,
    "格式化 microSD 卡": 950,
    "检测人脸": 951,
    "检测关键身体部位的位置": 952,
    "检测面部特征点": 953,
    "模型": 954,
    "模型优化": 955,
    "模型优化工具包": 956,
    "模型可以跟 Python 等其他语言模型进行互转": 957,
    "模型复杂度": 958,
    "模型大小": 959,
    "模型大小显著减小": 960,
    "模型推理": 961,
    "模型文件和标签文件": 962,
    "模型精度提高到98%": 963,
    "模型精度达到98%": 964,
    "模型编译": 965,
    "模型训练": 966,
    "比 CPU 执行更快的浮点矩阵运算": 967,
    "池化层": 968,
    "汽车油耗（MPG）": 969,
    "汽车的功率（Horsepower）": 970,
    "没有操作系统": 971,
    "注重实时性，内存高效": 972,
    "流入模型第一层的数据的形状": 973,
    "测试Python开发环境并查看当前的Python版本": 974,
    "测试的软件包、webpack或Babel": 975,
    "测试集": 976,
    "浏览器可以很好可视化机器训练过程，同时浏览器可调用设备的摄像头、麦克风等增加机器学习的应用场景": 977,
    "混淆矩阵": 978,
    "清华 Raspbian 软件仓库镜像": 979,
    "清华源": 980,
    "清理GPIO引脚的设置": 981,
    "清除张量或变量并释放其GPU内存": 982,
    "清除所有创建的中间张量并释放它们的GPU内存": 983,
    "滑动卷积滤波器窗口的大小": 984,
    "滑动窗口的步长": 985,
    "激活→设置为输出状态→写入1使PIN处于高电压点亮LED": 986,
    "灵活的架构可以将模型部署到桌面、服务器或移动设备中的 CPU 或 GPU 上": 987,
    "点亮LED灯": 988,
    "爱奇艺": 989,
    "版本变化后 API 函数会改变": 990,
    "版本变化后API函数会改变": 991,
    "瓶颈层": 992,
    "生成 HDF5 文件的绝对路径目录": 993,
    "生成SavedModel": 994,
    "生成一个批次一个批次的图片，以生成器的形式给模型训练": 995,
    "生成一个批次的图片，以生成器的形式给模型训练": 996,
    "生成批次的图片数据用于模型训练": 997,
    "生成配置文件": 998,
    "用apply()方法将上一层的输出作为本层的输入": 999,
    "用于 5V 电源输入": 1000,
    "用于 flower_classification 项目": 1001,
    "用于工具的配置中心": 1002,
    "用于构建 TensorFlow 模型": 1003,
    "用于监督学习，表明多个类别是否有混淆": 1004,
    "用于训练模型": 1005,
    "用于预测": 1006,
    "用作启动设备和主存储器": 1007,
    "用户在自己的工作台中使用 TensorFlow API 构造 TensorFlow 模型": 1008,
    "用来转换 SavedModel 格式模型": 1009,
    "用来转换 concrete functions": 1010,
    "用来转换 tf.keras 模型": 1011,
    "用来连接 DP 屏幕": 1012,
    "电信号从低电平到高电平或从高电平到低电平状态的改变": 1013,
    "电阻": 1014,
    "登录 Jetson Nano": 1015,
    "目前有130个左右": 1016,
    "直接串联3.3V电源会产生过大电流可能损坏LED": 1017,
    "直接在 Objective-C 代码中使用 C API": 1018,
    "直接部署或用于迁移学习": 1019,
    "直流桶式插座": 1020,
    "相比 Protocol Buffer 有更高的性能和更小的大小": 1021,
    "硬件加速代理(Hardware accelerator delegate)": 1022,
    "确认 CUDA 已经被正常安装": 1023,
    "神经元权重计算中的偏置量": 1024,
    "离线语音识别": 1025,
    "科沃斯扫地机器人": 1026,
    "移动应用中的OCR处理": 1027,
    "移动行业处理器接口（MIPI）的相机串行接口（CSI）端口": 1028,
    "移动设备": 1029,
    "端侧机器学习": 1030,
    "端侧语音识别": 1031,
    "第一个卷积层": 1032,
    "第二、三卷积层": 1033,
    "简化图像预处理": 1034,
    "简化图像预处理和模型输出处理": 1035,
    "简单的线性回归的实验": 1036,
    "算子优化": 1037,
    "算子实现": 1038,
    "算子库": 1039,
    "算子库(Op kernels)": 1040,
    "算子索引": 1041,
    "算子融合、常数折叠和无用代码删除": 1042,
    "类型: bool. (default False) Enables the converter and flags used in TF 1.x instead of TF 2.x": 1043,
    "类型: string. Full path of the output file": 1044,
    "类型: string. Full path to the Keras H5 model file": 1045,
    "类型: string. Full path to the SavedModel directory": 1046,
    "线性堆叠layers的模型": 1047,
    "绘制混淆矩阵": 1048,
    "给脸部编码": 1049,
    "继承了 TFMini 和内部其他类似项目的很多优秀工作": 1050,
    "编程语言": 1051,
    "编译 OpenCV": 1052,
    "编译Python模块": 1053,
    "编译模型": 1054,
    "编译模型，损失函数使用类别交叉熵": 1055,
    "编译模型，设置优化器和损失函数": 1056,
    "缩减模型大小并提高效率，同时最大限度地降低对准确率的影响": 1057,
    "网易": 1058,
    "网络环境较差时可以考虑更换源": 1059,
    "能够利用各种硬件加速": 1060,
    "能够执行完整全面的反向传播": 1061,
    "脚本标签": 1062,
    "自定制算子": 1063,
    "花卉数据集中的图片": 1064,
    "花卉识别": 1065,
    "花卉识别 app": 1066,
    "花卉识别模型": 1067,
    "获取一些非常有意思的项目": 1068,
    "获取图像数据、创建位图、调用estimateSinglePose函数、绘制骨架": 1069,
    "获取张量的指针": 1070,
    "获取张量的数据": 1071,
    "获取数组中最大值的索引": 1072,
    "获取更多的 Jetson 平台信息": 1073,
    "衡量所有预测中正确预测的百分比": 1074,
    "观测开关去抖效果": 1075,
    "视频中的AR效果": 1076,
    "解决JavaScript内存回收问题": 1077,
    "解决跨域问题": 1078,
    "解释器、转换器、算子库、硬件加速代理": 1079,
    "解释输出": 1080,
    "计算已知人脸和未知人脸特征向量的距离": 1081,
    "计算并显示每个类别的准确度": 1082,
    "计算机视觉应用": 1083,
    "计算节点": 1084,
    "计算节点的输入和输出": 1085,
    "计算预测结果的softmax概率": 1086,
    "让硬件厂商可以扩展支持这样的接口": 1087,
    "让输入输出映射到0-1之间，保证后期更有效地训练": 1088,
    "训练分类器查看数千个图像及其标签": 1089,
    "训练后量化": 1090,
    "训练好的模型": 1091,
    "训练好的模型文件和标签文件": 1092,
    "训练数据和测试数据": 1093,
    "训练时从数据集中的不同类中随机选出的图像数量": 1094,
    "训练曲线": 1095,
    "训练期间需要更多的内存": 1096,
    "训练模型": 1097,
    "训练模型并记录训练和验证准确性/损失": 1098,
    "训练集": 1099,
    "训练集和验证集的值": 1100,
    "训练顶层分类器并将预先训练的模型设置为不可训练之后才应尝试": 1101,
    "训练预训练模型的顶层权重": 1102,
    "记录训练日志": 1103,
    "记录训练过程中的指标和计算直方图": 1104,
    "设置 OpenCV 的内容、位置和方式": 1105,
    "设置 model.trainable = False 参数后，训练期间将不更新预训练网络的权重": 1106,
    "设置18号引脚为输入模式并启用上拉电阻": 1107,
    "设置GPIO引脚的工作模式": 1108,
    "设置model.trainable = False": 1109,
    "设置为32，表示一次采样32条训练数据": 1110,
    "设置为50，表示遍历所有样本50次": 1111,
    "设置为true，表示打乱数据集": 1112,
    "设置前100层为不可训练": 1113,
    "设置受限于GPU内存大小": 1114,
    "设置在训练中基础模型的各项参数变量不会被新的训练修改数据": 1115,
    "设置系统引脚作为输入或输出": 1116,
    "设置访问密码": 1117,
    "设置输入张量值": 1118,
    "访问密码可以为空": 1119,
    "评估模型对新数据的泛化情况": 1120,
    "识别图像里的空间模式，例如线条和物体局部": 1121,
    "识别手写数字": 1122,
    "识别花卉图片": 1123,
    "识别输入图像": 1124,
    "语音识别": 1125,
    "请注意版本": 1126,
    "读取传感器数据，控制 LED 等外部设备": 1127,
    "读取输出张量值": 1128,
    "调整数据集形状": 1129,
    "调用CSI摄像头和USB摄像头": 1130,
    "调用model.fit方法进行训练": 1131,
    "调用不同的硬件加速器比如 GPU 进行执行": 1132,
    "调用解释器的方式：try (Interpreter interpreter = new Interpreter(tensorflow_lite_model_file)) { interpreter.run(input, output); }": 1133,
    "负极连接到GND": 1134,
    "资源有限可能导致内存溢出": 1135,
    "资源有限，可能导致内存溢出": 1136,
    "跨平台运行，包括 Android、iOS 以及基于 Linux 的 IoT 设备和微控制器": 1137,
    "身份验证": 1138,
    "转换 SavedModel 格式模型": 1139,
    "转换 concrete functions": 1140,
    "转换 tf.keras 模型": 1141,
    "转换、运行 TensorFlow 模型所需的所有工具": 1142,
    "转换为TensorFlow Lite模型": 1143,
    "转换数据": 1144,
    "转换模型": 1145,
    "软件源的配置文件/etc/apt/sources.list": 1146,
    "软链接": 1147,
    "轻量化": 1148,
    "轻量级": 1149,
    "轻量级、快速启动、内存高效": 1150,
    "输入层": 1151,
    "输入层和输出层": 1152,
    "输入输出用到的 Tensor 索引": 1153,
    "输入（Xs）和标签（Ys）": 1154,
    "输出为一个数字（油耗）": 1155,
    "输出宽度和高度会收缩": 1156,
    "输出层": 1157,
    "输出是一个三维的张量，其形状描述了 (height, width, channels)": 1158,
    "输出模式": 1159,
    "输出电压约为3.3V": 1160,
    "输出的通道数量取决于声明层时的 filters 参数": 1161,
    "输出的通道数量取决于声明层时的filters参数": 1162,
    "输出通道数为32": 1163,
    "输出通道数为64": 1164,
    "边做边学的理想工具": 1165,
    "边缘": 1166,
    "边缘操作": 1167,
    "边缘计算": 1168,
    "边缘计算设备": 1169,
    "迁移学习": 1170,
    "运行 TensorFlow Lite 模型": 1171,
    "运行Sync Gradle": 1172,
    "运行功率仅为 5 瓦": 1173,
    "运行各种深度学习模型": 1174,
    "运行已有的 Python 版 TensorFlow 模型": 1175,
    "运行服务监听的IP地址、端口、notebooks内核的目录、是否打开浏览器": 1176,
    "运行模型推理": 1177,
    "返回图像中每张人脸的128维人脸编码": 1178,
    "返回张量数据的副本": 1179,
    "返回的是数据的副本而非引用": 1180,
    "进行 VNC 连接": 1181,
    "进行内存清理工作，防止内存泄露": 1182,
    "进行大量的张量操作时使用可能会很麻烦": 1183,
    "连接其他电子设备": 1184,
    "连接到具有一个隐藏单元的dense层": 1185,
    "连接显示器、键盘和鼠标或通过 SSH 或 VNC 服务远程访问": 1186,
    "适应性低阶矩估计": 1187,
    "适用于多个平台": 1188,
    "适用于多个平台，提供了一个简单的 API": 1189,
    "选择模型": 1190,
    "选择模型、转换模型、部署到设备、优化模型": 1191,
    "逐步加载单个数据集的图像": 1192,
    "通过 pip 安装": 1193,
    "通过 script 标签引入 index.js": 1194,
    "通过GPIO控制点亮": 1195,
    "通过利用 WebGL 在 GPU 上执行计算大幅提高了速度": 1196,
    "通过命令行转换模型": 1197,
    "通过脚本标签（script tags）或从 yarn（或者 NPM）安装并使用 Parcel，WebPack 或 Rollup 等工具构建工程": 1198,
    "通过计算每个滑动窗口的最大值来缩减卷积结果的大小": 1199,
    "通过限流电阻串联到GPIO21": 1200,
    "郁金香(tulips)、玫瑰(roses)、浦公英(dandelion)、向日葵(sunflowers)、雏菊(daisy)": 1201,
    "部署TensorFlow Lite模型": 1202,
    "部署到设备": 1203,
    "部署在 Raspberry Pi 和基于 Arm64 的主板的嵌入式 Linux 系统上": 1204,
    "配置项目依赖": 1205,
    "释放张量的GPU内存": 1206,
    "量化": 1207,
    "针对移动端及IoT设备端的深度学习技术": 1208,
    "错误的连接和编程可能会导致设备损坏或故障": 1209,
    "防止Android在生成应用程序二进制文件时压缩TensorFlow Lite模型文件": 1210,
    "防止应用程序中的内存泄漏": 1211,
    "降低卷积层对位置的敏感": 1212,
    "降低存储器访问成本": 1213,
    "降低权重的精确表示": 1214,
    "降低权重的精确表示，并且可选的降低存储和计算的激活值": 1215,
    "随机初始化模型权重的方法": 1216,
    "随机打乱数据集": 1217,
    "随着层越来越高，这些功能越来越多地针对训练模型的数据集": 1218,
    "需先训练顶层分类器并设置预训练模型为不可训练": 1219,
    "需要使用GStreamer读取视频流": 1220,
    "需要使用能够在开发者套件的 Micro-USB 接口处提供 5V⎓2A 的高品质电源": 1221,
    "需要大约两个半小时": 1222,
    "需要安装依赖项": 1223,
    "需要成功配置好 CUDA": 1224,
    "需要更多灵活性和控制时使用": 1225,
    "需要确认版本": 1226,
    "需要配置proxy或使用国内镜像": 1227,
    "面包板、杜邦线公对母、LED灯、330欧姆电阻": 1228,
    "面向有兴趣学习 AI 和构建有趣应用程序的创客、学生和爱好者": 1229,
    "页面的基本结构，包含div标签、UI元素和JavaScript代码": 1230,
    "项目的清单文件": 1231,
    "预加载了ImageNet训练权重的深度学习模型": 1232,
    "预处理模型输入和后处理模型输出": 1233,
    "预处理输入图像": 1234,
    "预捕获图像宽度、高度，窗口显示宽度、高度，捕获帧率，旋转图像设置": 1235,
    "预测": 1236,
    "预测汽车油耗效率": 1237,
    "预测汽车油耗（MPG）": 1238,
    "预测汽车的油耗效率 MPG": 1239,
    "预训练模型和全连接的分类器": 1240,
    "首次打开项目时": 1241,
    "验证损失高于训练损失": 1242,
    "验证集": 1243,
    "高性能": 1244,
    "高性能场景创建的序列化库": 1245,
    "高阶 tf.keras.* API（Keras 模型）或低阶 tf.* API（用于生成具体函数）": 1246,
    "默认分类到1000类": 1247,
    "默认安装 JetPack 安装了对应的 OpenCV": 1248
  },
  "id_to_entity": {
    "0": "--enable_v1_converter",
    "1": "--keras_model_file",
    "2": "--output_file",
    "3": "--output_file, --saved_model_dir, --keras_model_file, --enable_v1_converter",
    "4": "--saved_model_dir",
    "5": "-D WITH_QT=OFF 禁用了 Qt5 支持",
    "6": ".tflite",
    "7": "/dev",
    "8": "/opt/python",
    "9": "/sys/class/gpio目录",
    "10": "/usr/bin/python",
    "11": "0.2s的响应时间",
    "12": "100",
    "13": "128个神经元和relu激活函数",
    "14": "128核 NVIDIA Maxwell 架构的 GPU",
    "15": "14个引脚用于其他功能",
    "16": "155层网络",
    "17": "2.0.0",
    "18": "2.3.0",
    "19": "26个引脚可以用作数字输入或输出",
    "20": "3个Conv2D和2个MaxPooling2D层",
    "21": "40 针 GPIO 扩展接口",
    "22": "40个 GPIO 引脚",
    "23": "4GB 的内存",
    "24": "5个节点",
    "25": "5个节点的输出层",
    "26": "6",
    "27": "64位四核的 ARM Cortex-A57 CPU",
    "28": "800万像素、感光芯片为索尼 IMX219，静态图片分辨率为3280 × 2464、支持1080p30, 720p60以及640 × 480p90视频录像",
    "29": "<html> <body> <h4>TFJS example<hr/></h4> <div id=\"micro-out-div\">TensorFlow.js Test</div> <script src=\"./index.js\"> </script> </body> </html>",
    "30": "<script>标签",
    "31": "@tensorflow/tfjs",
    "32": "@tensorflow/tfjs-vis",
    "33": "AC8265",
    "34": "API 函数",
    "35": "ARM Cortex-A57 CPU、NVIDIA Maxwell GPU",
    "36": "Adam",
    "37": "Adam优化器",
    "38": "Android",
    "39": "Android Studio",
    "40": "Android 应用",
    "41": "Android 开发人员",
    "42": "Android 开发者",
    "43": "Android 神经网络 API",
    "44": "Android 设备",
    "45": "Android、iOS 和 Linux",
    "46": "Android环境部署",
    "47": "Android部署",
    "48": "B02版本有两路",
    "49": "BCM编号方式",
    "50": "C",
    "51": "C#",
    "52": "C++",
    "53": "C++ 和 Python 提供的 TensorFlow Lite API",
    "54": "CNN",
    "55": "CSI 摄像头",
    "56": "CSI 相机接口",
    "57": "CSI摄像头",
    "58": "CSI端口",
    "59": "ClassifierFloatMobileNet类",
    "60": "Classifier类",
    "61": "CocoaPods",
    "62": "CocoaPods for Swift or Objective-C",
    "63": "Conv2D",
    "64": "Conv2D, MaxPooling2D, Flatten, Dense",
    "65": "Core API",
    "66": "DOCTYPE声明、html、head和body元素",
    "67": "DeepLearning.js",
    "68": "Dense",
    "69": "Display Port 接口",
    "70": "Dropout",
    "71": "Etcher",
    "72": "Face Recognition",
    "73": "FlatBuffers",
    "74": "FlatBuffers 格式",
    "75": "GPIO",
    "76": "GPIO.cleanup()方法",
    "77": "GPIO.setmod()",
    "78": "GPIO.setup()",
    "79": "GPIO.setup()方法",
    "80": "GPIO.setup(18, GPIO.IN, pull_up_down=GPIO.PUD_UP)",
    "81": "GPIO21",
    "82": "GPIO库",
    "83": "GPIO引脚",
    "84": "GPIO引脚编号模式的一种",
    "85": "GPIO端口文件",
    "86": "GPU",
    "87": "GPU 委托",
    "88": "GPU代理",
    "89": "GStreamer",
    "90": "GStreamer管道参数",
    "91": "GlobalAveragePooling2D",
    "92": "Google",
    "93": "Google Arts & Culture",
    "94": "Google Assistant",
    "95": "Google Edge TPU Coral Dev Board",
    "96": "Google Photos",
    "97": "Google Photos、Google Assistant、Uber、Airbnb、网易、爱奇艺、WPS 等",
    "98": "Google 内部用于计算机视觉场景的解决方案",
    "99": "Gradle 同步",
    "100": "HDMI 接口",
    "101": "HIGH电平",
    "102": "HTML 文件",
    "103": "HTML文件、JS文件和配置文件",
    "104": "HTML文档",
    "105": "HTML文档类型声明、html标签、head标签和body标签",
    "106": "Haar 特征分类器",
    "107": "Haar 特征的 cascade 分类器",
    "108": "Haar特征的cascade分类器",
    "109": "Hello AI World",
    "110": "IMAGE_WIDTH, IMAGE_HEIGHT, testData, testxs, labels, preds",
    "111": "ImageDataGenerator",
    "112": "ImageProcessor",
    "113": "Intel Neural Compute Stick 2",
    "114": "IoT领域",
    "115": "JCenter Bintray 的 TFLite AAR",
    "116": "Java",
    "117": "Java 或 C++ API 执行 TensorFlow Lite 推理",
    "118": "JavaScript",
    "119": "JavaScript 语言版本的扩展",
    "120": "JetBot",
    "121": "JetPack SDK",
    "122": "Jetson",
    "123": "Jetson Nano",
    "124": "Jetson Nano 开发板",
    "125": "Jetson Nano开发板",
    "126": "Jetson 项目社区",
    "127": "Jupyter Lab",
    "128": "Jupyter Notebook",
    "129": "Jupyter Notebook 的全面升级",
    "130": "Jupyter Notebook、文本编辑器、终端",
    "131": "Jupyter Notebook、文本编辑器、终端以及各种个性化组件",
    "132": "Jupyter lab",
    "133": "JupyterLab",
    "134": "Keras H5",
    "135": "Keras Model 和 SavedModel",
    "136": "Keras模型",
    "137": "Keras的模型定义方式",
    "138": "LED",
    "139": "LED灯",
    "140": "LED的控制引脚",
    "141": "LOW电平",
    "142": "Layers API",
    "143": "Layers API和Core API",
    "144": "Linux",
    "145": "Linux 中所有设备文件或特殊文件的存储位置",
    "146": "Linux 开发环境",
    "147": "Live Caption功能",
    "148": "MCU",
    "149": "MIPI",
    "150": "MIPI CSI-2",
    "151": "MIPI 联盟发起的为移动应用处理器制定的开放标准",
    "152": "MNIST数据集",
    "153": "MNIST项目",
    "154": "MaxPooling2D",
    "155": "Micro USB 接口",
    "156": "Micro-USB 接口",
    "157": "MnistData",
    "158": "MobileNet",
    "159": "MobileNet V2",
    "160": "MobileNet V2模型",
    "161": "MobileNet 模型",
    "162": "MobileNetV2",
    "163": "MobileNets_v2",
    "164": "NVIDIA Jetson Nano",
    "165": "NVIDIA Jetson Nano 开发板",
    "166": "NVIDIA Jetson 开发者专区",
    "167": "NVIDIA Jetson 论坛",
    "168": "Node.js 或浏览器环境",
    "169": "Object C",
    "170": "OpenCV",
    "171": "OpenCV 编译",
    "172": "OperatorCode",
    "173": "Parcel, WebPack 或 Rollup",
    "174": "PoseNet模型",
    "175": "PoseNet示例应用程序",
    "176": "Python",
    "177": "Python 2.7",
    "178": "Python API 和命令行工具",
    "179": "Python GPIO引脚",
    "180": "Python 代码片段",
    "181": "Python 官网",
    "182": "Python 源码包",
    "183": "Python包管理",
    "184": "Python开发环境",
    "185": "Python模块编译失败",
    "186": "Quantization-aware training",
    "187": "RPI.GPIO库",
    "188": "RandomFlip",
    "189": "RandomRotation",
    "190": "Raspberry Camera V2",
    "191": "Raspberry Pi 4",
    "192": "Raspberry Pi、Arducam 等常见的相机模块",
    "193": "SD Memory Card Formatter",
    "194": "SavedModel",
    "195": "SavedModel 和 Keras Sequential",
    "196": "SavedModel 和 Keras Sequential 两种模型导出方法和格式",
    "197": "Sequential模型",
    "198": "SparseCategoricalCrossentropy",
    "199": "Swift",
    "200": "Swift 和 Objective-C 编写的原生 iOS 库",
    "201": "TF Mobile",
    "202": "TFLite",
    "203": "TFLite API",
    "204": "TFLite model",
    "205": "TFLite 文件格式",
    "206": "TFLite 模型",
    "207": "TFLite 模型文件格式",
    "208": "TFLite 模型文件格式，支持内存高效读取",
    "209": "TFLite 模型转换过程",
    "210": "TFLite 解释器",
    "211": "TFLite 解释执行器",
    "212": "TFLite 转换器",
    "213": "TFLiteConverter.from_concrete_functions()",
    "214": "TFLiteConverter.from_keras_model()",
    "215": "TFLiteConverter.from_saved_model()",
    "216": "TFLite模型",
    "217": "TFLite模型转换器",
    "218": "TFLite解释器",
    "219": "TFLite解释器和GPU代理",
    "220": "TFLite转换",
    "221": "TFMini",
    "222": "TensorBoard",
    "223": "TensorFLow-vis",
    "224": "TensorFlow",
    "225": "TensorFlow 2.x 模型",
    "226": "TensorFlow GPU 版本",
    "227": "TensorFlow Hub",
    "228": "TensorFlow Hub 上可搜索到的模型",
    "229": "TensorFlow Lite",
    "230": "TensorFlow Lite AAR",
    "231": "TensorFlow Lite API",
    "232": "TensorFlow Lite 工作流程",
    "233": "TensorFlow Lite 开发工作流程",
    "234": "TensorFlow Lite 推理",
    "235": "TensorFlow Lite 支持库",
    "236": "TensorFlow Lite 模型",
    "237": "TensorFlow Lite 模型文件",
    "238": "TensorFlow Lite 算子库",
    "239": "TensorFlow Lite 解释器",
    "240": "TensorFlow Lite 解释器(Interpreter)",
    "241": "TensorFlow Lite 解释器、TensorFlow Lite 转换器、算子库、硬件加速代理",
    "242": "TensorFlow Lite 转换器",
    "243": "TensorFlow Lite 转换器(Converter)",
    "244": "TensorFlow Lite 转换器命令行工具",
    "245": "TensorFlow Lite推理过程",
    "246": "TensorFlow Lite支持库",
    "247": "TensorFlow Lite模型",
    "248": "TensorFlow Lite的工作流程",
    "249": "TensorFlow Lite的简称",
    "250": "TensorFlow Lite解释器",
    "251": "TensorFlow 安装",
    "252": "TensorFlow 模型",
    "253": "TensorFlow 模型导出",
    "254": "TensorFlow 版本",
    "255": "TensorFlow 的 JavaScript 版本",
    "256": "TensorFlow.js",
    "257": "TensorFlow.js 模块",
    "258": "TensorFlow.js 进行浏览器可视化的一组实用工具库",
    "259": "TensorFlow.js中的中心数据单元，是一维或多维数组",
    "260": "TensorFlow团队开发的产品",
    "261": "Tensorflow.js",
    "262": "Tensorflow.js模型",
    "263": "Text，Image，Video 和 Publishers 等类别",
    "264": "Total params: 2,626,821, Trainable params: 368,837, Non-trainable params: 2,257,984",
    "265": "USB",
    "266": "USB摄像头",
    "267": "VNC",
    "268": "VNC Viewer",
    "269": "VNC 服务",
    "270": "WPS",
    "271": "Web Server for Chrome",
    "272": "Web 开发新手",
    "273": "XML 文件，该文件中会描述人体各个部位的 Haar 特征值",
    "274": "aaptOptions",
    "275": "activation",
    "276": "add_event_detect()函数",
    "277": "allocate_tensors()",
    "278": "android/start 目录下的项目模板和 finish 目录下的完整代码",
    "279": "base_model",
    "280": "batchSize",
    "281": "batchSize设置为512",
    "282": "batch_size",
    "283": "batch_size用于设置每批图像数量",
    "284": "build.gradle",
    "285": "buildscript",
    "286": "callback",
    "287": "callbacks设置为fitCallbacks",
    "288": "categorical_crossentropy",
    "289": "cdn.jsdelivr.net",
    "290": "class_names",
    "291": "compare_faces",
    "292": "console.log语句",
    "293": "conv2d, dropout, global_average_pooling2d, dense",
    "294": "convertToTensor函数",
    "295": "converter.convert",
    "296": "converter.optimizations=[tf.lite.Optimize.DEFAULT]",
    "297": "createModel()",
    "298": "daisy, dandelion, roses, sunflowers, tulips",
    "299": "data.js",
    "300": "data.js文件",
    "301": "dense层",
    "302": "dependencies",
    "303": "dispose",
    "304": "dispose和tf.tidy两种内存管理方法",
    "305": "doPrediction函数",
    "306": "epochs",
    "307": "epochs设置为10",
    "308": "face_cascade.detectMultiScale",
    "309": "face_distance",
    "310": "face_encodings",
    "311": "face_landmarks",
    "312": "face_locations",
    "313": "filters",
    "314": "fine_tune_at",
    "315": "flatten层",
    "316": "flow_from_directory",
    "317": "flower_classification",
    "318": "from_saved_model(), from_keras_model(), from_concrete_functions()",
    "319": "functional模型",
    "320": "get_input_details()",
    "321": "get_output_details()",
    "322": "get_tensor()",
    "323": "hog模型在CPU上运行更快但不太准确，cnn模型更准确但需要GPU加速",
    "324": "hub.KerasLayer",
    "325": "iOS",
    "326": "iOS 开发人员",
    "327": "iOS 开发者",
    "328": "import * as tf from '@tensorflow/tfjs' console.log(tf.version.tfjs) const shape = [2, 3]; // 2 rows, 3 columns const a = tf.tensor",
    "329": "import语句",
    "330": "include_top=False",
    "331": "index",
    "332": "index.html",
    "333": "index.html、index.js和data.js文件",
    "334": "index.js",
    "335": "inputShape",
    "336": "inputShape, kernelSize, filters, strides, activation, kernelInitializer",
    "337": "inputShape为[1]，units为1，useBias为true",
    "338": "interpreter",
    "339": "interpreter.get_tensor()",
    "340": "invoke()",
    "341": "jtop 命令",
    "342": "jupyter",
    "343": "jupyter_notebook_config.py",
    "344": "kernelInitializer",
    "345": "kernelSize",
    "346": "labels.txt",
    "347": "lambda x, y: (normalization_layer(x), y)",
    "348": "layers.Flatten()",
    "349": "load_image_file",
    "350": "loss='categorical_crossentropy'",
    "351": "make install命令",
    "352": "make命令",
    "353": "metrics=['accuracy']",
    "354": "microSD 卡",
    "355": "microSD 卡插槽",
    "356": "minNeighbors",
    "357": "mnist项目",
    "358": "mobilenetv2_1.00_224",
    "359": "model",
    "360": "model.compile",
    "361": "model.fit",
    "362": "model.fit()",
    "363": "model.predict()",
    "364": "model.tflite",
    "365": "model.tflite和label.txt",
    "366": "model.trainable = False",
    "367": "modelSummary",
    "368": "model和保存目录路径",
    "369": "nextTestBatch",
    "370": "nextTrainBatch",
    "371": "nextTrainBatch（batchSize）和 nextTestBatch（batchSize）方法",
    "372": "noCompress \"tflite\"",
    "373": "normalization_layer",
    "374": "np.argmax()",
    "375": "num_classes个输出",
    "376": "number_of_times_to_upsample",
    "377": "optimizer='sgd', loss='mean_squared_error'",
    "378": "optimizer=tf.keras.optimizers.Adam(1e-5)",
    "379": "org.tensorflow 库",
    "380": "output_details",
    "381": "package.json",
    "382": "pip",
    "383": "pip install",
    "384": "pip3",
    "385": "poolSize",
    "386": "poolSize, strides",
    "387": "post-training quantization",
    "388": "predict()",
    "389": "print(\"button pressed!\")",
    "390": "python",
    "391": "recognizeImage方法",
    "392": "repositories和dependencies",
    "393": "resize原始图像到模型输入大小",
    "394": "saved_model_dir",
    "395": "schema.fbs",
    "396": "sequential模型",
    "397": "sequential模型和functional模型",
    "398": "set_tensor()",
    "399": "showAccuracy函数",
    "400": "showConfusion函数",
    "401": "shuffle",
    "402": "shuffle用于控制批处理顺序",
    "403": "shuffle设置为true",
    "404": "strides",
    "405": "target_size参数",
    "406": "target_size用于设置图像大小",
    "407": "tensor()",
    "408": "tensorflow-lite",
    "409": "tensor创建代码",
    "410": "tf.keras model",
    "411": "tf.keras.Sequential",
    "412": "tf.keras.layers.Dense",
    "413": "tf.keras.losses.SparseCategoricalCrossentropy",
    "414": "tf.keras.models.Sequential",
    "415": "tf.keras.optimizers.Adam",
    "416": "tf.linspace()",
    "417": "tf.lite.TFLiteConverter",
    "418": "tf.lite.TFLiteConverter.from_keras_model",
    "419": "tf.lite.TFLiteConverter.from_saved_model",
    "420": "tf.losses.meanSquaredError",
    "421": "tf.model()",
    "422": "tf.nn.softmax()",
    "423": "tf.saved_model.save",
    "424": "tf.sequential",
    "425": "tf.sequential()",
    "426": "tf.sequential()和tf.model()两种创建模型的方式",
    "427": "tf.tensor",
    "428": "tf.tensor2d",
    "429": "tf.tidy",
    "430": "tf.tidy()",
    "431": "tf.train.adam()",
    "432": "tfjs-examples/mnist",
    "433": "tfliteModel和tfliteOptions",
    "434": "tflite_convert",
    "435": "tflite_convert --keras_model_file=/tmp/mobilenet_keras_model.h5 --output_file=/tmp/mobilenet.tflite",
    "436": "tflite_convert --saved_model_dir=/tmp/mobilenet_saved_model --output_file=/tmp/mobilenet.tflite",
    "437": "tfvis.render.scatterplot",
    "438": "time.sleep()方法",
    "439": "train_generator",
    "440": "train_generator, steps_per_epoch=len(train_generator), epochs=epochs, validation_data=val_generator, validation_steps=len(val_generator)",
    "441": "ui.js",
    "442": "units=1",
    "443": "units=1, input_shape=[1]",
    "444": "units=16, activation='relu'",
    "445": "units为1，useBias为true",
    "446": "useBias",
    "447": "val_ds",
    "448": "val_ds.map",
    "449": "val_generator",
    "450": "validationData设置为[testXs, testYs]",
    "451": "vino",
    "452": "wait_for_edge()函数",
    "453": "wait_for_edge()函数和add_event_detect()函数",
    "454": "x—images, y—labels",
    "455": "yarn",
    "456": "一个使用数据流图进行数值计算的开源软件库",
    "457": "一个多媒体框架，用于后端处理任务，如格式修改、显示驱动程序协调和数据处理",
    "458": "一个强大、简单、易上手的人脸识别开源项目",
    "459": "一个端到端的机器学习开源框架",
    "460": "一个缩减版的 TensorFlow，简化了算子集，也缩小了运行库",
    "461": "一个轻量、快速、兼容度高的专门针对移动式应用场景的深度学习工具",
    "462": "一切皆文件，对文件的读写操作相当于外设输入输出",
    "463": "一种数据结构，包含了在解决一个特定问题时，训练得到的机器学习网络的逻辑和知识",
    "464": "一种有效的物品检测方法",
    "465": "一种特定的矩阵用来呈现算法性能的可视化效果",
    "466": "一系列的函数，这些函数以一个或多个张量作为输入，并输出另一个张量",
    "467": "一系列的计算节点",
    "468": "一组帮助开发者在移动设备、嵌入式设备和IoT设备上运行TensorFlow模型的工具",
    "469": "一组数字引脚，可用于将树莓派连接到其他电子设备",
    "470": "上拉电阻",
    "471": "下载 OpenCV",
    "472": "下载和访问mnist数据集",
    "473": "下载解压",
    "474": "不支持 CUDA",
    "475": "不支持 CUDA 且版本是固定搭配的",
    "476": "不改变基础模型的各项参数变量",
    "477": "不清除内部函数的返回值",
    "478": "不需要原始模型构建代码",
    "479": "不需要原始模型构建代码就可以运行",
    "480": "不需要原有模型中最后的神经网络层",
    "481": "不需要序列化或可以创造自己的序列化方法",
    "482": "不需要改变模型",
    "483": "与 NVIDIA TensorRT 一起使用预训练模型进行实时图像分类和对象检测",
    "484": "与 TensorFlow 一起安装",
    "485": "与 TensorFlow 的核心算子库略有不同",
    "486": "与树莓派结合可以将项目与现实世界轻松的联系起来",
    "487": "串联在LED和电源之间限制电流保护LED和GPIO引脚",
    "488": "为LED提供电源",
    "489": "主要应用于游戏场景",
    "490": "主要应用于游戏场景，是为了高性能场景创建的序列化库",
    "491": "了解模型效率、调试超参数",
    "492": "二维卷积层",
    "493": "二进制文件小、延迟低、支持设备端机器学习推断",
    "494": "二进制文件很小",
    "495": "二进制文件的大小约为 1 MB（针对 32 位 ARM build）",
    "496": "交互式、富文本、多语言支持",
    "497": "交叉熵损失函数",
    "498": "人脸检测",
    "499": "人脸检测、检测面部特征点、给脸部编码、从编码中找出人的名字",
    "500": "人脸识别",
    "501": "仅用于开发的程序包",
    "502": "仅适用于卷积神经网络的一个子集",
    "503": "从 Keras Model 转换模型",
    "504": "从 MNIST 数据集中随机批量提取 MNIST 图像",
    "505": "从 SavedModel 转换模型",
    "506": "从GitHub克隆项目代码，进入项目目录，使用yarn构建和运行",
    "507": "从头自己编译",
    "508": "从指定目录生成训练数据批次",
    "509": "从指定目录生成验证数据批次",
    "510": "从测试集中返回一批图像及其标签",
    "511": "从训练集中返回一批随机图像及其标签",
    "512": "从该层开始进行微调的参数",
    "513": "以最小精度下降来训练网络",
    "514": "优化分类任务",
    "515": "优化模型",
    "516": "优化模型大小和性能",
    "517": "优化的 FlatBuffer 格式",
    "518": "优化的 FlatBuffer 格式，以 .tflite 为文件扩展名",
    "519": "优化的算子库",
    "520": "会使用 GPU 加速模型的运算，提高运算效率",
    "521": "会导致多次输出",
    "522": "位于其引脚排针上，可以通过跳线线连接到其他电路板或设备",
    "523": "低功耗、支持多种深度学习框架",
    "524": "作为优化器",
    "525": "作为优化器用于模型训练",
    "526": "作为判断训练结果的参数",
    "527": "作为损失函数",
    "528": "作为损失函数用于模型训练",
    "529": "作为模型优化算法",
    "530": "作为网页的入口文件",
    "531": "作为迁移学习的基础模型",
    "532": "使权重和激活值的 Post training 更简单",
    "533": "使模型的预测不受图像顺序的影响",
    "534": "使用 Python API 进行转换",
    "535": "使用 SavedModel 格式存储",
    "536": "使用 TFLite 模型转换器转换成 TFLite 文件格式(FlatBuffers 格式)",
    "537": "使用 TensorFlow Lite 解释器在设备端运行模型",
    "538": "使用 nano 或者 vi 编辑工具修改软件源的配置文件/etc/apt/sources.list",
    "539": "使用3×3的卷积核，并在输出上使用Relu激活函数",
    "540": "使用GPU来加速数学运算",
    "541": "使用TFLiteConverter转换模型",
    "542": "使用TensorFlow.js实现手势识别",
    "543": "使用VGG、ResNet等模型",
    "544": "使用tf.model() API创建非闭环的计算图",
    "545": "使用tf.saved_model.save保存模型",
    "546": "使用tf.saved_model.save函数",
    "547": "使用低学习率编译模型",
    "548": "使用低学习率重新编译模型",
    "549": "使用层构建模型",
    "550": "使用已编译好的库",
    "551": "使用模型从未见过的测试数据评估分类器准确性",
    "552": "使用模型优化工具包缩减模型大小并提高效率",
    "553": "使用测试数据集评估模型",
    "554": "使用深度可分离的卷积",
    "555": "使用自己的 TensorFlow 模型、在线查找模型或从 TensorFlow 预训练模型中选择",
    "556": "使用计算机视觉相关的模型，包括实时摄像机的使用",
    "557": "使用语言就是 Javascript，前端工程师不需要学习其他后端语言，降低入门门槛",
    "558": "使用预训练量化进行模型转换",
    "559": "使这些专用功能适应新数据集，而不是覆盖通用学习",
    "560": "保存为model.tflite文件",
    "561": "保存完整的TensorFlow程序包括权重和计算",
    "562": "保存完整的TensorFlow程序，包括权重值和计算",
    "563": "保存训练数据的类别标签",
    "564": "保持了很多通用性",
    "565": "保留原来大规模训练的优势",
    "566": "修改build.gradle配置",
    "567": "借助低级运算构建模型",
    "568": "做了移动设备相关的优化",
    "569": "允许解释器在设备的 GPU 上运行适当的运算符",
    "570": "全能 IDE",
    "571": "全能IDE",
    "572": "全连接 (Full Connected) 层",
    "573": "全连接(Full Connected)层",
    "574": "全连接网络",
    "575": "共享的内存缓冲区",
    "576": "关闭LED灯",
    "577": "具有shape属性定义数组形状",
    "578": "兼容度高",
    "579": "内存只有几十KB",
    "580": "内存回收问题突出",
    "581": "内存高效",
    "582": "内置的算子",
    "583": "写一段简单的测试代码",
    "584": "写入镜像",
    "585": "冻结前100层",
    "586": "冻结预训练模型并更新分类器的权重",
    "587": "准备训练集和验证集",
    "588": "准确度",
    "589": "减少内存碎片化",
    "590": "减少服务器的运算，提高服务器资源利用，增强客户端响应运算结果的速度",
    "591": "出门问问智能音箱",
    "592": "分类到5类",
    "593": "分类结果的概率",
    "594": "创建0~1之间平均分配的100个值",
    "595": "创建TFLite转换器实例",
    "596": "创建Tensor实例的主要构造函数",
    "597": "创建、训练和导出自定义 TensorFlow Lite 模型",
    "598": "创建包含多个 Dense 层的模型",
    "599": "创建管道，绑定视频流，逐帧提取和显示",
    "600": "创建解释器、分配张量",
    "601": "创新奇智",
    "602": "创新奇智智能读码机",
    "603": "创新奇智智能质检一体机",
    "604": "初始化TensorFlow Lite解释器",
    "605": "利用 ARM 的 NEON 指令集做了大量的优化",
    "606": "利用 SIMD 指令功能",
    "607": "利用在同一域中的较大数据集上训练的模型所学习的特征",
    "608": "前几层学习非常简单和通用的功能，这些功能可以推广到几乎所有类型的图像",
    "609": "剪刀石头布识别项目",
    "610": "功耗非常低",
    "611": "功能强大且易于使用",
    "612": "功能强大的编程语言，易于使用，易于阅读和编写",
    "613": "功能强大，交互式、富文本，还有丰富的插件、主题修改、多语言支持",
    "614": "加载 @tensorflow/tfjs 和 @tensorflow/tfjs-vis 的代码",
    "615": "加载 @tensorflow/tfjs 和 @tensorflow/tfjs-vis 的脚本",
    "616": "加载和运行TFLite模型",
    "617": "加载数据并准备进行训练, 定义模型结构, 训练模型并监视其性能, 评估模型",
    "618": "加载数据，定义模型，训练循环并指定UI元素",
    "619": "加载模型",
    "620": "加载模型、分配张量、设置输入张量、执行推理、读取输出张量",
    "621": "加载模型、转换数据、运行模型推理、解释输出",
    "622": "加载面孔照片",
    "623": "加速模型推理",
    "624": "动态显示训练的过程",
    "625": "包含JavaScript代码用于网页功能实现",
    "626": "包含MNIST数据集相关数据",
    "627": "包含完整的TensorFlow程序",
    "628": "千兆以太网端口",
    "629": "升级 Python 版本",
    "630": "单一芯片的小型计算机",
    "631": "单个图像的维度为[28,28,1]",
    "632": "卷积图像分类模型",
    "633": "卷积完成后应用于数据的激活函数",
    "634": "卷积层",
    "635": "卷积层与全连接层",
    "636": "卷积层输入",
    "637": "取消冻结模型的顶层",
    "638": "受限于GPU内存的大小",
    "639": "只提供了基本的转化功能",
    "640": "可从 github 下载源码",
    "641": "可以从同一网络上的另一台计算机控制 Jetson Nano 开发板",
    "642": "可以使用树莓派摄像头，IMX219模组800万像素",
    "643": "可以利用手机上的加速器，比如 GPU 或者 DSP",
    "644": "可以更换国内源如阿里、清华等",
    "645": "可以添加--no-cache-dir参数",
    "646": "可以直接使用cv2.videocapture打开",
    "647": "可以直接在浏览器中运行，无需进行安装，也无需借助后端运行",
    "648": "可以调用不同的硬件加速器比如GPU进行执行",
    "649": "可以通过软件编程进行控制",
    "650": "可以配置为输入或输出",
    "651": "可以配置图像加载的细节",
    "652": "可在手机、嵌入式 Linux 设备和微控制器等很多不同类型的硬件上运行",
    "653": "可能值得使用构建工具进行探索",
    "654": "可能存在过度拟合",
    "655": "可能导致模型过拟合",
    "656": "可视化模型训练的过程和结果",
    "657": "可视化模型预测结果和原始数据",
    "658": "可配置为输入或输出",
    "659": "同步项目依赖",
    "660": "启动Jupyter lab",
    "661": "启动图标",
    "662": "启用默认优化",
    "663": "命令行 TensorFlow Lite 转换器命令行工具",
    "664": "命令行与 Python API",
    "665": "命令行工具",
    "666": "命令行工具和 Python API",
    "667": "四引脚按键",
    "668": "四脚按键开关",
    "669": "回调函数",
    "670": "图像分类、物体检测、分割和语音处理等应用程序中并行运行多个神经网络",
    "671": "图像分类任务",
    "672": "图像和视频处理",
    "673": "图像识别",
    "674": "图像识别、文本处理、语音识别、AR 效果等",
    "675": "图像识别模型，可用于迁移学习",
    "676": "图像预处理",
    "677": "在 Android 与 iOS 平台上使用",
    "678": "在 Android 项目中使用 TensorFlow Lite",
    "679": "在 HTML 中直接引用 TensorFlow.js 发布的 NPM 包中已经打包安装好的 JavaScript 代码",
    "680": "在 MCU 上甚至可以小于 100KB",
    "681": "在 Node 环境进行运算的速度目前与 Python 速度不相上下",
    "682": "在 TensorFlow Lite 中使用",
    "683": "在18号引脚处设置上拉电阻",
    "684": "在不同设备上使用硬件加速",
    "685": "在图像中检测面部",
    "686": "在大规模数据处理上不如Python高效",
    "687": "在小型数据集上训练模型",
    "688": "在把模型嵌入到实际的二进制文件中时兼容性存在较大挑战",
    "689": "在有 GPU 加速的手机上运行 MobileNet 图像分类，模型运行速度可以提高",
    "690": "在每一个训练周期显示训练情况",
    "691": "在浏览器上开发模型",
    "692": "在浏览器中加载",
    "693": "在浏览器中训练模型",
    "694": "在浏览器环境中实现深度学习的功能",
    "695": "在生产环境中不需要",
    "696": "在移动端（mobile）、嵌入式（embeded）和物联网（IoT）设备上运行 TensorFlow 模型",
    "697": "在移动设备、嵌入式设备和 IoT 设备上运行 TensorFlow 模型",
    "698": "在移动设备上运行 TensorFlow",
    "699": "在移动设备和嵌入式设备上运行TensorFlow模型",
    "700": "在给定设备上实现性能、模型大小和准确性的理想平衡",
    "701": "在训练过程中不能用于训练",
    "702": "在设备端运行TFLite模型",
    "703": "在资源限制严重的移动和嵌入式设备上执行",
    "704": "在边缘设备上运行 TensorFlow 模型推理",
    "705": "在边缘设备上运行 TensorFlow 模型推理的官方框架",
    "706": "基于 TF Mobile 的经验",
    "707": "基于 WebGL 加速的开放源代码 JavaScript 机器学习库",
    "708": "基于一个流线型的架构，使用深度可分离的卷积",
    "709": "基于流线型架构的轻量级深层神经网络",
    "710": "基于浏览器运行已训练的模型",
    "711": "基于现有模型构建 Interpreter",
    "712": "基于现有的模型进行继续训练",
    "713": "基础模型的各项参数变量不会被新的训练修改数据",
    "714": "增加一个事件的检测函数",
    "715": "处理简单的数据",
    "716": "多个张量",
    "717": "多个连续层",
    "718": "大大降低了移动端及IoT设备端的深度学习技术门槛",
    "719": "大量易于学习的教程",
    "720": "头信息",
    "721": "如果仅使用支持常见图像分类模型（InceptionV3 和 MobileNet）所需的运算符，二进制文件的大小不到 300 KB",
    "722": "如果在预先训练的模型上添加一个随机初始化的分类器并尝试联合训练所有层，则梯度更新的幅度将太大，并且预训练模型将忘记它学到的东西",
    "723": "子图",
    "724": "子图、算子库和共享的内存缓冲区",
    "725": "子图本身的输入和输出",
    "726": "存储图像和标签数据",
    "727": "存储已安装软件包的名称和版本",
    "728": "存储模型权重",
    "729": "存放Python相关程序模块",
    "730": "存放训练好的模型供开发人员复用",
    "731": "学习空间不变的变换",
    "732": "安卓应用只需 1 兆左右的运行环境",
    "733": "安装 OpenCV",
    "734": "安装 Python 包依赖项",
    "735": "安装 TensorFlow",
    "736": "安装 python",
    "737": "安装Package",
    "738": "安装Python",
    "739": "安装依赖",
    "740": "安装依赖项",
    "741": "安装和升级 pip3",
    "742": "安装所需的系统包",
    "743": "完全基于 JavaScript 从头开发、训练和部署模型",
    "744": "完成分类",
    "745": "官方已经停止维护",
    "746": "官方推荐使用的无线网卡",
    "747": "官方集成到Python的工具",
    "748": "定义的神经元网络层与层之间的关系较为随意",
    "749": "定义需要监视的指标",
    "750": "定位图像中的人脸位置",
    "751": "实例化对象，调用add方法添加输入层和输出层",
    "752": "实例化预训练模型作为基础模型",
    "753": "实时识别照相机所拍摄的花卉",
    "754": "实现 headless 远程桌面访问 Jetson Nano",
    "755": "实现了一组优化的算子内核",
    "756": "实现人体姿势估计",
    "757": "实现花卉识别 app",
    "758": "实现识别花卉模型",
    "759": "室内避开障碍物",
    "760": "对 SIMD 指令功能特别有益",
    "761": "对val_ds中的每个元素应用lambda函数进行归一化处理",
    "762": "对图像数据进行归一化处理",
    "763": "对图像进行多少次上采样以查找人脸",
    "764": "对手写数字的图像进行分类",
    "765": "对数据降维",
    "766": "对模型的权重产生更一致且变化较小的渐变更新",
    "767": "对现有 CPU 平台的支持",
    "768": "对训练图像随机变换的方法来人为引入样本多样性",
    "769": "对随机目标函数执行一阶梯度优化的算法",
    "770": "导致每个时期的梯度更新数量较少",
    "771": "将 Keras 模型保存为 SavedModel 格式",
    "772": "将 NPM 模块转换为在线可以引用的免费服务",
    "773": "将 TensorFlow 模型格式转换为 TensorFlow Lite 的 Python API",
    "774": "将 TensorFlow 模型转换为 TFLite 专用格式",
    "775": "将 TensorFlow 模型转换为 TFLite 专用格式，通过解释器在设备端运行",
    "776": "将 TensorFlow 模型转换为 TFLite 格式",
    "777": "将 TensorFlow 模型转换为方便解释器使用的格式",
    "778": "将Keras模型转换为TFLite模型",
    "779": "将Python相关程序模块拷贝到/opt/python目录",
    "780": "将SavedModel转换为TFLite兼容格式",
    "781": "将SavedModel转换为TFLite模型",
    "782": "将SavedModel转换为TensorFlow Lite兼容格式",
    "783": "将TensorFlow模型转换为TFLite文件格式(FlatBuffers格式)",
    "784": "将TensorFlow模型转换为轻量级格式",
    "785": "将maven源google()和jcenter()替换为国内镜像",
    "786": "将三维张量展开到1维",
    "787": "将三维张量展开到1维以便传入Dense层",
    "788": "将人脸编码列表与候选编码进行比较",
    "789": "将前一层的输出平铺到一个向量中",
    "790": "将图片分类到1000类",
    "791": "将彩色图像转换为灰度图像，检测人脸，绘制边界矩形",
    "792": "将所有图像加载到一个模型需要的特定的大小",
    "793": "将数据转换为TensorFlow可读的张量格式",
    "794": "将模型加载到内存中",
    "795": "将模型嵌入到二进制文件中，这样就可以在设备上运行和部署模型",
    "796": "将模型文件和标签文件拷贝到assets目录",
    "797": "将模型显示在浏览器中",
    "798": "将特征转换为每个图像对应一个1280元素向量",
    "799": "将网络的每一层简单的叠在一起",
    "800": "将自定义模型转换为 TensorFlow Lite 格式",
    "801": "将输入数据转换成模型接收的形式或排布",
    "802": "将输入的 TensorFlow 模型生成 TensorFlow Lite 模型",
    "803": "将镜像写入 microSD 卡",
    "804": "将需要的层按顺序写在一个列表里，然后将列表作为sequential()函数的输入",
    "805": "嵌入式开发平台",
    "806": "工业物联智能设备的开发",
    "807": "已经训练好的分类器，其中包括面部，眼睛，微笑等",
    "808": "常开触点",
    "809": "常开触点和常闭触点",
    "810": "常见的移动/嵌入式平台",
    "811": "常闭触点",
    "812": "应用于输入数据的滑动窗口大小",
    "813": "应用于输入数据的滤波器窗口数量",
    "814": "底层 Core API 或最高级的 Layers API",
    "815": "廉价且周边设备多",
    "816": "延迟一秒钟",
    "817": "延迟低、二进制文件小",
    "818": "延迟较低",
    "819": "建议使用脚本代码",
    "820": "建议最小采用 64 GB UHS-1 卡",
    "821": "开关去抖",
    "822": "开关抖动",
    "823": "开发依赖",
    "824": "开发板",
    "825": "开发者套件",
    "826": "开源项目",
    "827": "开箱即用的开发库，开发者无需花精力去编写基础复杂的数学问题",
    "828": "引入优化以减小二进制文件的大小和提高性能",
    "829": "引用 Model 的内存缓冲区的一片区域，提高内存效率",
    "830": "张量",
    "831": "张量(Tensor)",
    "832": "张量形状是(image_height, image_width, color_channels)",
    "833": "归一化操作",
    "834": "当压力撤销时电路恢复原始状态",
    "835": "当压力施压时电路接通",
    "836": "形状为[null, 10]的张量",
    "837": "形状为[null, 28, 28, 1]的张量",
    "838": "形状是(224,224,3)",
    "839": "影响准确性",
    "840": "影响磁盘和内存占用以及运行效率",
    "841": "微调",
    "842": "微调少量顶层而不是整个 MobileNet 模型",
    "843": "微调结果",
    "844": "微调过程",
    "845": "微调预训练模型的顶层权重",
    "846": "必须与正在使用的 JetPack 版本一致",
    "847": "必须在开机前先装上去，系统才能识别",
    "848": "快速",
    "849": "快速启动",
    "850": "快速的启动并运行一组深度学习推理演示",
    "851": "忽略由于开关抖动引起的小于",
    "852": "恢复训练",
    "853": "手写数字识别",
    "854": "打乱数据顺序，创建特征向量和标签向量，转换为张量格式，进行归一化操作",
    "855": "打开现有 Android Studio 项目",
    "856": "打开项目图标",
    "857": "执行一个函数并清除所有创建的中间张量，释放它们的GPU内存",
    "858": "执行推理",
    "859": "执行推理并获取类别概率",
    "860": "执行最终的分类",
    "861": "执行模型推理",
    "862": "执行模型文件在输入数据上定义的运算符，输出推理结果",
    "863": "执行模型转换过程",
    "864": "批次大小",
    "865": "指定引脚编号系统",
    "866": "损失函数",
    "867": "接受 TFLite 模型",
    "868": "控制GPIO引脚",
    "869": "控制LED灯的亮暗",
    "870": "控制外部硬件设备",
    "871": "推理过程",
    "872": "推理速度提高了30%",
    "873": "描述构建和运行示例所需的依赖项",
    "874": "提供了一个简单的 API，用于从 Java、Swift、Objective-C、C++ 和 Python 运行 TensorFlow Lite 模型",
    "875": "提供了一些转换工具压缩模型，进行算子融合并生成代码",
    "876": "提供了各种库和工具，使编程更加方便",
    "877": "提供了大量方便的工具，例如权重初始化，模型序列化，训练监测，可迁移性和安全检查",
    "878": "提供低级的机器学习构建模块和高级的类似Keras的API",
    "879": "提供多种语言的 API",
    "880": "提供已经训练好且经过充分认证的模型",
    "881": "提供预训练模型用于图像分类、对象检测、姿势估计、文本恶意检测等",
    "882": "提供预训练模型，供开发者复用",
    "883": "提升移动设备上的性能",
    "884": "提问或分享项目",
    "885": "提高性能的方法是训练预训练模型的顶层的权重以及刚添加的分类器的训练",
    "886": "摄像头接口",
    "887": "支持 GPU 硬件加速",
    "888": "支持像素缩放和数据增强",
    "889": "支持多种编程语言包括Python、C++、Java、Swift和Javascript",
    "890": "支持多种视觉任务",
    "891": "支持大规模的模型训练和各种环境的部署",
    "892": "支持将文件映射到内存中直接读取和解释，不需要额外解析",
    "893": "支持微控制器(MCU)",
    "894": "支持微控制器(MCU)，模型大小可小至20 KB",
    "895": "支持服务器和移动端的部署",
    "896": "支持算子优化、量化等模型优化",
    "897": "支持算子优化和编译优化",
    "898": "支持自定义输入形状",
    "899": "支持设备端机器学习推断",
    "900": "支持设置目标图像大小和批次大小",
    "901": "支持量化",
    "902": "支持预装驱动程序的 RPi 相机，并且可以很容易地用作即插即用外围设备，不需要安装驱动程序",
    "903": "效果和服务器端十分接近",
    "904": "教育",
    "905": "数字引脚",
    "906": "数据图像的采集、模型的训练、参数的调整、模型文件生成、网页端部署",
    "907": "数据规范化和转换为张量类型",
    "908": "数据转换",
    "909": "数据转换、执行推理、解释输出",
    "910": "数据集",
    "911": "数据预处理",
    "912": "文字处理",
    "913": "易于设置和使用，并且与许多流行的配件兼容",
    "914": "是阻塞函数，会阻塞程序执行直到检测到一个边沿",
    "915": "智能质检一体机、智能读码机等工业物联智能设备",
    "916": "更新可视化元素",
    "917": "更新系统需要 root 权限",
    "918": "更谨慎地控制内存何时回收",
    "919": "更适合于边缘设备部署",
    "920": "最后的神经网络层",
    "921": "最大池化层",
    "922": "有一个名字",
    "923": "有两种模式：5W（低功耗模式；可以使用 USB 口供电）和 10W（必须使用 Power Jack 外接5V 电源供电）",
    "924": "有两种模式：BCM编号模式和物理引脚Broad编号模式",
    "925": "有助于避免因错误的样本而改向错误的方向",
    "926": "有经验的开发者",
    "927": "服装厂质检",
    "928": "未安装相应的依赖包",
    "929": "未满足的对等依赖 seedrandom@~",
    "930": "机器学习和计算机视觉应用，如物体检测、人脸识别、图像分割等视觉任务",
    "931": "机身只有 Ethernet 有线网络，不包括无线网卡",
    "932": "构建Tensorflow.js模型来识别手写数字",
    "933": "构建和运行mnist代码",
    "934": "构建和运行机器学习模型",
    "935": "构建小型移动机器人、人脸签到打卡、口罩识别、智能门锁、智能音箱等复杂 AI 系统",
    "936": "构建工具",
    "937": "构建神经网络",
    "938": "构建神经网络的全连接层",
    "939": "构成检测目标的相邻矩形的最小个数",
    "940": "查看开发板系统信息",
    "941": "标准算子",
    "942": "树莓派",
    "943": "树莓派系统",
    "944": "树莓派系统升级",
    "945": "树莓派编程",
    "946": "树莓派通用输入/输出接口（GPIO）",
    "947": "核心板可拆的设计",
    "948": "核心板的大小只有70 x 45 mm",
    "949": "核心运行时",
    "950": "格式化 microSD 卡",
    "951": "检测人脸",
    "952": "检测关键身体部位的位置",
    "953": "检测面部特征点",
    "954": "模型",
    "955": "模型优化",
    "956": "模型优化工具包",
    "957": "模型可以跟 Python 等其他语言模型进行互转",
    "958": "模型复杂度",
    "959": "模型大小",
    "960": "模型大小显著减小",
    "961": "模型推理",
    "962": "模型文件和标签文件",
    "963": "模型精度提高到98%",
    "964": "模型精度达到98%",
    "965": "模型编译",
    "966": "模型训练",
    "967": "比 CPU 执行更快的浮点矩阵运算",
    "968": "池化层",
    "969": "汽车油耗（MPG）",
    "970": "汽车的功率（Horsepower）",
    "971": "没有操作系统",
    "972": "注重实时性，内存高效",
    "973": "流入模型第一层的数据的形状",
    "974": "测试Python开发环境并查看当前的Python版本",
    "975": "测试的软件包、webpack或Babel",
    "976": "测试集",
    "977": "浏览器可以很好可视化机器训练过程，同时浏览器可调用设备的摄像头、麦克风等增加机器学习的应用场景",
    "978": "混淆矩阵",
    "979": "清华 Raspbian 软件仓库镜像",
    "980": "清华源",
    "981": "清理GPIO引脚的设置",
    "982": "清除张量或变量并释放其GPU内存",
    "983": "清除所有创建的中间张量并释放它们的GPU内存",
    "984": "滑动卷积滤波器窗口的大小",
    "985": "滑动窗口的步长",
    "986": "激活→设置为输出状态→写入1使PIN处于高电压点亮LED",
    "987": "灵活的架构可以将模型部署到桌面、服务器或移动设备中的 CPU 或 GPU 上",
    "988": "点亮LED灯",
    "989": "爱奇艺",
    "990": "版本变化后 API 函数会改变",
    "991": "版本变化后API函数会改变",
    "992": "瓶颈层",
    "993": "生成 HDF5 文件的绝对路径目录",
    "994": "生成SavedModel",
    "995": "生成一个批次一个批次的图片，以生成器的形式给模型训练",
    "996": "生成一个批次的图片，以生成器的形式给模型训练",
    "997": "生成批次的图片数据用于模型训练",
    "998": "生成配置文件",
    "999": "用apply()方法将上一层的输出作为本层的输入",
    "1000": "用于 5V 电源输入",
    "1001": "用于 flower_classification 项目",
    "1002": "用于工具的配置中心",
    "1003": "用于构建 TensorFlow 模型",
    "1004": "用于监督学习，表明多个类别是否有混淆",
    "1005": "用于训练模型",
    "1006": "用于预测",
    "1007": "用作启动设备和主存储器",
    "1008": "用户在自己的工作台中使用 TensorFlow API 构造 TensorFlow 模型",
    "1009": "用来转换 SavedModel 格式模型",
    "1010": "用来转换 concrete functions",
    "1011": "用来转换 tf.keras 模型",
    "1012": "用来连接 DP 屏幕",
    "1013": "电信号从低电平到高电平或从高电平到低电平状态的改变",
    "1014": "电阻",
    "1015": "登录 Jetson Nano",
    "1016": "目前有130个左右",
    "1017": "直接串联3.3V电源会产生过大电流可能损坏LED",
    "1018": "直接在 Objective-C 代码中使用 C API",
    "1019": "直接部署或用于迁移学习",
    "1020": "直流桶式插座",
    "1021": "相比 Protocol Buffer 有更高的性能和更小的大小",
    "1022": "硬件加速代理(Hardware accelerator delegate)",
    "1023": "确认 CUDA 已经被正常安装",
    "1024": "神经元权重计算中的偏置量",
    "1025": "离线语音识别",
    "1026": "科沃斯扫地机器人",
    "1027": "移动应用中的OCR处理",
    "1028": "移动行业处理器接口（MIPI）的相机串行接口（CSI）端口",
    "1029": "移动设备",
    "1030": "端侧机器学习",
    "1031": "端侧语音识别",
    "1032": "第一个卷积层",
    "1033": "第二、三卷积层",
    "1034": "简化图像预处理",
    "1035": "简化图像预处理和模型输出处理",
    "1036": "简单的线性回归的实验",
    "1037": "算子优化",
    "1038": "算子实现",
    "1039": "算子库",
    "1040": "算子库(Op kernels)",
    "1041": "算子索引",
    "1042": "算子融合、常数折叠和无用代码删除",
    "1043": "类型: bool. (default False) Enables the converter and flags used in TF 1.x instead of TF 2.x",
    "1044": "类型: string. Full path of the output file",
    "1045": "类型: string. Full path to the Keras H5 model file",
    "1046": "类型: string. Full path to the SavedModel directory",
    "1047": "线性堆叠layers的模型",
    "1048": "绘制混淆矩阵",
    "1049": "给脸部编码",
    "1050": "继承了 TFMini 和内部其他类似项目的很多优秀工作",
    "1051": "编程语言",
    "1052": "编译 OpenCV",
    "1053": "编译Python模块",
    "1054": "编译模型",
    "1055": "编译模型，损失函数使用类别交叉熵",
    "1056": "编译模型，设置优化器和损失函数",
    "1057": "缩减模型大小并提高效率，同时最大限度地降低对准确率的影响",
    "1058": "网易",
    "1059": "网络环境较差时可以考虑更换源",
    "1060": "能够利用各种硬件加速",
    "1061": "能够执行完整全面的反向传播",
    "1062": "脚本标签",
    "1063": "自定制算子",
    "1064": "花卉数据集中的图片",
    "1065": "花卉识别",
    "1066": "花卉识别 app",
    "1067": "花卉识别模型",
    "1068": "获取一些非常有意思的项目",
    "1069": "获取图像数据、创建位图、调用estimateSinglePose函数、绘制骨架",
    "1070": "获取张量的指针",
    "1071": "获取张量的数据",
    "1072": "获取数组中最大值的索引",
    "1073": "获取更多的 Jetson 平台信息",
    "1074": "衡量所有预测中正确预测的百分比",
    "1075": "观测开关去抖效果",
    "1076": "视频中的AR效果",
    "1077": "解决JavaScript内存回收问题",
    "1078": "解决跨域问题",
    "1079": "解释器、转换器、算子库、硬件加速代理",
    "1080": "解释输出",
    "1081": "计算已知人脸和未知人脸特征向量的距离",
    "1082": "计算并显示每个类别的准确度",
    "1083": "计算机视觉应用",
    "1084": "计算节点",
    "1085": "计算节点的输入和输出",
    "1086": "计算预测结果的softmax概率",
    "1087": "让硬件厂商可以扩展支持这样的接口",
    "1088": "让输入输出映射到0-1之间，保证后期更有效地训练",
    "1089": "训练分类器查看数千个图像及其标签",
    "1090": "训练后量化",
    "1091": "训练好的模型",
    "1092": "训练好的模型文件和标签文件",
    "1093": "训练数据和测试数据",
    "1094": "训练时从数据集中的不同类中随机选出的图像数量",
    "1095": "训练曲线",
    "1096": "训练期间需要更多的内存",
    "1097": "训练模型",
    "1098": "训练模型并记录训练和验证准确性/损失",
    "1099": "训练集",
    "1100": "训练集和验证集的值",
    "1101": "训练顶层分类器并将预先训练的模型设置为不可训练之后才应尝试",
    "1102": "训练预训练模型的顶层权重",
    "1103": "记录训练日志",
    "1104": "记录训练过程中的指标和计算直方图",
    "1105": "设置 OpenCV 的内容、位置和方式",
    "1106": "设置 model.trainable = False 参数后，训练期间将不更新预训练网络的权重",
    "1107": "设置18号引脚为输入模式并启用上拉电阻",
    "1108": "设置GPIO引脚的工作模式",
    "1109": "设置model.trainable = False",
    "1110": "设置为32，表示一次采样32条训练数据",
    "1111": "设置为50，表示遍历所有样本50次",
    "1112": "设置为true，表示打乱数据集",
    "1113": "设置前100层为不可训练",
    "1114": "设置受限于GPU内存大小",
    "1115": "设置在训练中基础模型的各项参数变量不会被新的训练修改数据",
    "1116": "设置系统引脚作为输入或输出",
    "1117": "设置访问密码",
    "1118": "设置输入张量值",
    "1119": "访问密码可以为空",
    "1120": "评估模型对新数据的泛化情况",
    "1121": "识别图像里的空间模式，例如线条和物体局部",
    "1122": "识别手写数字",
    "1123": "识别花卉图片",
    "1124": "识别输入图像",
    "1125": "语音识别",
    "1126": "请注意版本",
    "1127": "读取传感器数据，控制 LED 等外部设备",
    "1128": "读取输出张量值",
    "1129": "调整数据集形状",
    "1130": "调用CSI摄像头和USB摄像头",
    "1131": "调用model.fit方法进行训练",
    "1132": "调用不同的硬件加速器比如 GPU 进行执行",
    "1133": "调用解释器的方式：try (Interpreter interpreter = new Interpreter(tensorflow_lite_model_file)) { interpreter.run(input, output); }",
    "1134": "负极连接到GND",
    "1135": "资源有限可能导致内存溢出",
    "1136": "资源有限，可能导致内存溢出",
    "1137": "跨平台运行，包括 Android、iOS 以及基于 Linux 的 IoT 设备和微控制器",
    "1138": "身份验证",
    "1139": "转换 SavedModel 格式模型",
    "1140": "转换 concrete functions",
    "1141": "转换 tf.keras 模型",
    "1142": "转换、运行 TensorFlow 模型所需的所有工具",
    "1143": "转换为TensorFlow Lite模型",
    "1144": "转换数据",
    "1145": "转换模型",
    "1146": "软件源的配置文件/etc/apt/sources.list",
    "1147": "软链接",
    "1148": "轻量化",
    "1149": "轻量级",
    "1150": "轻量级、快速启动、内存高效",
    "1151": "输入层",
    "1152": "输入层和输出层",
    "1153": "输入输出用到的 Tensor 索引",
    "1154": "输入（Xs）和标签（Ys）",
    "1155": "输出为一个数字（油耗）",
    "1156": "输出宽度和高度会收缩",
    "1157": "输出层",
    "1158": "输出是一个三维的张量，其形状描述了 (height, width, channels)",
    "1159": "输出模式",
    "1160": "输出电压约为3.3V",
    "1161": "输出的通道数量取决于声明层时的 filters 参数",
    "1162": "输出的通道数量取决于声明层时的filters参数",
    "1163": "输出通道数为32",
    "1164": "输出通道数为64",
    "1165": "边做边学的理想工具",
    "1166": "边缘",
    "1167": "边缘操作",
    "1168": "边缘计算",
    "1169": "边缘计算设备",
    "1170": "迁移学习",
    "1171": "运行 TensorFlow Lite 模型",
    "1172": "运行Sync Gradle",
    "1173": "运行功率仅为 5 瓦",
    "1174": "运行各种深度学习模型",
    "1175": "运行已有的 Python 版 TensorFlow 模型",
    "1176": "运行服务监听的IP地址、端口、notebooks内核的目录、是否打开浏览器",
    "1177": "运行模型推理",
    "1178": "返回图像中每张人脸的128维人脸编码",
    "1179": "返回张量数据的副本",
    "1180": "返回的是数据的副本而非引用",
    "1181": "进行 VNC 连接",
    "1182": "进行内存清理工作，防止内存泄露",
    "1183": "进行大量的张量操作时使用可能会很麻烦",
    "1184": "连接其他电子设备",
    "1185": "连接到具有一个隐藏单元的dense层",
    "1186": "连接显示器、键盘和鼠标或通过 SSH 或 VNC 服务远程访问",
    "1187": "适应性低阶矩估计",
    "1188": "适用于多个平台",
    "1189": "适用于多个平台，提供了一个简单的 API",
    "1190": "选择模型",
    "1191": "选择模型、转换模型、部署到设备、优化模型",
    "1192": "逐步加载单个数据集的图像",
    "1193": "通过 pip 安装",
    "1194": "通过 script 标签引入 index.js",
    "1195": "通过GPIO控制点亮",
    "1196": "通过利用 WebGL 在 GPU 上执行计算大幅提高了速度",
    "1197": "通过命令行转换模型",
    "1198": "通过脚本标签（script tags）或从 yarn（或者 NPM）安装并使用 Parcel，WebPack 或 Rollup 等工具构建工程",
    "1199": "通过计算每个滑动窗口的最大值来缩减卷积结果的大小",
    "1200": "通过限流电阻串联到GPIO21",
    "1201": "郁金香(tulips)、玫瑰(roses)、浦公英(dandelion)、向日葵(sunflowers)、雏菊(daisy)",
    "1202": "部署TensorFlow Lite模型",
    "1203": "部署到设备",
    "1204": "部署在 Raspberry Pi 和基于 Arm64 的主板的嵌入式 Linux 系统上",
    "1205": "配置项目依赖",
    "1206": "释放张量的GPU内存",
    "1207": "量化",
    "1208": "针对移动端及IoT设备端的深度学习技术",
    "1209": "错误的连接和编程可能会导致设备损坏或故障",
    "1210": "防止Android在生成应用程序二进制文件时压缩TensorFlow Lite模型文件",
    "1211": "防止应用程序中的内存泄漏",
    "1212": "降低卷积层对位置的敏感",
    "1213": "降低存储器访问成本",
    "1214": "降低权重的精确表示",
    "1215": "降低权重的精确表示，并且可选的降低存储和计算的激活值",
    "1216": "随机初始化模型权重的方法",
    "1217": "随机打乱数据集",
    "1218": "随着层越来越高，这些功能越来越多地针对训练模型的数据集",
    "1219": "需先训练顶层分类器并设置预训练模型为不可训练",
    "1220": "需要使用GStreamer读取视频流",
    "1221": "需要使用能够在开发者套件的 Micro-USB 接口处提供 5V⎓2A 的高品质电源",
    "1222": "需要大约两个半小时",
    "1223": "需要安装依赖项",
    "1224": "需要成功配置好 CUDA",
    "1225": "需要更多灵活性和控制时使用",
    "1226": "需要确认版本",
    "1227": "需要配置proxy或使用国内镜像",
    "1228": "面包板、杜邦线公对母、LED灯、330欧姆电阻",
    "1229": "面向有兴趣学习 AI 和构建有趣应用程序的创客、学生和爱好者",
    "1230": "页面的基本结构，包含div标签、UI元素和JavaScript代码",
    "1231": "项目的清单文件",
    "1232": "预加载了ImageNet训练权重的深度学习模型",
    "1233": "预处理模型输入和后处理模型输出",
    "1234": "预处理输入图像",
    "1235": "预捕获图像宽度、高度，窗口显示宽度、高度，捕获帧率，旋转图像设置",
    "1236": "预测",
    "1237": "预测汽车油耗效率",
    "1238": "预测汽车油耗（MPG）",
    "1239": "预测汽车的油耗效率 MPG",
    "1240": "预训练模型和全连接的分类器",
    "1241": "首次打开项目时",
    "1242": "验证损失高于训练损失",
    "1243": "验证集",
    "1244": "高性能",
    "1245": "高性能场景创建的序列化库",
    "1246": "高阶 tf.keras.* API（Keras 模型）或低阶 tf.* API（用于生成具体函数）",
    "1247": "默认分类到1000类",
    "1248": "默认安装 JetPack 安装了对应的 OpenCV"
  }
}