{
  "entity_to_id": {
    "\"saved_model_keras_dir\"": 0,
    "--enable_v1_converter": 1,
    "--keras_model_file": 2,
    "--output_file": 3,
    "--output_file, --saved_model_dir, --keras_model_file, --enable_v1_converter": 4,
    "--saved_model_dir": 5,
    "-D WITH_QT=OFF 禁用了 Qt5 支持": 6,
    ".tflite": 7,
    "/dev/video0": 8,
    "/etc/apt/sources.list": 9,
    "/opt/python": 10,
    "/sys/class/gpio目录下的端口文件": 11,
    "/usr/bin/python": 12,
    "0.2s的响应时间": 13,
    "0.2的丢弃率": 14,
    "100": 15,
    "128核 NVIDIA Maxwell 架构的 GPU": 16,
    "14个引脚用于其他功能": 17,
    "155层": 18,
    "155层网络": 19,
    "2.0.0": 20,
    "2.3.0": 21,
    "26个引脚可以用作数字输入或输出": 22,
    "32个过滤器，3x3大小，激活函数为relu": 23,
    "3个 Conv2D 和 2个 MaxPooling2D 层": 24,
    "40 针 GPIO 扩展接口": 25,
    "40个 GPIO 引脚": 26,
    "40个引脚": 27,
    "4GB 的内存": 28,
    "5个节点": 29,
    "5个节点，激活函数为softmax": 30,
    "6": 31,
    "64位四核的 ARM Cortex-A57 CPU": 32,
    "800万像素": 33,
    "800万像素、感光芯片为索尼 IMX219，静态图片分辨率为3280 × 2464、支持1080p30, 720p60以及640 × 480p90视频录像": 34,
    "<!DOCTYPE html> <html> <head> <meta charset=\"utf-8\"> <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\"> <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">": 35,
    "@tensorflow/tfjs": 36,
    "@tensorflow/tfjs-vis": 37,
    "AC8265": 38,
    "ARM Cortex-A57 CPU": 39,
    "ARM Cortex-A72 CPU": 40,
    "Android": 41,
    "Android Studio": 42,
    "Android 应用": 43,
    "Android 开发人员": 44,
    "Android 开发人员使用": 45,
    "Android 开发者使用 JCenter Bintray 的 TFLite AAR": 46,
    "Android 设备": 47,
    "Android 部署": 48,
    "Android环境部署": 49,
    "Android部署": 50,
    "BCM2835芯片的GPIO库": 51,
    "BCM编号": 52,
    "BCM编号模式和物理引脚Broad编号模式": 53,
    "Broadcom针脚号，通常称的GPIO": 54,
    "C": 55,
    "C#": 56,
    "C++": 57,
    "C++ 和 Python 提供的 TensorFlow Lite API": 58,
    "CNN": 59,
    "CSI摄像头": 60,
    "CSI摄像头模块": 61,
    "CSI相机接口": 62,
    "CSI端口": 63,
    "ClassifierFloatMobileNet类": 64,
    "Classifier类": 65,
    "CocoaPods": 66,
    "Conv2D": 67,
    "Conv2D和MaxPooling2D层": 68,
    "Core API": 69,
    "C、C++": 70,
    "Dense": 71,
    "Display Port 接口": 72,
    "Dropout": 73,
    "Enables the converter and flags used in TF 1.x instead of TF 2.x": 74,
    "Etcher": 75,
    "Face Recognition": 76,
    "False": 77,
    "FlatBuffers": 78,
    "FlatBuffers 格式": 79,
    "Full path of the output file": 80,
    "Full path to the Keras H5 model file": 81,
    "Full path to the SavedModel directory": 82,
    "GPIO.cleanup()": 83,
    "GPIO.setmod()": 84,
    "GPIO.setup()": 85,
    "GPIO.setup(18, GPIO.IN, pull_up_down=GPIO.PUD_UP)": 86,
    "GPIO21": 87,
    "GPIO口": 88,
    "GPIO库": 89,
    "GPIO引脚": 90,
    "GPU": 91,
    "GPU 委托": 92,
    "GPU代理": 93,
    "GStreamer": 94,
    "GStreamer管道": 95,
    "GlobalAveragePooling2D": 96,
    "Google": 97,
    "Google Assistant、Google Photos、Uber、Airbnb、网易、爱奇艺、WPS等公司的应用": 98,
    "Google 内部用于计算机视觉场景的解决方案": 99,
    "Google 尝试简化 TensorFlow 并在移动设备上运行的缩减版 TensorFlow": 100,
    "Gordon Henderson": 101,
    "HDMI 接口": 102,
    "HIGH电平": 103,
    "HTML 文件": 104,
    "HTML文件、JS文件和配置文件": 105,
    "Haar特征分类器": 106,
    "Haar特征的cascade分类器": 107,
    "Hello AI World": 108,
    "I2C库": 109,
    "I2C接口(SCL、SDA)": 110,
    "IMAGE_WIDTH, IMAGE_HEIGHT, testData, testxs, labels, preds": 111,
    "IMX219模组": 112,
    "ImageDataGenerator": 113,
    "ImageProcessor": 114,
    "Java": 115,
    "Java 或 C++ API": 116,
    "JavaScript": 117,
    "JetBot": 118,
    "JetPack SDK": 119,
    "Jetson": 120,
    "Jetson B02版本": 121,
    "Jetson Nano": 122,
    "Jetson Nano 开发板": 123,
    "Jetson Nano开发板": 124,
    "Jupyter Notebook": 125,
    "Jupyter Notebook 的全面升级": 126,
    "Jupyter Notebook、文本编辑器、终端以及各种个性化组件": 127,
    "Jupyter lab": 128,
    "JupyterLab": 129,
    "Keras H5": 130,
    "Keras 模型": 131,
    "Keras模型": 132,
    "Keras的模型定义方式": 133,
    "LED": 134,
    "LED灯": 135,
    "LED的控制引脚": 136,
    "LOW电平": 137,
    "Layers API": 138,
    "Layers API 和 Core API": 139,
    "Layers API和Core API": 140,
    "Linux": 141,
    "Linux 开发环境": 142,
    "Linux开发环境": 143,
    "MIPI": 144,
    "MIPI CSI-2": 145,
    "MIPI的相机串行接口（CSI）端口": 146,
    "MIPI联盟发起的为移动应用处理器制定的开放标准": 147,
    "MNIST 分类器": 148,
    "MNIST数据集": 149,
    "MaxPooling2D": 150,
    "Micro USB 接口": 151,
    "MnistData": 152,
    "MobileNet": 153,
    "MobileNet V2": 154,
    "MobileNet 图像分类": 155,
    "MobileNetV2": 156,
    "MobileNets_v2": 157,
    "NVIDIA Jetson Nano": 158,
    "NVIDIA Jetson Nano 开发板": 159,
    "NVIDIA Maxwell GPU": 160,
    "NVIDIA Maxwell 架构的 GPU": 161,
    "NVIDIA TensorRT": 162,
    "Object C": 163,
    "OpenCV": 164,
    "OpenCV 安装": 165,
    "OpenCV 安装脚本": 166,
    "OpenCV 编译": 167,
    "OperatorCode": 168,
    "PCB板上针脚的物理位置对应的编号（1~40）": 169,
    "PWM接口": 170,
    "Parcel": 171,
    "Pi 4B": 172,
    "Pi v3+安装包是不同的": 173,
    "PoseNet": 174,
    "PoseNet模型": 175,
    "PoseNet示例应用": 176,
    "PoseNet示例应用程序": 177,
    "Post training quantization": 178,
    "Python": 179,
    "Python 2.7": 180,
    "Python API": 181,
    "Python API 和命令行工具": 182,
    "Python GPIO编程": 183,
    "Python 模块": 184,
    "Python 源码包": 185,
    "Python 相关程序模块会拷贝到/opt/python": 186,
    "Python安装": 187,
    "Python开发环境": 188,
    "Python程序": 189,
    "Quantization-aware training": 190,
    "RPI.GPIO库": 191,
    "RandomFlip": 192,
    "RandomRotation": 193,
    "Raspberry Camera V2": 194,
    "Raspberry Pi 4": 195,
    "Raspberry Pi、Arducam等常见的相机模块": 196,
    "Raspbian软件仓库镜像": 197,
    "ResizeOp": 198,
    "ResizeWithCropOrPadOp": 199,
    "Rot90Op": 200,
    "SD Memory Card Formatter": 201,
    "SPI库": 202,
    "SPI接口（MISO、MOSI、CLK、CS片选信号SPICE0_N）": 203,
    "SavedModel": 204,
    "SavedModel 和 Keras Sequential": 205,
    "SavedModel 和 Keras Sequential 两种模型导出方法和格式": 206,
    "SavedModel 格式": 207,
    "SavedModel的目录路径": 208,
    "SparseCategoricalCrossentropy": 209,
    "Swift": 210,
    "Swift 和 Objective-C 编写的原生库": 211,
    "SymbolicTensor": 212,
    "TF Mobile": 213,
    "TFLite": 214,
    "TFLite interpreter": 215,
    "TFLite model": 216,
    "TFLite 文件格式": 217,
    "TFLite 模型文件": 218,
    "TFLite 模型文件格式": 219,
    "TFLite 模型转换器": 220,
    "TFLite 模型转换过程": 221,
    "TFLite 算子库": 222,
    "TFLite 解释器": 223,
    "TFLite 解释执行器": 224,
    "TFLite 转换器": 225,
    "TFLiteConverter": 226,
    "TFLiteConverter.from_concrete_functions()": 227,
    "TFLiteConverter.from_keras_model()": 228,
    "TFLiteConverter.from_saved_model()": 229,
    "TFLite模型": 230,
    "TFLite解释器tflite和GPU代理gpuDelegate": 231,
    "TFMini": 232,
    "TensorBoard": 233,
    "TensorFLow-vis": 234,
    "TensorFlow": 235,
    "TensorFlow 2.x 模型": 236,
    "TensorFlow GPU 版本": 237,
    "TensorFlow Hub": 238,
    "TensorFlow Hub 上可搜索到的模型之一": 239,
    "TensorFlow Lite": 240,
    "TensorFlow Lite AAR": 241,
    "TensorFlow Lite API": 242,
    "TensorFlow Lite 工作流程": 243,
    "TensorFlow Lite 开发工作流程": 244,
    "TensorFlow Lite 推理": 245,
    "TensorFlow Lite 支持库": 246,
    "TensorFlow Lite 模型": 247,
    "TensorFlow Lite 模型文件": 248,
    "TensorFlow Lite 的工作流程": 249,
    "TensorFlow Lite 示例": 250,
    "TensorFlow Lite 算子库": 251,
    "TensorFlow Lite 解释器": 252,
    "TensorFlow Lite 解释器(Interpreter)": 253,
    "TensorFlow Lite 解释器(Interpreter)、TensorFlow Lite 转换器(Converter)、算子库(Op kernels)、硬件加速代理(Hardware accelerator delegate)": 254,
    "TensorFlow Lite 转换器": 255,
    "TensorFlow Lite 转换器(Converter)": 256,
    "TensorFlow Lite 转换器命令行工具": 257,
    "TensorFlow Lite支持库": 258,
    "TensorFlow Lite模型": 259,
    "TensorFlow Lite解释器": 260,
    "TensorFlow 官网": 261,
    "TensorFlow 模型": 262,
    "TensorFlow 模型导出": 263,
    "TensorFlow 的 JavaScript 版本": 264,
    "TensorFlow.js": 265,
    "TensorFlow.js 中的中心数据单元，是一维或多维数组": 266,
    "TensorFlow.js 模块": 267,
    "TensorFlow.js 进行浏览器可视化的一组实用工具库": 268,
    "TensorFlow.js预训练模型": 269,
    "TensorFlow，PyTorch，Caffe / Caffe2，Keras，MXNet 等深度学习框架": 270,
    "TensorLabel": 271,
    "Tensorflow Lite post-training quantization": 272,
    "Tensorflow.js": 273,
    "Total params: 2,626,821, Trainable params: 368,837, Non-trainable params: 2,257,984": 274,
    "UART串口接口（TXD、RXD）": 275,
    "UART库": 276,
    "USB 接口": 277,
    "USB摄像头": 278,
    "VNC": 279,
    "VNC Viewer": 280,
    "VNC服务器软件": 281,
    "Web Server for Chrome": 282,
    "Webpack": 283,
    "Wiring Pi": 284,
    "Wiring Pi编号": 285,
    "Wiring Pi编号模式": 286,
    "XML文件，描述人体各个部位的Haar特征值": 287,
    "aaptOptions": 288,
    "accuracy": 289,
    "add_event_detect()函数": 290,
    "allocate_tensors()": 291,
    "base_model, Conv2D, Dropout, GlobalAveragePooling2D, Dense": 292,
    "batchSize": 293,
    "batchSize设置为512": 294,
    "batch_size": 295,
    "batch_size用于设置每批图像数量": 296,
    "bool": 297,
    "build.gradle": 298,
    "buildscript": 299,
    "callback": 300,
    "callbacks设置为fitCallbacks": 301,
    "categorical_crossentropy": 302,
    "cdn.jsdelivr.net": 303,
    "class_names": 304,
    "cnn模型": 305,
    "compare_faces": 306,
    "conv2d, dropout, global_average_pooling2d, dense": 307,
    "convertToTensor函数": 308,
    "converter.convert": 309,
    "createModel()": 310,
    "daisy, dandelion, roses, sunflowers, tulips": 311,
    "data.js": 312,
    "data.js文件": 313,
    "dependencies": 314,
    "dispose": 315,
    "dispose和tf.tidy两种内存管理方法": 316,
    "dlib库": 317,
    "doPrediction函数": 318,
    "epochs": 319,
    "epochs设置为10": 320,
    "face_cascade.detectMultiScale": 321,
    "face_distance": 322,
    "face_encodings": 323,
    "face_locations": 324,
    "face_recognition": 325,
    "fine_tune": 326,
    "fine_tune_at": 327,
    "finish 目录": 328,
    "flow_from_directory": 329,
    "flower_classification 项目": 330,
    "from_saved_model(), from_keras_model(), from_concrete_functions()": 331,
    "from_saved_model方法": 332,
    "functional模型": 333,
    "get_input_details()": 334,
    "get_output_details()": 335,
    "get_tensor()": 336,
    "gpio readall命令": 337,
    "hog模型": 338,
    "hub.KerasLayer": 339,
    "iOS": 340,
    "iOS 开发人员": 341,
    "iOS 开发人员使用": 342,
    "iOS 开发者通过 CocoaPods 获取": 343,
    "images和labels": 344,
    "include_top=False": 345,
    "index": 346,
    "index.html": 347,
    "index.js": 348,
    "inputShape为[1]，units为1，useBias为true": 349,
    "interpreter": 350,
    "interpreter.get_tensor()": 351,
    "invoke()": 352,
    "jtop 命令": 353,
    "jupyter": 354,
    "jupyter_notebook_config.py": 355,
    "labels.txt": 356,
    "layers.Flatten()": 357,
    "load_image_file、face_locations、face_encodings、compare_faces、face_distance": 358,
    "loss": 359,
    "make install": 360,
    "make 命令": 361,
    "metrics": 362,
    "microSD 卡插槽": 363,
    "microSD 卡用作启动设备和主存储器": 364,
    "microSD卡": 365,
    "microSD卡插槽": 366,
    "minNeighbors": 367,
    "mnist项目": 368,
    "mobilenetv2_1.00_224": 369,
    "model": 370,
    "model.compile": 371,
    "model.fit": 372,
    "model.fit()": 373,
    "model.predict()": 374,
    "model.tflite和label.txt": 375,
    "model.tflite文件": 376,
    "model.trainable = False": 377,
    "model可以是'hog'或'cnn'": 378,
    "nano或vi": 379,
    "nextTestBatch": 380,
    "nextTrainBatch": 381,
    "nextTrainBatch 和 nextTestBatch 方法": 382,
    "noCompress \"tflite\"": 383,
    "normalization_layer": 384,
    "np.argmax()": 385,
    "number_of_times_to_upsample设置对图像进行多少次上采样以查找人脸": 386,
    "optimizer": 387,
    "optimizer, loss, metrics": 388,
    "optimizer='sgd', loss='mean_squared_error'": 389,
    "org.tensorflow:tensorflow-lite:+": 390,
    "output_details": 391,
    "package.json": 392,
    "pip": 393,
    "pip install": 394,
    "pip3": 395,
    "predict()": 396,
    "print(\"button pressed!\")": 397,
    "python": 398,
    "repositories和dependencies": 399,
    "resize原始图像到模型输入大小": 400,
    "saved_model_dir": 401,
    "sequential模型": 402,
    "sequential模型和functional模型": 403,
    "set_tensor()": 404,
    "showAccuracy函数": 405,
    "showConfusion函数": 406,
    "shuffle": 407,
    "shuffle设置为true": 408,
    "softmax": 409,
    "start 目录": 410,
    "start 目录和 finish 目录": 411,
    "string": 412,
    "target_size用于设置图像大小": 413,
    "tensor()": 414,
    "tf.image.resize": 415,
    "tf.keras model": 416,
    "tf.keras.Sequential": 417,
    "tf.keras.callbacks.TensorBoard": 418,
    "tf.keras.layers.Dense": 419,
    "tf.keras.losses.SparseCategoricalCrossentropy": 420,
    "tf.keras.models.Sequential": 421,
    "tf.keras.optimizers.Adam": 422,
    "tf.keras.optimizers.Adam()": 423,
    "tf.linspace()": 424,
    "tf.lite.TFLiteConverter": 425,
    "tf.lite.TFLiteConverter.from_keras_model": 426,
    "tf.lite.TFLiteConverter.from_saved_model": 427,
    "tf.losses.meanSquaredError": 428,
    "tf.model()": 429,
    "tf.nn.softmax": 430,
    "tf.nn.softmax()": 431,
    "tf.saved_model.save": 432,
    "tf.sequential()": 433,
    "tf.sequential()和tf.model()两种创建模型的方式": 434,
    "tf.tensor 函数": 435,
    "tf.tidy": 436,
    "tf.tidy()": 437,
    "tf.train.adam()": 438,
    "tfjs-examples/mnist": 439,
    "tflite_convert": 440,
    "tflite_convert --keras_model_file=/tmp/mobilenet_keras_model.h5 --output_file=/tmp/mobilenet.tflite": 441,
    "tflite_convert --saved_model_dir=/tmp/mobilenet_saved_model --output_file=/tmp/mobilenet.tflite": 442,
    "tflite_model": 443,
    "tfvis.render.scatterplot": 444,
    "tfvis.show.modelSummary": 445,
    "time.sleep()": 446,
    "train_ds, validation_data=val_ds, epochs=NUM_EPOCHS, callbacks=tensorboard_callback": 447,
    "train_generator": 448,
    "train_generator, steps_per_epoch=len(train_generator), epochs=epochs, validation_data=val_generator, validation_steps=len(val_generator)": 449,
    "ui.js": 450,
    "units=1": 451,
    "units=1, input_shape=[1]": 452,
    "units=16, activation='relu'": 453,
    "units为1，useBias为true": 454,
    "useBias": 455,
    "val_ds": 456,
    "val_ds.map": 457,
    "val_generator": 458,
    "validationData设置为[testXs, testYs]": 459,
    "vi": 460,
    "vino": 461,
    "wait_for_edge()函数": 462,
    "wait_for_edge()函数和add_event_detect()函数": 463,
    "yarn": 464,
    "一个使用数据流图进行数值计算的开源软件库": 465,
    "一个多媒体框架，用于后端处理任务，如格式修改、显示驱动程序协调和数据处理": 466,
    "一个强大、简单、易上手的人脸识别开源项目": 467,
    "一个端到端的机器学习开源框架": 468,
    "一个轻量、快速、兼容度高的专门针对移动式应用场景的深度学习工具": 469,
    "一种数据结构，包含了在解决一个特定问题时，训练得到的机器学习网络的逻辑和知识": 470,
    "一种有效的物品检测方法": 471,
    "一种特定的矩阵用来呈现算法性能的可视化效果": 472,
    "一系列函数，以张量作为输入并输出另一个张量": 473,
    "一系列的计算节点、多个张量，以及子图本身的输入和输出": 474,
    "一组工具，可帮助开发者在移动设备、嵌入式设备和 IoT 设备上运行 TensorFlow 模型": 475,
    "一组数字引脚，可用于将树莓派连接到其他电子设备": 476,
    "上拉电阻": 477,
    "下载 OpenCV": 478,
    "下载和访问mnist数据集": 479,
    "下载源代码使用GIT工具下载代码，然后编译安装": 480,
    "下载源码": 481,
    "下载特定版本的Python": 482,
    "下载解压": 483,
    "不包括顶层分类层": 484,
    "不同形式的模型，包括 Keras Model 和 SavedModel": 485,
    "不支持 CUDA": 486,
    "不改变基础模型的各项参数变量": 487,
    "不清除内部函数的返回值": 488,
    "不需要原始模型构建代码": 489,
    "不需要原始模型构建代码就可以运行": 490,
    "不需要原有模型中最后的神经网络层": 491,
    "不需要原有模型中最后的神经网络层（分类到1000类）": 492,
    "不需要序列化或可以创造自己的序列化方法": 493,
    "不需要改变模型，最少情况只需多加一行代码": 494,
    "与 TensorFlow 一起安装": 495,
    "与树莓派结合可以将项目与现实世界轻松的联系起来": 496,
    "串联在LED和电源之间限制电流": 497,
    "为了高性能场景创建的序列化库": 498,
    "主要应用于游戏场景": 499,
    "主要应用于游戏场景，是为了高性能场景创建的序列化库": 500,
    "了解模型效率、调试超参数": 501,
    "二进制文件很小": 502,
    "二进制文件很小，支持设备端机器学习推断，延迟较低": 503,
    "人体姿势估计": 504,
    "人脸检测": 505,
    "人脸检测、检测面部特征点、给脸部编码、从编码中找出人的名字": 506,
    "人脸识别": 507,
    "人脸识别与商品识别": 508,
    "仅用于开发的程序包": 509,
    "仅适用于卷积神经网络的一个子集": 510,
    "从 MNIST 数据集中随机批量提取 MNIST 图像": 511,
    "从 Python 官网下载": 512,
    "从 github 下载源码": 513,
    "从头自己编译": 514,
    "从指定层开始进行模型微调": 515,
    "从指定目录加载训练图像数据": 516,
    "从指定目录加载验证图像数据": 517,
    "从测试集中返回一批图像及其标签": 518,
    "从训练集中返回一批随机图像及其标签": 519,
    "以 ldconfig 结束": 520,
    "以依赖项的安装开始": 521,
    "以最小精度下降来训练网络": 522,
    "优化器Adam": 523,
    "优化模型": 524,
    "优化模型大小和性能": 525,
    "优化模型性能": 526,
    "优化的 FlatBuffer 格式": 527,
    "优化的 FlatBuffer 格式，以 .tflite 为文件扩展名": 528,
    "优化的算子库": 529,
    "低功耗": 530,
    "作为functional模型第一层的输入": 531,
    "作为判断训练结果的参数": 532,
    "作为损失函数用于多分类任务": 533,
    "作为最终的 Dense 层的激活函数": 534,
    "作为最终的Dense层激活函数进行多分类": 535,
    "作为模型优化算法": 536,
    "作为模型的优化器": 537,
    "作为模型的损失函数": 538,
    "作为迁移学习的基础模型": 539,
    "使权重和激活值的 Post training 更简单": 540,
    "使用 CocoaPods for Swift or Objective-C": 541,
    "使用 Python API 进行转换": 542,
    "使用 SavedModel 格式存储": 543,
    "使用 TFLite 模型转换器转换成 TFLite 文件格式(FlatBuffers 格式)": 544,
    "使用 TensorFlow Lite AAR": 545,
    "使用 TensorFlow Lite 解释器（提供多种语言的 API）在设备端运行模型": 546,
    "使用 TensorFlow Lite 转换器将模型转换为 TensorFlow Lite 格式": 547,
    "使用 TensorFlow.js 进行训练模型的基本流程与概念和语法": 548,
    "使用3×3的卷积核，并在输出上使用 Relu 激活函数": 549,
    "使用GPU来加速数学运算": 550,
    "使用TensorFlow.js实现根据摄像头采集的手势图像确定剪刀、石头、布": 551,
    "使用lambda函数对images进行归一化处理并保留labels": 552,
    "使用min-max缩放方法": 553,
    "使用tf.model() API创建非闭环的计算图": 554,
    "使用tf.saved_model.save函数": 555,
    "使用低学习率编译模型": 556,
    "使用低学习率重新编译模型": 557,
    "使用命令行 tflite_convert 进行转换": 558,
    "使用层构建模型": 559,
    "使用已编译好的库": 560,
    "使用模型优化工具包缩减模型的大小并提高其效率，同时最大限度地降低对准确率的影响": 561,
    "使用测试数据集评估模型": 562,
    "使用深度可分离的卷积": 563,
    "使用深度可分离的卷积来构建轻量级的深层神经网络": 564,
    "使这些专用功能适应新数据集，而不是覆盖通用学习": 565,
    "保存模型": 566,
    "保存训练数据的类别标签": 567,
    "保护LED和GPIO引脚": 568,
    "保持了很多通用性": 569,
    "保留原来大规模训练的优势": 570,
    "修改配置文件": 571,
    "借助低级运算（例如 tf.matMul()、tf.add() 等）创建机器学习模型": 572,
    "做了移动设备相关的优化": 573,
    "允许解释器在设备的 GPU 上运行适当的运算符": 574,
    "全能 IDE": 575,
    "全连接 (Full Connected) 层": 576,
    "全连接层": 577,
    "全连接网络": 578,
    "关联概率与类别标签": 579,
    "关闭LED灯": 580,
    "其他语言包（如python、ruby或PHP）": 581,
    "具有 shape 属性定义其数组形状": 582,
    "兼容度高": 583,
    "内存回收问题突出": 584,
    "内存高效": 585,
    "内存高效，支持将文件映射到内存中，然后直接进行读取和解释": 586,
    "冻结前100层": 587,
    "冻结预训练模型并更新分类器权重": 588,
    "冻结预训练的模型，仅在训练期间更新分类器的权重": 589,
    "准备训练集和验证集": 590,
    "减少内存碎片化": 591,
    "分类器": 592,
    "分类结果的概率": 593,
    "创建 Tensor 实例的主要构造函数": 594,
    "创建0~1之间平均分配的100个值": 595,
    "创建Python包的软链接": 596,
    "创建TFLite转换器实例": 597,
    "创建、训练和导出自定义 TensorFlow Lite 模型": 598,
    "创建了一个安装脚本": 599,
    "创建任意非闭环的计算图": 600,
    "创建位图对象": 601,
    "初始化TensorFlow Lite解释器": 602,
    "利用 Android 神经网络 API（Android NN API)": 603,
    "利用手机上的加速器，比如 GPU 或者 DSP": 604,
    "利用计算机对图像进行处理、分析和理解，以识别各种不同模式的目标和对象的技术": 605,
    "前几层学习非常简单和通用的功能，这些功能可以推广到几乎所有类型的图像": 606,
    "功耗非常低，有两种模式：5W（低功耗模式；可以使用 USB 口供电）和10W（必须使用 Power Jack 外接5V 电源供电）": 607,
    "功能强大的编程语言，易于使用，易于阅读和编写": 608,
    "功能强大，交互式、富文本，还有丰富的插件、主题修改、多语言支持": 609,
    "功能接线的引脚号（如TXD、PWM0等）": 610,
    "加载 @tensorflow/tfjs 和 @tensorflow/tfjs-vis 两个 TFJS 模块的代码": 611,
    "加载和运行TFLite模型": 612,
    "加载数据并准备进行训练": 613,
    "加载数据，定义模型，训练循环并指定UI元素": 614,
    "加载模型": 615,
    "加载模型、分配张量、设置输入、执行推理、读取输出": 616,
    "加速模型推理过程": 617,
    "动态显示训练的过程": 618,
    "包含一个完整的TensorFlow程序，不仅包含权重值，还包含计算": 619,
    "包含完整的TensorFlow程序（权重和计算）": 620,
    "包含完整的TensorFlow程序，包括权重和计算": 621,
    "千兆以太网端口": 622,
    "单个图像的维度为[28,28,1]": 623,
    "占用更少的磁盘和内存，更快更高效": 624,
    "卷积层": 625,
    "卷积层与全连接层": 626,
    "卷积层和全连接层": 627,
    "卷积层输入": 628,
    "卷积神经网络": 629,
    "取消冻结模型的顶层": 630,
    "受限于GPU内存的大小": 631,
    "只使用在C语言中": 632,
    "只提供了基本的转化功能": 633,
    "可以使用自己的 TensorFlow 模型、在线查找模型，或者从的 TensorFlow 预训练模型中选择一个模型直接使用或重新训练": 634,
    "可以利用手机上的加速器，比如 GPU 或者 DSP 等": 635,
    "可以增加自己的输出层": 636,
    "可以是内置的算子或自定制算子，有一个名字": 637,
    "可以添加--no-cache-dir参数来避免缓存问题": 638,
    "可以直接使用cv2.videocapture打开": 639,
    "可以直接部署或用于迁移学习": 640,
    "可以设置访问密码增强安全性": 641,
    "可以调用不同的硬件加速器比如 GPU 进行执行": 642,
    "可以通过软件编程进行控制": 643,
    "可以配置为输入或输出": 644,
    "可在手机、嵌入式 Linux 设备和微控制器等很多不同类型的硬件上运行": 645,
    "可在手机、嵌入式 Linux 设备和微控制器等很多不同类型的硬件上运行经过专门优化的模型": 646,
    "可能出现OOM错误": 647,
    "可能存在过度拟合": 648,
    "可能导致多次输出": 649,
    "可能导致模型过拟合": 650,
    "可视化模型训练的过程和结果": 651,
    "可视化模型预测结果和原始数据": 652,
    "可配置为输入或输出": 653,
    "启动一个显示窗口，并确认可以从摄像头看到实时流": 654,
    "启动图标": 655,
    "命令行 TensorFlow Lite 转换器命令行工具": 656,
    "命令行与 Python API": 657,
    "命令行工具": 658,
    "命令行工具gpio": 659,
    "命令行工具和 Python API": 660,
    "商品流通过程中，特别是无人货架、智能零售柜等无人零售领域": 661,
    "商品识别": 662,
    "四引脚按键": 663,
    "四脚按键开关": 664,
    "回调函数": 665,
    "图像、文本和语音处理": 666,
    "图像分类、对象检测、姿势估计、文本恶意检测": 667,
    "图像分类任务": 668,
    "图像识别": 669,
    "图像识别技术": 670,
    "图像识别项目": 671,
    "图像预处理": 672,
    "在 Android 与 iOS 平台上使用": 673,
    "在18号引脚处设置": 674,
    "在Jetson开发工具包上运行预训练模型": 675,
    "在Shell脚本中控制GPIO管脚": 676,
    "在不同设备上使用硬件加速": 677,
    "在内存有限的移动环境中使用": 678,
    "在图像中检测面部，如果检测到面部会返回面部所在的矩形区域Rect(x,y,w,h)": 679,
    "在图像分类、物体检测、分割和语音处理等应用程序中并行运行多个神经网络": 680,
    "在大规模数据处理上不如Python高效": 681,
    "在官网下载安装包后安装": 682,
    "在小型数据集上训练模型": 683,
    "在展平操作之前依赖于最后一层": 684,
    "在把模型嵌入到实际的二进制文件中时兼容性存在较大挑战": 685,
    "在有 GPU 加速的手机上运行 MobileNet 图像分类，模型运行速度可以提高": 686,
    "在有 GPU 加速的手机上运行，模型运行速度可以提高 5.5 倍": 687,
    "在每一个训练周期显示训练情况": 688,
    "在浏览器中训练模型": 689,
    "在生产环境中不需要": 690,
    "在硬件加速层面，对于 CPU 利用了 ARM 的 NEON 指令集做了大量的优化": 691,
    "在移动和嵌入式设备上执行难度较大": 692,
    "在移动端（mobile）、嵌入式（embeded）和物联网（IoT）设备上运行 TensorFlow 模型": 693,
    "在移动设备、嵌入式设备和 IoT 设备上运行 TensorFlow 模型": 694,
    "在移动设备和嵌入式设备上运行TensorFlow模型": 695,
    "在给定设备上实现性能、模型大小和准确性的理想平衡": 696,
    "在训练过程中不能用于训练": 697,
    "在训练顶层分类器并将预先训练的模型设置为不可训练之后": 698,
    "在设备端运行 TFLite 模型": 699,
    "在设备端运行模型": 700,
    "在资源有限的硬件上运行": 701,
    "在边缘设备上运行 TensorFlow 模型推理": 702,
    "在边缘设备上运行 TensorFlow 模型推理的官方框架": 703,
    "在需要场景中更关心的对象": 704,
    "在靠近物或数据源头的一侧，采用网络、计算、存储、应用核心能力为一体的开放平台，就近提供最近端服务": 705,
    "地": 706,
    "基于 TF Mobile 的经验，继承了 TFMini 和内部其他类似项目的优秀工作": 707,
    "基于XML文件": 708,
    "基于流线型架构的轻量级深层神经网络": 709,
    "基于现有模型构建 Interpreter": 710,
    "基于现有的模型进行继续训练": 711,
    "基础模型的各项参数变量不会被新的训练修改数据": 712,
    "增加一个事件的检测函数": 713,
    "处理多媒体应用程序": 714,
    "处理简单的数据": 715,
    "多种级别的量化支持": 716,
    "多种编程语言": 717,
    "大电流可能损坏LED和供电设备": 718,
    "大而复杂的模型": 719,
    "头信息和脚本加载部分": 720,
    "如果仅使用支持常见图像分类模型（InceptionV3 和 MobileNet）所需的运算符，二进制文件的大小不到 300 KB": 721,
    "如果联合训练所有层，则梯度更新的幅度将太大，并且预训练模型将忘记它学到的东西": 722,
    "子图": 723,
    "子图、算子库和共享的内存缓冲区": 724,
    "存储Jetson系统镜像": 725,
    "存储已安装软件包的名称和版本": 726,
    "存储模型权重或计算节点的输入和输出": 727,
    "存放 Python 相关程序模块": 728,
    "存放训练好的模型供开发人员复用": 729,
    "学习AI和构建应用程序": 730,
    "安全检查、身份核验与移动支付": 731,
    "安卓应用只需 1 兆左右的运行环境": 732,
    "安卓应用只需 1 兆左右的运行环境，在 MCU 上甚至可以小于 100KB": 733,
    "安装 Android Studio": 734,
    "安装 OpenCV": 735,
    "安装 Python 包依赖项": 736,
    "安装 TensorFlow": 737,
    "安装 TensorFlow 所需的系统包": 738,
    "安装Android Studio": 739,
    "安装Python": 740,
    "安装Python包": 741,
    "安装依赖": 742,
    "安装依赖项": 743,
    "安装和升级 pip3": 744,
    "安装的 TensorFlow 版本必须与正在使用的 JetPack 版本一致": 745,
    "安装相应的依赖包": 746,
    "完全基于 JavaScript 从头开发、训练和部署模型": 747,
    "完成分类": 748,
    "完成分类任务": 749,
    "官方已经停止维护": 750,
    "官方推荐使用的无线网卡": 751,
    "官方集成到Python的工具": 752,
    "定义模型结构": 753,
    "定义的神经元网络层与层之间的关系较为随意": 754,
    "定义需要监视的指标": 755,
    "实例化预先训练的模型，并在顶部添加全连接的分类器": 756,
    "实时识别照相机所拍摄的花卉": 757,
    "实现了一组优化的算子内核": 758,
    "实现人体姿势估计": 759,
    "实现刷脸登录功能": 760,
    "实现花卉识别 app": 761,
    "对 SIMD 指令功能特别有益": 762,
    "对images进行归一化处理": 763,
    "对手写数字的图像进行分类": 764,
    "对数据降维": 765,
    "对模型的权重产生更一致且变化较小的渐变更新": 766,
    "对测试数据进行预测并返回预测结果和真实标签": 767,
    "对现有 CPU 平台的支持": 768,
    "对训练图像随机变换来人为引入样本多样性": 769,
    "对训练图像随机旋转以增加数据多样性": 770,
    "对训练图像随机水平翻转以增加数据多样性": 771,
    "导致每个时期的梯度更新数量较少": 772,
    "将 NPM 模块转换为在线可以引用的免费服务": 773,
    "将 TensorFlow 模型格式转换为 TensorFlow Lite 的 Python API": 774,
    "将 TensorFlow 模型转换为 TFLite 文件格式(FlatBuffers 格式)": 775,
    "将 TensorFlow 模型转换为 TFLite 格式": 776,
    "将 TensorFlow 模型转换为 TensorFlow Lite 专用的模型文件格式": 777,
    "将 TensorFlow 模型转换为方便解释器使用的格式": 778,
    "将 TensorFlow 模型转换为方便解释器使用的格式，并可引入优化以减小二进制文件的大小和提高性能": 779,
    "将Keras模型转换为TFLite模型": 780,
    "将SavedModel转换为TFLite模型": 781,
    "将SavedModel转换为TensorFlow Lite格式": 782,
    "将TensorFlow模型转换为移动设备兼容格式": 783,
    "将maven源google()和jcenter()替换为国内镜像": 784,
    "将三维张量展开到1维": 785,
    "将三维张量展开到1维以便传入Dense层": 786,
    "将人脸编码列表与候选编码进行比较以查看它们是否匹配": 787,
    "将原始数据转变为TensorFlow可读的张量格式": 788,
    "将图片分类到1000类": 789,
    "将外设操作视为文件读写": 790,
    "将大规模内存操作放置在其回调中执行": 791,
    "将层按顺序写在列表里作为sequential()函数的输入": 792,
    "将彩色图像转换为灰度图像，检测人脸，在边界周围绘制矩形": 793,
    "将模型保存为TFLite兼容格式": 794,
    "将模型加载到内存中": 795,
    "将模型嵌入到二进制文件中，可以在设备上运行和部署模型": 796,
    "将模型显示在浏览器中": 797,
    "将模型直接映射到内存中，同时有一个静态执行计划": 798,
    "将模型输出转换为概率分布": 799,
    "将特征转换为每个图像对应一个1280元素向量": 800,
    "将网络的每一层简单的叠在一起": 801,
    "将自定义模型转换为 TensorFlow Lite 格式": 802,
    "将输入数据转换成模型接收的形式或排布": 803,
    "将输入的 TensorFlow 模型生成 TensorFlow Lite 模型": 804,
    "将镜像写入microSD卡": 805,
    "小一点的模型": 806,
    "工业物联智能设备的开发": 807,
    "已经训练好的分类器，其中包括面部，眼睛，微笑等": 808,
    "常开触点": 809,
    "常见的移动/嵌入式平台": 810,
    "常闭触点": 811,
    "应对快速变化需求的软件开发模式": 812,
    "应用于树莓派的GPIO控制库函数": 813,
    "应用深度学习算法的一种实践应用": 814,
    "应用程序在边缘侧发起，产生更快的网络服务响应，满足行业在实时业务、应用智能、安全与隐私保护等方面的基本需求": 815,
    "廉价且周边设备多": 816,
    "延迟一秒钟": 817,
    "延迟较低": 818,
    "开关去抖": 819,
    "开关抖动": 820,
    "开发依赖": 821,
    "引入优化以减小二进制文件的大小和提高性能": 822,
    "引用 Model 的内存缓冲区的一片区域，提高内存效率": 823,
    "张量": 824,
    "张量(Tensor)": 825,
    "张量形状是 (image_height, image_width, color_channels)": 826,
    "张量（Tensor）": 827,
    "归一化操作": 828,
    "当压力施压时电路接通": 829,
    "当撤销压力时电路恢复": 830,
    "形状是 (224,224, 3)": 831,
    "微调": 832,
    "微调少量顶层而不是整个 MobileNet 模型": 833,
    "微调结果": 834,
    "微调过程": 835,
    "必须在开机前先装上去，系统才能识别": 836,
    "快速": 837,
    "快速启动": 838,
    "快速启动深度学习推理演示": 839,
    "忽略由于开关抖动引起的小于": 840,
    "恢复训练": 841,
    "手写数字识别": 842,
    "手势识别项目": 843,
    "打乱数据顺序，创建特征向量和标签向量，转换为张量格式，进行归一化操作": 844,
    "打开现有 Android Studio 项目": 845,
    "打开项目图标": 846,
    "执行 TensorFlow Lite 推理": 847,
    "执行一个函数并清除所有创建的中间张量，释放它们的GPU内存": 848,
    "执行推理": 849,
    "执行模型推理": 850,
    "执行模型推理过程": 851,
    "执行模型文件在输入数据上定义的运算符，输出推理结果": 852,
    "执行模型转换过程": 853,
    "批次大小": 854,
    "指定从哪个层开始进行微调的参数": 855,
    "指定引脚编号系统": 856,
    "损失函数categorical_crossentropy": 857,
    "接受 TFLite 模型": 858,
    "接收汽车的功率作为输入": 859,
    "控制GPIO引脚": 860,
    "控制LED灯的亮暗": 861,
    "控制外部硬件设备": 862,
    "控制树莓派GPIO引脚": 863,
    "控制树莓派的GPIO": 864,
    "推理过程": 865,
    "描述构建和运行示例所需的依赖项": 866,
    "提供了一个简单的 API，用于从 Java、Swift、Objective-C、C++ 和 Python 运行 TensorFlow Lite 模型": 867,
    "提供低级的机器学习构建模块和高级的类似 Keras 的 API 来构建神经网络": 868,
    "提供已经训练好且经过充分认证的模型": 869,
    "提供权重初始化、模型序列化、训练监测、可迁移性和安全检查等工具": 870,
    "提供转换工具压缩模型，进行算子融合并生成代码": 871,
    "提供预训练模型，开发人员可以复用这些已经训练好且经过充分认证的模型": 872,
    "提高性能的方法是训练预训练模型的顶层的权重以及刚添加的分类器的训练": 873,
    "提高模型准确率": 874,
    "提高模型性能": 875,
    "摄像头接口": 876,
    "摄像头预捕获的图像宽度、高度、窗口显示的图像宽度、高度、捕获帧率、是否旋转图像": 877,
    "支持 GPU 硬件加速": 878,
    "支持像素缩放和数据增强": 879,
    "支持各种环境的部署": 880,
    "支持多种编程语言": 881,
    "支持大规模的模型训练": 882,
    "支持将文件映射到内存中直接读取和解释，不需要额外解析": 883,
    "支持微控制器(MCU)，可以应用于 IoT 领域": 884,
    "支持批量加载图像数据": 885,
    "支持格式修改和数据处理": 886,
    "支持算子优化和常见的编译优化": 887,
    "支持算子优化和常见的编译优化，比如算子融合、常数折叠或无用代码删除等": 888,
    "支持训练后量化": 889,
    "支持设备端机器学习推断": 890,
    "支持设备端机器学习推断，延迟较低，并且二进制文件很小": 891,
    "支持量化原生支持": 892,
    "支持预装驱动程序的RPi相机，可以很容易地用作即插即用外围设备，不需要安装驱动程序": 893,
    "敏捷开发": 894,
    "教育": 895,
    "数据图像的采集、模型的训练、参数的调整、模型文件生成、网页端部署、网络摄像头检查": 896,
    "数据规范化和转换为张量类型": 897,
    "数据转换": 898,
    "数据转换、执行推理、解释输出": 899,
    "数据预处理": 900,
    "整个安装需要两个小时才能完成": 901,
    "旋转图像": 902,
    "是一个线性堆叠layers的模型": 903,
    "普通GPIO口": 904,
    "更准确的深度学习模型需要GPU加速": 905,
    "更换国内源如阿里、清华": 906,
    "更新可视化元素": 907,
    "更谨慎地控制内存何时回收": 908,
    "更适合于边缘设备部署": 909,
    "最后的神经网络层": 910,
    "最大池化层": 911,
    "最新的安卓系统提供了 Android 神经网络 API（Android NN API)，让硬件厂商可以扩展支持这样的接口": 912,
    "有两路CSI相机接口": 913,
    "有助于避免因错误的样本而改向错误的方向": 914,
    "未满足的对等依赖 seedrandom@~": 915,
    "机器学习和计算机视觉应用，如物体检测、人脸识别、图像分割等视觉任务": 916,
    "机身只有 Ethernet 有线网络，不包括无线网卡": 917,
    "权重值": 918,
    "构建CNN模型": 919,
    "构建和训练深度学习模型": 920,
    "构建和运行mnist代码": 921,
    "构建和运行机器学习模型": 922,
    "构建图像处理流程": 923,
    "构建小型移动机器人、人脸签到打卡、口罩识别、智能门锁、智能音箱等复杂 AI 系统": 924,
    "构建工具用于编写更大的程序": 925,
    "构成检测目标的相邻矩形的最小个数": 926,
    "查看图像及其标签": 927,
    "查看开发板系统信息": 928,
    "查看树莓派的GPIO引脚信息": 929,
    "标准算子": 930,
    "树莓派": 931,
    "树莓派 GPIO": 932,
    "树莓派4B的18号引脚": 933,
    "树莓派GPIO": 934,
    "树莓派接口": 935,
    "树莓派摄像头": 936,
    "树莓派的21号引脚": 937,
    "树莓派的官方编程语言": 938,
    "树莓派系统": 939,
    "树莓派通用输入/输出接口（GPIO）": 940,
    "核心板可拆的设计，核心板的大小只有70 x 45 mm": 941,
    "核心运行时": 942,
    "格式化microSD卡": 943,
    "检查摄像头信息": 944,
    "检测人脸": 945,
    "检测关键身体部位的位置": 946,
    "模型": 947,
    "模型优化": 948,
    "模型优化工具包": 949,
    "模型大小只有20 KB 左右": 950,
    "模型执行流图": 951,
    "模型推理": 952,
    "模型文件和标签文件": 953,
    "模型的计算节点": 954,
    "模型精度达到98%": 955,
    "模型编译": 956,
    "模型评估": 957,
    "模型部署": 958,
    "模型预测": 959,
    "比 CPU 执行更快的浮点矩阵运算": 960,
    "池化层": 961,
    "注重实时性，内存高效": 962,
    "测试Python开发环境并查看当前的Python版本": 963,
    "测试数据": 964,
    "测试的软件包、webpack或Babel": 965,
    "混淆矩阵": 966,
    "清华源": 967,
    "清理GPIO引脚设置": 968,
    "清除张量或变量并释放其GPU内存": 969,
    "清除所有创建的中间张量并释放它们的GPU内存": 970,
    "激活、设置为输出状态、写入1使其输出高电压": 971,
    "灵活的架构可以将模型部署到桌面、服务器或移动设备中的 CPU 或 GPU 上": 972,
    "点亮LED灯": 973,
    "版本变化后 API 函数会改变": 974,
    "版本变化后API函数会改变": 975,
    "物理引脚Broad编号": 976,
    "特征提取": 977,
    "瓶颈层": 978,
    "生成 HDF5 文件的绝对路径目录": 979,
    "生成SavedModel": 980,
    "生成一个批次一个批次的图片，以生成器的形式给模型训练": 981,
    "生成一个批次的图片，以生成器的形式给模型训练": 982,
    "生成批次的图片数据用于模型训练": 983,
    "用于 5V 电源输入": 984,
    "用于 flower_classification 项目": 985,
    "用于加载 TensorFlow Hub 上的模型": 986,
    "用于存储数据的JavaScript文件": 987,
    "用于嵌入式设备，从功耗、体积、价格上也算一个平衡": 988,
    "用于工具的配置中心": 989,
    "用于构建网页的HTML文件": 990,
    "用于特征提取": 991,
    "用于监督学习中以显示多个类别是否有混淆": 992,
    "用于编写JavaScript代码的文件": 993,
    "用于训练模型": 994,
    "用于预测": 995,
    "用到的算子索引和输入输出用到的 Tensor 索引": 996,
    "用户在自己的工作台中使用 TensorFlow API 构造 TensorFlow 模型": 997,
    "用来连接 DP 屏幕": 998,
    "由 schema.fbs 文件使用 FlatBuffers 定义": 999,
    "由jupyter软件自动生成": 1000,
    "由常开触点、常闭触点组合而成": 1001,
    "电信号从低电平到高电平或从高电平到低电平状态的改变": 1002,
    "电阻": 1003,
    "登录 Jetson Nano": 1004,
    "目前有130个左右": 1005,
    "目前有130个左右，它与 TensorFlow 的核心算子库略有不同，并做了移动设备相关的优化": 1006,
    "直接串联3.3V电源会产生非常大的电流": 1007,
    "直接在 Objective-C 代码中使用 C API": 1008,
    "直接更新树莓派系统": 1009,
    "直流桶式插座": 1010,
    "相机模块": 1011,
    "相比 Protocol Buffer 有更高的性能和更小的大小": 1012,
    "硬件加速代理(Hardware accelerator delegate)": 1013,
    "确认 CUDA 已经被正常安装": 1014,
    "神经元权重计算中的偏置量": 1015,
    "移动端及IoT设备端的深度学习技术": 1016,
    "第一个卷积层": 1017,
    "第二、三卷积层": 1018,
    "简化了算子集，缩小了运行库": 1019,
    "简化图像预处理和模型输出处理": 1020,
    "简单的线性回归的实验": 1021,
    "算子实现": 1022,
    "算子库(Op kernels)": 1023,
    "算子融合、常数折叠或无用代码删除": 1024,
    "精度达到98%": 1025,
    "精确度不高的任务": 1026,
    "系统升级": 1027,
    "结果不太准确但在CPU上运行更快": 1028,
    "绘制混淆矩阵以可视化模型性能": 1029,
    "绘制骨架": 1030,
    "给定汽车的功率（Horsepower），预测汽车油耗（MPG）": 1031,
    "编译 OpenCV": 1032,
    "编译 Python 模块": 1033,
    "缩减模型的大小并提高其效率，同时最大限度地降低对准确率的影响": 1034,
    "缩放图像": 1035,
    "缩短开发周期": 1036,
    "网络环境较差时可以考虑更换源": 1037,
    "能够利用各种硬件加速": 1038,
    "花卉数据集中的图片": 1039,
    "花卉识别 app": 1040,
    "花卉识别模型": 1041,
    "获取score中的最大值索引": 1042,
    "获取图像数据、创建位图、调用estimateSinglePose()、绘制骨架": 1043,
    "获取图像数据并转换格式": 1044,
    "获取张量的指针": 1045,
    "获取张量的数据": 1046,
    "观测开关去抖效果": 1047,
    "解决JavaScript内存回收问题": 1048,
    "解决跨域问题": 1049,
    "解释器和转换器": 1050,
    "解释输出": 1051,
    "计算图": 1052,
    "计算已知人脸和未知人脸特征向量的距离": 1053,
    "计算并显示每个类别的准确度": 1054,
    "计算机视觉应用": 1055,
    "计算能力不高，勉强可以使用一些小规模、并且优化过的网络进行推理，训练的话还是不够用的": 1056,
    "计算预测结果的softmax概率": 1057,
    "让输入输出映射到0-1之间，保证后期更有效地训练": 1058,
    "训练分类器": 1059,
    "训练后量化": 1060,
    "训练数据": 1061,
    "训练数据和测试数据": 1062,
    "训练曲线": 1063,
    "训练期间将不更新预训练网络的权重，只在 MobileNet V2基础模型上训练了几层": 1064,
    "训练模型": 1065,
    "训练模型，并监视其性能": 1066,
    "训练集": 1067,
    "训练集和验证集的值由不同的颜色符号显示": 1068,
    "记录训练日志": 1069,
    "记录训练过程中的指标和计算图": 1070,
    "许多 CPU 和硬件加速器实现提供 SIMD 指令功能，对量化特别有益": 1071,
    "设置 OpenCV 的内容、位置和方式": 1072,
    "设置 model.trainable = False": 1073,
    "设置GPIO引脚模式": 1074,
    "设置GPIO管脚": 1075,
    "设置上拉电阻": 1076,
    "设置为32，表示一次采样32条训练数据": 1077,
    "设置为50，表示遍历所有样本50次": 1078,
    "设置为true，表示打乱数据集": 1079,
    "设置前100层为不可训练": 1080,
    "设置引脚为输入或输出模式": 1081,
    "设置输入张量值": 1082,
    "评估准确性": 1083,
    "评估分类器的准确性": 1084,
    "评估指标accuracy": 1085,
    "评估模型": 1086,
    "评估模型的泛化能力": 1087,
    "评估训练有素的模型的性能": 1088,
    "识别图像空间模式": 1089,
    "识别图像里的空间模式，例如线条和物体局部": 1090,
    "识别手写数字": 1091,
    "识别花卉图片": 1092,
    "识别输入图像": 1093,
    "试运行应用": 1094,
    "读写GPIO管脚": 1095,
    "读取传感器数据，控制 LED 等外部设备": 1096,
    "读取输出张量值": 1097,
    "调整图像大小": 1098,
    "调整数据集形状": 1099,
    "调整输入图像大小以匹配模型输入要求": 1100,
    "调整预训练模型的顶层权重": 1101,
    "调整预训练模型的顶层权重，以便模型学习特定于数据集的高级特征": 1102,
    "调用 tf.lite.TFLiteConverter.from_saved_model() 或 TFLiteConverter.from_keras_model()": 1103,
    "调用CSI摄像头和USB摄像头": 1104,
    "调用estimateSinglePose()函数": 1105,
    "调用model.fit方法进行训练": 1106,
    "调用不同的硬件加速器比如 GPU 进行执行": 1107,
    "调用解释器的方式：try (Interpreter interpreter = new Interpreter(tensorflow_lite_model_file)) { interpreter.run(input, output); }": 1108,
    "资源有限，不能训练网络": 1109,
    "跨平台运行，包括 Android、iOS 以及基于 Linux 的 IoT 设备和微控制器": 1110,
    "转换 SavedModel 格式模型": 1111,
    "转换 concrete functions": 1112,
    "转换 tf.keras 模型": 1113,
    "转换Keras模型到TensorFlow Lite模型": 1114,
    "转换、运行 TensorFlow 模型所需的所有工具": 1115,
    "转换为 TensorFlow Lite 模型的代码示例": 1116,
    "转换后得到的TFLite格式模型": 1117,
    "转换后的原模型": 1118,
    "转换数据": 1119,
    "转换模型": 1120,
    "软件PWM库": 1121,
    "软件源配置文件": 1122,
    "软链接": 1123,
    "轻量化": 1124,
    "轻量级": 1125,
    "轻量级、快速启动、内存高效": 1126,
    "轻量，二进制文件的大小约为 1 MB（针对 32 位 ARM build）": 1127,
    "输入层": 1128,
    "输入层和输出层": 1129,
    "输入状态变化": 1130,
    "输出宽度和高度会收缩": 1131,
    "输出层": 1132,
    "输出是一个三维的张量，其形状描述了 (height, width, channels)": 1133,
    "输出模式": 1134,
    "输出油耗": 1135,
    "输出电压约为3.3V": 1136,
    "输出的通道数量取决于声明层时的 filters 参数": 1137,
    "输出的通道数量取决于声明层时的filters参数": 1138,
    "输出通道数为32": 1139,
    "输出通道数为64": 1140,
    "边做边学的理想工具": 1141,
    "边缘": 1142,
    "边缘操作": 1143,
    "边缘计算": 1144,
    "边缘计算设备": 1145,
    "迁移学习": 1146,
    "运行 TensorFlow Lite 模型": 1147,
    "运行Sync Gradle": 1148,
    "运行功率仅为 5 瓦": 1149,
    "运行各种深度学习模型": 1150,
    "运行在 Node.js 或浏览器环境中": 1151,
    "运行已有的 Python 版 TensorFlow 模型": 1152,
    "运行服务监听的IP地址、端口、notebooks内核目录、浏览器开关设置": 1153,
    "运行模型推理": 1154,
    "返回图像中每张人脸的128维人脸编码": 1155,
    "返回张量数据的副本": 1156,
    "返回的是数据的副本而非引用": 1157,
    "进行内存清理工作，防止内存泄露": 1158,
    "进行大量的张量操作时使用可能会很麻烦": 1159,
    "进行迁移学习，实现识别花卉模型": 1160,
    "远程桌面访问Jetson Nano": 1161,
    "连接LED灯和限流电阻到GPIO21和GND": 1162,
    "连接其他电子设备": 1163,
    "连接显示器、键盘和鼠标或通过SSH/VNC远程访问": 1164,
    "适用于多个平台": 1165,
    "适用于多个平台，提供了一个简单的 API": 1166,
    "选择模型": 1167,
    "选择模型、转换模型、部署到设备、优化模型": 1168,
    "选择模型、转换模型、部署到设备和优化模型": 1169,
    "逐步加载单个数据集的图像": 1170,
    "通过 pip 安装": 1171,
    "通过浏览器访问进行交互式计算": 1172,
    "通过许多正负样例中训练得到cascade方程，然后将其应用于其他图片": 1173,
    "通过跳线线连接到其他电路板或设备": 1174,
    "通过限流电阻串联到GPIO21，负极连接到GND": 1175,
    "部署到Jetson Nano开发板": 1176,
    "部署到设备": 1177,
    "部署在 Raspberry Pi 和基于 Arm64 的主板的嵌入式 Linux 系统上": 1178,
    "配置Jupyter lab的运行参数": 1179,
    "配置build.gradle": 1180,
    "配置proxy或使用国内镜像": 1181,
    "配置文件jupyter_notebook_config.py": 1182,
    "配置项目依赖": 1183,
    "采用更小的模型格式": 1184,
    "采用更小的模型格式，并提供了方便的模型转换器": 1185,
    "采用更小的模型格式，并提供了方便的模型转换器，可将 TensorFlow 模型转换为方便解释器使用的格式": 1186,
    "释放张量的GPU内存": 1187,
    "量化": 1188,
    "错误的连接和编程可能会导致设备损坏或故障": 1189,
    "防止Android在生成应用程序二进制文件时压缩TensorFlow Lite模型文件": 1190,
    "防止应用程序中的内存泄漏": 1191,
    "防止过拟合": 1192,
    "阻塞函数，会阻塞程序执行直到检测到一个边沿": 1193,
    "降低卷积层对位置的敏感": 1194,
    "降低存储器访问成本": 1195,
    "降低权重的精确表示，并且可选的降低存储和计算的激活值": 1196,
    "降低权重的精确表示，降低存储和计算的激活值": 1197,
    "降低用于读取和存储中间激活值的存储器访问成本": 1198,
    "随着层越来越高，这些功能越来越多地针对训练模型的数据集": 1199,
    "需要root权限": 1200,
    "需要使用GStreamer读取视频流": 1201,
    "需要使用能够在开发者套件的 Micro-USB 接口处提供 5V⎓2A 的高品质电源": 1202,
    "需要大容量快速存储卡，建议最小采用 64 GB UHS-1 卡": 1203,
    "需要大约两个半小时": 1204,
    "需要成功配置好 CUDA": 1205,
    "需要更多灵活性和控制时使用": 1206,
    "需要更多的内存": 1207,
    "需要注意版本变化": 1208,
    "需要高准确率的任务": 1209,
    "面包板、杜邦线公对母、LED灯、330欧姆电阻": 1210,
    "页面的基本结构，包含div标签、UI元素和JavaScript代码": 1211,
    "项目完整代码": 1212,
    "项目模板": 1213,
    "项目的清单文件": 1214,
    "预加载了ImageNet训练权重的深度学习模型": 1215,
    "预处理模型输入和后处理模型输出": 1216,
    "预测": 1217,
    "预测不受图像顺序影响": 1218,
    "预测汽车油耗效率": 1219,
    "预测汽车的油耗效率 MPG": 1220,
    "预训练模型": 1221,
    "预训练模型和全连接的分类器": 1222,
    "验证损失高于训练损失": 1223,
    "验证集": 1224,
    "高阶 tf.keras.* API（Keras 模型）或低阶 tf.* API（用于生成具体函数）": 1225,
    "默认分类到1000类": 1226,
    "默认包含最后的神经网络层（分类到1000类）": 1227,
    "默认安装 JetPack 安装了对应的 OpenCV 不支持 CUDA 且版本是固定搭配的": 1228
  },
  "id_to_entity": {
    "0": "\"saved_model_keras_dir\"",
    "1": "--enable_v1_converter",
    "2": "--keras_model_file",
    "3": "--output_file",
    "4": "--output_file, --saved_model_dir, --keras_model_file, --enable_v1_converter",
    "5": "--saved_model_dir",
    "6": "-D WITH_QT=OFF 禁用了 Qt5 支持",
    "7": ".tflite",
    "8": "/dev/video0",
    "9": "/etc/apt/sources.list",
    "10": "/opt/python",
    "11": "/sys/class/gpio目录下的端口文件",
    "12": "/usr/bin/python",
    "13": "0.2s的响应时间",
    "14": "0.2的丢弃率",
    "15": "100",
    "16": "128核 NVIDIA Maxwell 架构的 GPU",
    "17": "14个引脚用于其他功能",
    "18": "155层",
    "19": "155层网络",
    "20": "2.0.0",
    "21": "2.3.0",
    "22": "26个引脚可以用作数字输入或输出",
    "23": "32个过滤器，3x3大小，激活函数为relu",
    "24": "3个 Conv2D 和 2个 MaxPooling2D 层",
    "25": "40 针 GPIO 扩展接口",
    "26": "40个 GPIO 引脚",
    "27": "40个引脚",
    "28": "4GB 的内存",
    "29": "5个节点",
    "30": "5个节点，激活函数为softmax",
    "31": "6",
    "32": "64位四核的 ARM Cortex-A57 CPU",
    "33": "800万像素",
    "34": "800万像素、感光芯片为索尼 IMX219，静态图片分辨率为3280 × 2464、支持1080p30, 720p60以及640 × 480p90视频录像",
    "35": "<!DOCTYPE html> <html> <head> <meta charset=\"utf-8\"> <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\"> <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">",
    "36": "@tensorflow/tfjs",
    "37": "@tensorflow/tfjs-vis",
    "38": "AC8265",
    "39": "ARM Cortex-A57 CPU",
    "40": "ARM Cortex-A72 CPU",
    "41": "Android",
    "42": "Android Studio",
    "43": "Android 应用",
    "44": "Android 开发人员",
    "45": "Android 开发人员使用",
    "46": "Android 开发者使用 JCenter Bintray 的 TFLite AAR",
    "47": "Android 设备",
    "48": "Android 部署",
    "49": "Android环境部署",
    "50": "Android部署",
    "51": "BCM2835芯片的GPIO库",
    "52": "BCM编号",
    "53": "BCM编号模式和物理引脚Broad编号模式",
    "54": "Broadcom针脚号，通常称的GPIO",
    "55": "C",
    "56": "C#",
    "57": "C++",
    "58": "C++ 和 Python 提供的 TensorFlow Lite API",
    "59": "CNN",
    "60": "CSI摄像头",
    "61": "CSI摄像头模块",
    "62": "CSI相机接口",
    "63": "CSI端口",
    "64": "ClassifierFloatMobileNet类",
    "65": "Classifier类",
    "66": "CocoaPods",
    "67": "Conv2D",
    "68": "Conv2D和MaxPooling2D层",
    "69": "Core API",
    "70": "C、C++",
    "71": "Dense",
    "72": "Display Port 接口",
    "73": "Dropout",
    "74": "Enables the converter and flags used in TF 1.x instead of TF 2.x",
    "75": "Etcher",
    "76": "Face Recognition",
    "77": "False",
    "78": "FlatBuffers",
    "79": "FlatBuffers 格式",
    "80": "Full path of the output file",
    "81": "Full path to the Keras H5 model file",
    "82": "Full path to the SavedModel directory",
    "83": "GPIO.cleanup()",
    "84": "GPIO.setmod()",
    "85": "GPIO.setup()",
    "86": "GPIO.setup(18, GPIO.IN, pull_up_down=GPIO.PUD_UP)",
    "87": "GPIO21",
    "88": "GPIO口",
    "89": "GPIO库",
    "90": "GPIO引脚",
    "91": "GPU",
    "92": "GPU 委托",
    "93": "GPU代理",
    "94": "GStreamer",
    "95": "GStreamer管道",
    "96": "GlobalAveragePooling2D",
    "97": "Google",
    "98": "Google Assistant、Google Photos、Uber、Airbnb、网易、爱奇艺、WPS等公司的应用",
    "99": "Google 内部用于计算机视觉场景的解决方案",
    "100": "Google 尝试简化 TensorFlow 并在移动设备上运行的缩减版 TensorFlow",
    "101": "Gordon Henderson",
    "102": "HDMI 接口",
    "103": "HIGH电平",
    "104": "HTML 文件",
    "105": "HTML文件、JS文件和配置文件",
    "106": "Haar特征分类器",
    "107": "Haar特征的cascade分类器",
    "108": "Hello AI World",
    "109": "I2C库",
    "110": "I2C接口(SCL、SDA)",
    "111": "IMAGE_WIDTH, IMAGE_HEIGHT, testData, testxs, labels, preds",
    "112": "IMX219模组",
    "113": "ImageDataGenerator",
    "114": "ImageProcessor",
    "115": "Java",
    "116": "Java 或 C++ API",
    "117": "JavaScript",
    "118": "JetBot",
    "119": "JetPack SDK",
    "120": "Jetson",
    "121": "Jetson B02版本",
    "122": "Jetson Nano",
    "123": "Jetson Nano 开发板",
    "124": "Jetson Nano开发板",
    "125": "Jupyter Notebook",
    "126": "Jupyter Notebook 的全面升级",
    "127": "Jupyter Notebook、文本编辑器、终端以及各种个性化组件",
    "128": "Jupyter lab",
    "129": "JupyterLab",
    "130": "Keras H5",
    "131": "Keras 模型",
    "132": "Keras模型",
    "133": "Keras的模型定义方式",
    "134": "LED",
    "135": "LED灯",
    "136": "LED的控制引脚",
    "137": "LOW电平",
    "138": "Layers API",
    "139": "Layers API 和 Core API",
    "140": "Layers API和Core API",
    "141": "Linux",
    "142": "Linux 开发环境",
    "143": "Linux开发环境",
    "144": "MIPI",
    "145": "MIPI CSI-2",
    "146": "MIPI的相机串行接口（CSI）端口",
    "147": "MIPI联盟发起的为移动应用处理器制定的开放标准",
    "148": "MNIST 分类器",
    "149": "MNIST数据集",
    "150": "MaxPooling2D",
    "151": "Micro USB 接口",
    "152": "MnistData",
    "153": "MobileNet",
    "154": "MobileNet V2",
    "155": "MobileNet 图像分类",
    "156": "MobileNetV2",
    "157": "MobileNets_v2",
    "158": "NVIDIA Jetson Nano",
    "159": "NVIDIA Jetson Nano 开发板",
    "160": "NVIDIA Maxwell GPU",
    "161": "NVIDIA Maxwell 架构的 GPU",
    "162": "NVIDIA TensorRT",
    "163": "Object C",
    "164": "OpenCV",
    "165": "OpenCV 安装",
    "166": "OpenCV 安装脚本",
    "167": "OpenCV 编译",
    "168": "OperatorCode",
    "169": "PCB板上针脚的物理位置对应的编号（1~40）",
    "170": "PWM接口",
    "171": "Parcel",
    "172": "Pi 4B",
    "173": "Pi v3+安装包是不同的",
    "174": "PoseNet",
    "175": "PoseNet模型",
    "176": "PoseNet示例应用",
    "177": "PoseNet示例应用程序",
    "178": "Post training quantization",
    "179": "Python",
    "180": "Python 2.7",
    "181": "Python API",
    "182": "Python API 和命令行工具",
    "183": "Python GPIO编程",
    "184": "Python 模块",
    "185": "Python 源码包",
    "186": "Python 相关程序模块会拷贝到/opt/python",
    "187": "Python安装",
    "188": "Python开发环境",
    "189": "Python程序",
    "190": "Quantization-aware training",
    "191": "RPI.GPIO库",
    "192": "RandomFlip",
    "193": "RandomRotation",
    "194": "Raspberry Camera V2",
    "195": "Raspberry Pi 4",
    "196": "Raspberry Pi、Arducam等常见的相机模块",
    "197": "Raspbian软件仓库镜像",
    "198": "ResizeOp",
    "199": "ResizeWithCropOrPadOp",
    "200": "Rot90Op",
    "201": "SD Memory Card Formatter",
    "202": "SPI库",
    "203": "SPI接口（MISO、MOSI、CLK、CS片选信号SPICE0_N）",
    "204": "SavedModel",
    "205": "SavedModel 和 Keras Sequential",
    "206": "SavedModel 和 Keras Sequential 两种模型导出方法和格式",
    "207": "SavedModel 格式",
    "208": "SavedModel的目录路径",
    "209": "SparseCategoricalCrossentropy",
    "210": "Swift",
    "211": "Swift 和 Objective-C 编写的原生库",
    "212": "SymbolicTensor",
    "213": "TF Mobile",
    "214": "TFLite",
    "215": "TFLite interpreter",
    "216": "TFLite model",
    "217": "TFLite 文件格式",
    "218": "TFLite 模型文件",
    "219": "TFLite 模型文件格式",
    "220": "TFLite 模型转换器",
    "221": "TFLite 模型转换过程",
    "222": "TFLite 算子库",
    "223": "TFLite 解释器",
    "224": "TFLite 解释执行器",
    "225": "TFLite 转换器",
    "226": "TFLiteConverter",
    "227": "TFLiteConverter.from_concrete_functions()",
    "228": "TFLiteConverter.from_keras_model()",
    "229": "TFLiteConverter.from_saved_model()",
    "230": "TFLite模型",
    "231": "TFLite解释器tflite和GPU代理gpuDelegate",
    "232": "TFMini",
    "233": "TensorBoard",
    "234": "TensorFLow-vis",
    "235": "TensorFlow",
    "236": "TensorFlow 2.x 模型",
    "237": "TensorFlow GPU 版本",
    "238": "TensorFlow Hub",
    "239": "TensorFlow Hub 上可搜索到的模型之一",
    "240": "TensorFlow Lite",
    "241": "TensorFlow Lite AAR",
    "242": "TensorFlow Lite API",
    "243": "TensorFlow Lite 工作流程",
    "244": "TensorFlow Lite 开发工作流程",
    "245": "TensorFlow Lite 推理",
    "246": "TensorFlow Lite 支持库",
    "247": "TensorFlow Lite 模型",
    "248": "TensorFlow Lite 模型文件",
    "249": "TensorFlow Lite 的工作流程",
    "250": "TensorFlow Lite 示例",
    "251": "TensorFlow Lite 算子库",
    "252": "TensorFlow Lite 解释器",
    "253": "TensorFlow Lite 解释器(Interpreter)",
    "254": "TensorFlow Lite 解释器(Interpreter)、TensorFlow Lite 转换器(Converter)、算子库(Op kernels)、硬件加速代理(Hardware accelerator delegate)",
    "255": "TensorFlow Lite 转换器",
    "256": "TensorFlow Lite 转换器(Converter)",
    "257": "TensorFlow Lite 转换器命令行工具",
    "258": "TensorFlow Lite支持库",
    "259": "TensorFlow Lite模型",
    "260": "TensorFlow Lite解释器",
    "261": "TensorFlow 官网",
    "262": "TensorFlow 模型",
    "263": "TensorFlow 模型导出",
    "264": "TensorFlow 的 JavaScript 版本",
    "265": "TensorFlow.js",
    "266": "TensorFlow.js 中的中心数据单元，是一维或多维数组",
    "267": "TensorFlow.js 模块",
    "268": "TensorFlow.js 进行浏览器可视化的一组实用工具库",
    "269": "TensorFlow.js预训练模型",
    "270": "TensorFlow，PyTorch，Caffe / Caffe2，Keras，MXNet 等深度学习框架",
    "271": "TensorLabel",
    "272": "Tensorflow Lite post-training quantization",
    "273": "Tensorflow.js",
    "274": "Total params: 2,626,821, Trainable params: 368,837, Non-trainable params: 2,257,984",
    "275": "UART串口接口（TXD、RXD）",
    "276": "UART库",
    "277": "USB 接口",
    "278": "USB摄像头",
    "279": "VNC",
    "280": "VNC Viewer",
    "281": "VNC服务器软件",
    "282": "Web Server for Chrome",
    "283": "Webpack",
    "284": "Wiring Pi",
    "285": "Wiring Pi编号",
    "286": "Wiring Pi编号模式",
    "287": "XML文件，描述人体各个部位的Haar特征值",
    "288": "aaptOptions",
    "289": "accuracy",
    "290": "add_event_detect()函数",
    "291": "allocate_tensors()",
    "292": "base_model, Conv2D, Dropout, GlobalAveragePooling2D, Dense",
    "293": "batchSize",
    "294": "batchSize设置为512",
    "295": "batch_size",
    "296": "batch_size用于设置每批图像数量",
    "297": "bool",
    "298": "build.gradle",
    "299": "buildscript",
    "300": "callback",
    "301": "callbacks设置为fitCallbacks",
    "302": "categorical_crossentropy",
    "303": "cdn.jsdelivr.net",
    "304": "class_names",
    "305": "cnn模型",
    "306": "compare_faces",
    "307": "conv2d, dropout, global_average_pooling2d, dense",
    "308": "convertToTensor函数",
    "309": "converter.convert",
    "310": "createModel()",
    "311": "daisy, dandelion, roses, sunflowers, tulips",
    "312": "data.js",
    "313": "data.js文件",
    "314": "dependencies",
    "315": "dispose",
    "316": "dispose和tf.tidy两种内存管理方法",
    "317": "dlib库",
    "318": "doPrediction函数",
    "319": "epochs",
    "320": "epochs设置为10",
    "321": "face_cascade.detectMultiScale",
    "322": "face_distance",
    "323": "face_encodings",
    "324": "face_locations",
    "325": "face_recognition",
    "326": "fine_tune",
    "327": "fine_tune_at",
    "328": "finish 目录",
    "329": "flow_from_directory",
    "330": "flower_classification 项目",
    "331": "from_saved_model(), from_keras_model(), from_concrete_functions()",
    "332": "from_saved_model方法",
    "333": "functional模型",
    "334": "get_input_details()",
    "335": "get_output_details()",
    "336": "get_tensor()",
    "337": "gpio readall命令",
    "338": "hog模型",
    "339": "hub.KerasLayer",
    "340": "iOS",
    "341": "iOS 开发人员",
    "342": "iOS 开发人员使用",
    "343": "iOS 开发者通过 CocoaPods 获取",
    "344": "images和labels",
    "345": "include_top=False",
    "346": "index",
    "347": "index.html",
    "348": "index.js",
    "349": "inputShape为[1]，units为1，useBias为true",
    "350": "interpreter",
    "351": "interpreter.get_tensor()",
    "352": "invoke()",
    "353": "jtop 命令",
    "354": "jupyter",
    "355": "jupyter_notebook_config.py",
    "356": "labels.txt",
    "357": "layers.Flatten()",
    "358": "load_image_file、face_locations、face_encodings、compare_faces、face_distance",
    "359": "loss",
    "360": "make install",
    "361": "make 命令",
    "362": "metrics",
    "363": "microSD 卡插槽",
    "364": "microSD 卡用作启动设备和主存储器",
    "365": "microSD卡",
    "366": "microSD卡插槽",
    "367": "minNeighbors",
    "368": "mnist项目",
    "369": "mobilenetv2_1.00_224",
    "370": "model",
    "371": "model.compile",
    "372": "model.fit",
    "373": "model.fit()",
    "374": "model.predict()",
    "375": "model.tflite和label.txt",
    "376": "model.tflite文件",
    "377": "model.trainable = False",
    "378": "model可以是'hog'或'cnn'",
    "379": "nano或vi",
    "380": "nextTestBatch",
    "381": "nextTrainBatch",
    "382": "nextTrainBatch 和 nextTestBatch 方法",
    "383": "noCompress \"tflite\"",
    "384": "normalization_layer",
    "385": "np.argmax()",
    "386": "number_of_times_to_upsample设置对图像进行多少次上采样以查找人脸",
    "387": "optimizer",
    "388": "optimizer, loss, metrics",
    "389": "optimizer='sgd', loss='mean_squared_error'",
    "390": "org.tensorflow:tensorflow-lite:+",
    "391": "output_details",
    "392": "package.json",
    "393": "pip",
    "394": "pip install",
    "395": "pip3",
    "396": "predict()",
    "397": "print(\"button pressed!\")",
    "398": "python",
    "399": "repositories和dependencies",
    "400": "resize原始图像到模型输入大小",
    "401": "saved_model_dir",
    "402": "sequential模型",
    "403": "sequential模型和functional模型",
    "404": "set_tensor()",
    "405": "showAccuracy函数",
    "406": "showConfusion函数",
    "407": "shuffle",
    "408": "shuffle设置为true",
    "409": "softmax",
    "410": "start 目录",
    "411": "start 目录和 finish 目录",
    "412": "string",
    "413": "target_size用于设置图像大小",
    "414": "tensor()",
    "415": "tf.image.resize",
    "416": "tf.keras model",
    "417": "tf.keras.Sequential",
    "418": "tf.keras.callbacks.TensorBoard",
    "419": "tf.keras.layers.Dense",
    "420": "tf.keras.losses.SparseCategoricalCrossentropy",
    "421": "tf.keras.models.Sequential",
    "422": "tf.keras.optimizers.Adam",
    "423": "tf.keras.optimizers.Adam()",
    "424": "tf.linspace()",
    "425": "tf.lite.TFLiteConverter",
    "426": "tf.lite.TFLiteConverter.from_keras_model",
    "427": "tf.lite.TFLiteConverter.from_saved_model",
    "428": "tf.losses.meanSquaredError",
    "429": "tf.model()",
    "430": "tf.nn.softmax",
    "431": "tf.nn.softmax()",
    "432": "tf.saved_model.save",
    "433": "tf.sequential()",
    "434": "tf.sequential()和tf.model()两种创建模型的方式",
    "435": "tf.tensor 函数",
    "436": "tf.tidy",
    "437": "tf.tidy()",
    "438": "tf.train.adam()",
    "439": "tfjs-examples/mnist",
    "440": "tflite_convert",
    "441": "tflite_convert --keras_model_file=/tmp/mobilenet_keras_model.h5 --output_file=/tmp/mobilenet.tflite",
    "442": "tflite_convert --saved_model_dir=/tmp/mobilenet_saved_model --output_file=/tmp/mobilenet.tflite",
    "443": "tflite_model",
    "444": "tfvis.render.scatterplot",
    "445": "tfvis.show.modelSummary",
    "446": "time.sleep()",
    "447": "train_ds, validation_data=val_ds, epochs=NUM_EPOCHS, callbacks=tensorboard_callback",
    "448": "train_generator",
    "449": "train_generator, steps_per_epoch=len(train_generator), epochs=epochs, validation_data=val_generator, validation_steps=len(val_generator)",
    "450": "ui.js",
    "451": "units=1",
    "452": "units=1, input_shape=[1]",
    "453": "units=16, activation='relu'",
    "454": "units为1，useBias为true",
    "455": "useBias",
    "456": "val_ds",
    "457": "val_ds.map",
    "458": "val_generator",
    "459": "validationData设置为[testXs, testYs]",
    "460": "vi",
    "461": "vino",
    "462": "wait_for_edge()函数",
    "463": "wait_for_edge()函数和add_event_detect()函数",
    "464": "yarn",
    "465": "一个使用数据流图进行数值计算的开源软件库",
    "466": "一个多媒体框架，用于后端处理任务，如格式修改、显示驱动程序协调和数据处理",
    "467": "一个强大、简单、易上手的人脸识别开源项目",
    "468": "一个端到端的机器学习开源框架",
    "469": "一个轻量、快速、兼容度高的专门针对移动式应用场景的深度学习工具",
    "470": "一种数据结构，包含了在解决一个特定问题时，训练得到的机器学习网络的逻辑和知识",
    "471": "一种有效的物品检测方法",
    "472": "一种特定的矩阵用来呈现算法性能的可视化效果",
    "473": "一系列函数，以张量作为输入并输出另一个张量",
    "474": "一系列的计算节点、多个张量，以及子图本身的输入和输出",
    "475": "一组工具，可帮助开发者在移动设备、嵌入式设备和 IoT 设备上运行 TensorFlow 模型",
    "476": "一组数字引脚，可用于将树莓派连接到其他电子设备",
    "477": "上拉电阻",
    "478": "下载 OpenCV",
    "479": "下载和访问mnist数据集",
    "480": "下载源代码使用GIT工具下载代码，然后编译安装",
    "481": "下载源码",
    "482": "下载特定版本的Python",
    "483": "下载解压",
    "484": "不包括顶层分类层",
    "485": "不同形式的模型，包括 Keras Model 和 SavedModel",
    "486": "不支持 CUDA",
    "487": "不改变基础模型的各项参数变量",
    "488": "不清除内部函数的返回值",
    "489": "不需要原始模型构建代码",
    "490": "不需要原始模型构建代码就可以运行",
    "491": "不需要原有模型中最后的神经网络层",
    "492": "不需要原有模型中最后的神经网络层（分类到1000类）",
    "493": "不需要序列化或可以创造自己的序列化方法",
    "494": "不需要改变模型，最少情况只需多加一行代码",
    "495": "与 TensorFlow 一起安装",
    "496": "与树莓派结合可以将项目与现实世界轻松的联系起来",
    "497": "串联在LED和电源之间限制电流",
    "498": "为了高性能场景创建的序列化库",
    "499": "主要应用于游戏场景",
    "500": "主要应用于游戏场景，是为了高性能场景创建的序列化库",
    "501": "了解模型效率、调试超参数",
    "502": "二进制文件很小",
    "503": "二进制文件很小，支持设备端机器学习推断，延迟较低",
    "504": "人体姿势估计",
    "505": "人脸检测",
    "506": "人脸检测、检测面部特征点、给脸部编码、从编码中找出人的名字",
    "507": "人脸识别",
    "508": "人脸识别与商品识别",
    "509": "仅用于开发的程序包",
    "510": "仅适用于卷积神经网络的一个子集",
    "511": "从 MNIST 数据集中随机批量提取 MNIST 图像",
    "512": "从 Python 官网下载",
    "513": "从 github 下载源码",
    "514": "从头自己编译",
    "515": "从指定层开始进行模型微调",
    "516": "从指定目录加载训练图像数据",
    "517": "从指定目录加载验证图像数据",
    "518": "从测试集中返回一批图像及其标签",
    "519": "从训练集中返回一批随机图像及其标签",
    "520": "以 ldconfig 结束",
    "521": "以依赖项的安装开始",
    "522": "以最小精度下降来训练网络",
    "523": "优化器Adam",
    "524": "优化模型",
    "525": "优化模型大小和性能",
    "526": "优化模型性能",
    "527": "优化的 FlatBuffer 格式",
    "528": "优化的 FlatBuffer 格式，以 .tflite 为文件扩展名",
    "529": "优化的算子库",
    "530": "低功耗",
    "531": "作为functional模型第一层的输入",
    "532": "作为判断训练结果的参数",
    "533": "作为损失函数用于多分类任务",
    "534": "作为最终的 Dense 层的激活函数",
    "535": "作为最终的Dense层激活函数进行多分类",
    "536": "作为模型优化算法",
    "537": "作为模型的优化器",
    "538": "作为模型的损失函数",
    "539": "作为迁移学习的基础模型",
    "540": "使权重和激活值的 Post training 更简单",
    "541": "使用 CocoaPods for Swift or Objective-C",
    "542": "使用 Python API 进行转换",
    "543": "使用 SavedModel 格式存储",
    "544": "使用 TFLite 模型转换器转换成 TFLite 文件格式(FlatBuffers 格式)",
    "545": "使用 TensorFlow Lite AAR",
    "546": "使用 TensorFlow Lite 解释器（提供多种语言的 API）在设备端运行模型",
    "547": "使用 TensorFlow Lite 转换器将模型转换为 TensorFlow Lite 格式",
    "548": "使用 TensorFlow.js 进行训练模型的基本流程与概念和语法",
    "549": "使用3×3的卷积核，并在输出上使用 Relu 激活函数",
    "550": "使用GPU来加速数学运算",
    "551": "使用TensorFlow.js实现根据摄像头采集的手势图像确定剪刀、石头、布",
    "552": "使用lambda函数对images进行归一化处理并保留labels",
    "553": "使用min-max缩放方法",
    "554": "使用tf.model() API创建非闭环的计算图",
    "555": "使用tf.saved_model.save函数",
    "556": "使用低学习率编译模型",
    "557": "使用低学习率重新编译模型",
    "558": "使用命令行 tflite_convert 进行转换",
    "559": "使用层构建模型",
    "560": "使用已编译好的库",
    "561": "使用模型优化工具包缩减模型的大小并提高其效率，同时最大限度地降低对准确率的影响",
    "562": "使用测试数据集评估模型",
    "563": "使用深度可分离的卷积",
    "564": "使用深度可分离的卷积来构建轻量级的深层神经网络",
    "565": "使这些专用功能适应新数据集，而不是覆盖通用学习",
    "566": "保存模型",
    "567": "保存训练数据的类别标签",
    "568": "保护LED和GPIO引脚",
    "569": "保持了很多通用性",
    "570": "保留原来大规模训练的优势",
    "571": "修改配置文件",
    "572": "借助低级运算（例如 tf.matMul()、tf.add() 等）创建机器学习模型",
    "573": "做了移动设备相关的优化",
    "574": "允许解释器在设备的 GPU 上运行适当的运算符",
    "575": "全能 IDE",
    "576": "全连接 (Full Connected) 层",
    "577": "全连接层",
    "578": "全连接网络",
    "579": "关联概率与类别标签",
    "580": "关闭LED灯",
    "581": "其他语言包（如python、ruby或PHP）",
    "582": "具有 shape 属性定义其数组形状",
    "583": "兼容度高",
    "584": "内存回收问题突出",
    "585": "内存高效",
    "586": "内存高效，支持将文件映射到内存中，然后直接进行读取和解释",
    "587": "冻结前100层",
    "588": "冻结预训练模型并更新分类器权重",
    "589": "冻结预训练的模型，仅在训练期间更新分类器的权重",
    "590": "准备训练集和验证集",
    "591": "减少内存碎片化",
    "592": "分类器",
    "593": "分类结果的概率",
    "594": "创建 Tensor 实例的主要构造函数",
    "595": "创建0~1之间平均分配的100个值",
    "596": "创建Python包的软链接",
    "597": "创建TFLite转换器实例",
    "598": "创建、训练和导出自定义 TensorFlow Lite 模型",
    "599": "创建了一个安装脚本",
    "600": "创建任意非闭环的计算图",
    "601": "创建位图对象",
    "602": "初始化TensorFlow Lite解释器",
    "603": "利用 Android 神经网络 API（Android NN API)",
    "604": "利用手机上的加速器，比如 GPU 或者 DSP",
    "605": "利用计算机对图像进行处理、分析和理解，以识别各种不同模式的目标和对象的技术",
    "606": "前几层学习非常简单和通用的功能，这些功能可以推广到几乎所有类型的图像",
    "607": "功耗非常低，有两种模式：5W（低功耗模式；可以使用 USB 口供电）和10W（必须使用 Power Jack 外接5V 电源供电）",
    "608": "功能强大的编程语言，易于使用，易于阅读和编写",
    "609": "功能强大，交互式、富文本，还有丰富的插件、主题修改、多语言支持",
    "610": "功能接线的引脚号（如TXD、PWM0等）",
    "611": "加载 @tensorflow/tfjs 和 @tensorflow/tfjs-vis 两个 TFJS 模块的代码",
    "612": "加载和运行TFLite模型",
    "613": "加载数据并准备进行训练",
    "614": "加载数据，定义模型，训练循环并指定UI元素",
    "615": "加载模型",
    "616": "加载模型、分配张量、设置输入、执行推理、读取输出",
    "617": "加速模型推理过程",
    "618": "动态显示训练的过程",
    "619": "包含一个完整的TensorFlow程序，不仅包含权重值，还包含计算",
    "620": "包含完整的TensorFlow程序（权重和计算）",
    "621": "包含完整的TensorFlow程序，包括权重和计算",
    "622": "千兆以太网端口",
    "623": "单个图像的维度为[28,28,1]",
    "624": "占用更少的磁盘和内存，更快更高效",
    "625": "卷积层",
    "626": "卷积层与全连接层",
    "627": "卷积层和全连接层",
    "628": "卷积层输入",
    "629": "卷积神经网络",
    "630": "取消冻结模型的顶层",
    "631": "受限于GPU内存的大小",
    "632": "只使用在C语言中",
    "633": "只提供了基本的转化功能",
    "634": "可以使用自己的 TensorFlow 模型、在线查找模型，或者从的 TensorFlow 预训练模型中选择一个模型直接使用或重新训练",
    "635": "可以利用手机上的加速器，比如 GPU 或者 DSP 等",
    "636": "可以增加自己的输出层",
    "637": "可以是内置的算子或自定制算子，有一个名字",
    "638": "可以添加--no-cache-dir参数来避免缓存问题",
    "639": "可以直接使用cv2.videocapture打开",
    "640": "可以直接部署或用于迁移学习",
    "641": "可以设置访问密码增强安全性",
    "642": "可以调用不同的硬件加速器比如 GPU 进行执行",
    "643": "可以通过软件编程进行控制",
    "644": "可以配置为输入或输出",
    "645": "可在手机、嵌入式 Linux 设备和微控制器等很多不同类型的硬件上运行",
    "646": "可在手机、嵌入式 Linux 设备和微控制器等很多不同类型的硬件上运行经过专门优化的模型",
    "647": "可能出现OOM错误",
    "648": "可能存在过度拟合",
    "649": "可能导致多次输出",
    "650": "可能导致模型过拟合",
    "651": "可视化模型训练的过程和结果",
    "652": "可视化模型预测结果和原始数据",
    "653": "可配置为输入或输出",
    "654": "启动一个显示窗口，并确认可以从摄像头看到实时流",
    "655": "启动图标",
    "656": "命令行 TensorFlow Lite 转换器命令行工具",
    "657": "命令行与 Python API",
    "658": "命令行工具",
    "659": "命令行工具gpio",
    "660": "命令行工具和 Python API",
    "661": "商品流通过程中，特别是无人货架、智能零售柜等无人零售领域",
    "662": "商品识别",
    "663": "四引脚按键",
    "664": "四脚按键开关",
    "665": "回调函数",
    "666": "图像、文本和语音处理",
    "667": "图像分类、对象检测、姿势估计、文本恶意检测",
    "668": "图像分类任务",
    "669": "图像识别",
    "670": "图像识别技术",
    "671": "图像识别项目",
    "672": "图像预处理",
    "673": "在 Android 与 iOS 平台上使用",
    "674": "在18号引脚处设置",
    "675": "在Jetson开发工具包上运行预训练模型",
    "676": "在Shell脚本中控制GPIO管脚",
    "677": "在不同设备上使用硬件加速",
    "678": "在内存有限的移动环境中使用",
    "679": "在图像中检测面部，如果检测到面部会返回面部所在的矩形区域Rect(x,y,w,h)",
    "680": "在图像分类、物体检测、分割和语音处理等应用程序中并行运行多个神经网络",
    "681": "在大规模数据处理上不如Python高效",
    "682": "在官网下载安装包后安装",
    "683": "在小型数据集上训练模型",
    "684": "在展平操作之前依赖于最后一层",
    "685": "在把模型嵌入到实际的二进制文件中时兼容性存在较大挑战",
    "686": "在有 GPU 加速的手机上运行 MobileNet 图像分类，模型运行速度可以提高",
    "687": "在有 GPU 加速的手机上运行，模型运行速度可以提高 5.5 倍",
    "688": "在每一个训练周期显示训练情况",
    "689": "在浏览器中训练模型",
    "690": "在生产环境中不需要",
    "691": "在硬件加速层面，对于 CPU 利用了 ARM 的 NEON 指令集做了大量的优化",
    "692": "在移动和嵌入式设备上执行难度较大",
    "693": "在移动端（mobile）、嵌入式（embeded）和物联网（IoT）设备上运行 TensorFlow 模型",
    "694": "在移动设备、嵌入式设备和 IoT 设备上运行 TensorFlow 模型",
    "695": "在移动设备和嵌入式设备上运行TensorFlow模型",
    "696": "在给定设备上实现性能、模型大小和准确性的理想平衡",
    "697": "在训练过程中不能用于训练",
    "698": "在训练顶层分类器并将预先训练的模型设置为不可训练之后",
    "699": "在设备端运行 TFLite 模型",
    "700": "在设备端运行模型",
    "701": "在资源有限的硬件上运行",
    "702": "在边缘设备上运行 TensorFlow 模型推理",
    "703": "在边缘设备上运行 TensorFlow 模型推理的官方框架",
    "704": "在需要场景中更关心的对象",
    "705": "在靠近物或数据源头的一侧，采用网络、计算、存储、应用核心能力为一体的开放平台，就近提供最近端服务",
    "706": "地",
    "707": "基于 TF Mobile 的经验，继承了 TFMini 和内部其他类似项目的优秀工作",
    "708": "基于XML文件",
    "709": "基于流线型架构的轻量级深层神经网络",
    "710": "基于现有模型构建 Interpreter",
    "711": "基于现有的模型进行继续训练",
    "712": "基础模型的各项参数变量不会被新的训练修改数据",
    "713": "增加一个事件的检测函数",
    "714": "处理多媒体应用程序",
    "715": "处理简单的数据",
    "716": "多种级别的量化支持",
    "717": "多种编程语言",
    "718": "大电流可能损坏LED和供电设备",
    "719": "大而复杂的模型",
    "720": "头信息和脚本加载部分",
    "721": "如果仅使用支持常见图像分类模型（InceptionV3 和 MobileNet）所需的运算符，二进制文件的大小不到 300 KB",
    "722": "如果联合训练所有层，则梯度更新的幅度将太大，并且预训练模型将忘记它学到的东西",
    "723": "子图",
    "724": "子图、算子库和共享的内存缓冲区",
    "725": "存储Jetson系统镜像",
    "726": "存储已安装软件包的名称和版本",
    "727": "存储模型权重或计算节点的输入和输出",
    "728": "存放 Python 相关程序模块",
    "729": "存放训练好的模型供开发人员复用",
    "730": "学习AI和构建应用程序",
    "731": "安全检查、身份核验与移动支付",
    "732": "安卓应用只需 1 兆左右的运行环境",
    "733": "安卓应用只需 1 兆左右的运行环境，在 MCU 上甚至可以小于 100KB",
    "734": "安装 Android Studio",
    "735": "安装 OpenCV",
    "736": "安装 Python 包依赖项",
    "737": "安装 TensorFlow",
    "738": "安装 TensorFlow 所需的系统包",
    "739": "安装Android Studio",
    "740": "安装Python",
    "741": "安装Python包",
    "742": "安装依赖",
    "743": "安装依赖项",
    "744": "安装和升级 pip3",
    "745": "安装的 TensorFlow 版本必须与正在使用的 JetPack 版本一致",
    "746": "安装相应的依赖包",
    "747": "完全基于 JavaScript 从头开发、训练和部署模型",
    "748": "完成分类",
    "749": "完成分类任务",
    "750": "官方已经停止维护",
    "751": "官方推荐使用的无线网卡",
    "752": "官方集成到Python的工具",
    "753": "定义模型结构",
    "754": "定义的神经元网络层与层之间的关系较为随意",
    "755": "定义需要监视的指标",
    "756": "实例化预先训练的模型，并在顶部添加全连接的分类器",
    "757": "实时识别照相机所拍摄的花卉",
    "758": "实现了一组优化的算子内核",
    "759": "实现人体姿势估计",
    "760": "实现刷脸登录功能",
    "761": "实现花卉识别 app",
    "762": "对 SIMD 指令功能特别有益",
    "763": "对images进行归一化处理",
    "764": "对手写数字的图像进行分类",
    "765": "对数据降维",
    "766": "对模型的权重产生更一致且变化较小的渐变更新",
    "767": "对测试数据进行预测并返回预测结果和真实标签",
    "768": "对现有 CPU 平台的支持",
    "769": "对训练图像随机变换来人为引入样本多样性",
    "770": "对训练图像随机旋转以增加数据多样性",
    "771": "对训练图像随机水平翻转以增加数据多样性",
    "772": "导致每个时期的梯度更新数量较少",
    "773": "将 NPM 模块转换为在线可以引用的免费服务",
    "774": "将 TensorFlow 模型格式转换为 TensorFlow Lite 的 Python API",
    "775": "将 TensorFlow 模型转换为 TFLite 文件格式(FlatBuffers 格式)",
    "776": "将 TensorFlow 模型转换为 TFLite 格式",
    "777": "将 TensorFlow 模型转换为 TensorFlow Lite 专用的模型文件格式",
    "778": "将 TensorFlow 模型转换为方便解释器使用的格式",
    "779": "将 TensorFlow 模型转换为方便解释器使用的格式，并可引入优化以减小二进制文件的大小和提高性能",
    "780": "将Keras模型转换为TFLite模型",
    "781": "将SavedModel转换为TFLite模型",
    "782": "将SavedModel转换为TensorFlow Lite格式",
    "783": "将TensorFlow模型转换为移动设备兼容格式",
    "784": "将maven源google()和jcenter()替换为国内镜像",
    "785": "将三维张量展开到1维",
    "786": "将三维张量展开到1维以便传入Dense层",
    "787": "将人脸编码列表与候选编码进行比较以查看它们是否匹配",
    "788": "将原始数据转变为TensorFlow可读的张量格式",
    "789": "将图片分类到1000类",
    "790": "将外设操作视为文件读写",
    "791": "将大规模内存操作放置在其回调中执行",
    "792": "将层按顺序写在列表里作为sequential()函数的输入",
    "793": "将彩色图像转换为灰度图像，检测人脸，在边界周围绘制矩形",
    "794": "将模型保存为TFLite兼容格式",
    "795": "将模型加载到内存中",
    "796": "将模型嵌入到二进制文件中，可以在设备上运行和部署模型",
    "797": "将模型显示在浏览器中",
    "798": "将模型直接映射到内存中，同时有一个静态执行计划",
    "799": "将模型输出转换为概率分布",
    "800": "将特征转换为每个图像对应一个1280元素向量",
    "801": "将网络的每一层简单的叠在一起",
    "802": "将自定义模型转换为 TensorFlow Lite 格式",
    "803": "将输入数据转换成模型接收的形式或排布",
    "804": "将输入的 TensorFlow 模型生成 TensorFlow Lite 模型",
    "805": "将镜像写入microSD卡",
    "806": "小一点的模型",
    "807": "工业物联智能设备的开发",
    "808": "已经训练好的分类器，其中包括面部，眼睛，微笑等",
    "809": "常开触点",
    "810": "常见的移动/嵌入式平台",
    "811": "常闭触点",
    "812": "应对快速变化需求的软件开发模式",
    "813": "应用于树莓派的GPIO控制库函数",
    "814": "应用深度学习算法的一种实践应用",
    "815": "应用程序在边缘侧发起，产生更快的网络服务响应，满足行业在实时业务、应用智能、安全与隐私保护等方面的基本需求",
    "816": "廉价且周边设备多",
    "817": "延迟一秒钟",
    "818": "延迟较低",
    "819": "开关去抖",
    "820": "开关抖动",
    "821": "开发依赖",
    "822": "引入优化以减小二进制文件的大小和提高性能",
    "823": "引用 Model 的内存缓冲区的一片区域，提高内存效率",
    "824": "张量",
    "825": "张量(Tensor)",
    "826": "张量形状是 (image_height, image_width, color_channels)",
    "827": "张量（Tensor）",
    "828": "归一化操作",
    "829": "当压力施压时电路接通",
    "830": "当撤销压力时电路恢复",
    "831": "形状是 (224,224, 3)",
    "832": "微调",
    "833": "微调少量顶层而不是整个 MobileNet 模型",
    "834": "微调结果",
    "835": "微调过程",
    "836": "必须在开机前先装上去，系统才能识别",
    "837": "快速",
    "838": "快速启动",
    "839": "快速启动深度学习推理演示",
    "840": "忽略由于开关抖动引起的小于",
    "841": "恢复训练",
    "842": "手写数字识别",
    "843": "手势识别项目",
    "844": "打乱数据顺序，创建特征向量和标签向量，转换为张量格式，进行归一化操作",
    "845": "打开现有 Android Studio 项目",
    "846": "打开项目图标",
    "847": "执行 TensorFlow Lite 推理",
    "848": "执行一个函数并清除所有创建的中间张量，释放它们的GPU内存",
    "849": "执行推理",
    "850": "执行模型推理",
    "851": "执行模型推理过程",
    "852": "执行模型文件在输入数据上定义的运算符，输出推理结果",
    "853": "执行模型转换过程",
    "854": "批次大小",
    "855": "指定从哪个层开始进行微调的参数",
    "856": "指定引脚编号系统",
    "857": "损失函数categorical_crossentropy",
    "858": "接受 TFLite 模型",
    "859": "接收汽车的功率作为输入",
    "860": "控制GPIO引脚",
    "861": "控制LED灯的亮暗",
    "862": "控制外部硬件设备",
    "863": "控制树莓派GPIO引脚",
    "864": "控制树莓派的GPIO",
    "865": "推理过程",
    "866": "描述构建和运行示例所需的依赖项",
    "867": "提供了一个简单的 API，用于从 Java、Swift、Objective-C、C++ 和 Python 运行 TensorFlow Lite 模型",
    "868": "提供低级的机器学习构建模块和高级的类似 Keras 的 API 来构建神经网络",
    "869": "提供已经训练好且经过充分认证的模型",
    "870": "提供权重初始化、模型序列化、训练监测、可迁移性和安全检查等工具",
    "871": "提供转换工具压缩模型，进行算子融合并生成代码",
    "872": "提供预训练模型，开发人员可以复用这些已经训练好且经过充分认证的模型",
    "873": "提高性能的方法是训练预训练模型的顶层的权重以及刚添加的分类器的训练",
    "874": "提高模型准确率",
    "875": "提高模型性能",
    "876": "摄像头接口",
    "877": "摄像头预捕获的图像宽度、高度、窗口显示的图像宽度、高度、捕获帧率、是否旋转图像",
    "878": "支持 GPU 硬件加速",
    "879": "支持像素缩放和数据增强",
    "880": "支持各种环境的部署",
    "881": "支持多种编程语言",
    "882": "支持大规模的模型训练",
    "883": "支持将文件映射到内存中直接读取和解释，不需要额外解析",
    "884": "支持微控制器(MCU)，可以应用于 IoT 领域",
    "885": "支持批量加载图像数据",
    "886": "支持格式修改和数据处理",
    "887": "支持算子优化和常见的编译优化",
    "888": "支持算子优化和常见的编译优化，比如算子融合、常数折叠或无用代码删除等",
    "889": "支持训练后量化",
    "890": "支持设备端机器学习推断",
    "891": "支持设备端机器学习推断，延迟较低，并且二进制文件很小",
    "892": "支持量化原生支持",
    "893": "支持预装驱动程序的RPi相机，可以很容易地用作即插即用外围设备，不需要安装驱动程序",
    "894": "敏捷开发",
    "895": "教育",
    "896": "数据图像的采集、模型的训练、参数的调整、模型文件生成、网页端部署、网络摄像头检查",
    "897": "数据规范化和转换为张量类型",
    "898": "数据转换",
    "899": "数据转换、执行推理、解释输出",
    "900": "数据预处理",
    "901": "整个安装需要两个小时才能完成",
    "902": "旋转图像",
    "903": "是一个线性堆叠layers的模型",
    "904": "普通GPIO口",
    "905": "更准确的深度学习模型需要GPU加速",
    "906": "更换国内源如阿里、清华",
    "907": "更新可视化元素",
    "908": "更谨慎地控制内存何时回收",
    "909": "更适合于边缘设备部署",
    "910": "最后的神经网络层",
    "911": "最大池化层",
    "912": "最新的安卓系统提供了 Android 神经网络 API（Android NN API)，让硬件厂商可以扩展支持这样的接口",
    "913": "有两路CSI相机接口",
    "914": "有助于避免因错误的样本而改向错误的方向",
    "915": "未满足的对等依赖 seedrandom@~",
    "916": "机器学习和计算机视觉应用，如物体检测、人脸识别、图像分割等视觉任务",
    "917": "机身只有 Ethernet 有线网络，不包括无线网卡",
    "918": "权重值",
    "919": "构建CNN模型",
    "920": "构建和训练深度学习模型",
    "921": "构建和运行mnist代码",
    "922": "构建和运行机器学习模型",
    "923": "构建图像处理流程",
    "924": "构建小型移动机器人、人脸签到打卡、口罩识别、智能门锁、智能音箱等复杂 AI 系统",
    "925": "构建工具用于编写更大的程序",
    "926": "构成检测目标的相邻矩形的最小个数",
    "927": "查看图像及其标签",
    "928": "查看开发板系统信息",
    "929": "查看树莓派的GPIO引脚信息",
    "930": "标准算子",
    "931": "树莓派",
    "932": "树莓派 GPIO",
    "933": "树莓派4B的18号引脚",
    "934": "树莓派GPIO",
    "935": "树莓派接口",
    "936": "树莓派摄像头",
    "937": "树莓派的21号引脚",
    "938": "树莓派的官方编程语言",
    "939": "树莓派系统",
    "940": "树莓派通用输入/输出接口（GPIO）",
    "941": "核心板可拆的设计，核心板的大小只有70 x 45 mm",
    "942": "核心运行时",
    "943": "格式化microSD卡",
    "944": "检查摄像头信息",
    "945": "检测人脸",
    "946": "检测关键身体部位的位置",
    "947": "模型",
    "948": "模型优化",
    "949": "模型优化工具包",
    "950": "模型大小只有20 KB 左右",
    "951": "模型执行流图",
    "952": "模型推理",
    "953": "模型文件和标签文件",
    "954": "模型的计算节点",
    "955": "模型精度达到98%",
    "956": "模型编译",
    "957": "模型评估",
    "958": "模型部署",
    "959": "模型预测",
    "960": "比 CPU 执行更快的浮点矩阵运算",
    "961": "池化层",
    "962": "注重实时性，内存高效",
    "963": "测试Python开发环境并查看当前的Python版本",
    "964": "测试数据",
    "965": "测试的软件包、webpack或Babel",
    "966": "混淆矩阵",
    "967": "清华源",
    "968": "清理GPIO引脚设置",
    "969": "清除张量或变量并释放其GPU内存",
    "970": "清除所有创建的中间张量并释放它们的GPU内存",
    "971": "激活、设置为输出状态、写入1使其输出高电压",
    "972": "灵活的架构可以将模型部署到桌面、服务器或移动设备中的 CPU 或 GPU 上",
    "973": "点亮LED灯",
    "974": "版本变化后 API 函数会改变",
    "975": "版本变化后API函数会改变",
    "976": "物理引脚Broad编号",
    "977": "特征提取",
    "978": "瓶颈层",
    "979": "生成 HDF5 文件的绝对路径目录",
    "980": "生成SavedModel",
    "981": "生成一个批次一个批次的图片，以生成器的形式给模型训练",
    "982": "生成一个批次的图片，以生成器的形式给模型训练",
    "983": "生成批次的图片数据用于模型训练",
    "984": "用于 5V 电源输入",
    "985": "用于 flower_classification 项目",
    "986": "用于加载 TensorFlow Hub 上的模型",
    "987": "用于存储数据的JavaScript文件",
    "988": "用于嵌入式设备，从功耗、体积、价格上也算一个平衡",
    "989": "用于工具的配置中心",
    "990": "用于构建网页的HTML文件",
    "991": "用于特征提取",
    "992": "用于监督学习中以显示多个类别是否有混淆",
    "993": "用于编写JavaScript代码的文件",
    "994": "用于训练模型",
    "995": "用于预测",
    "996": "用到的算子索引和输入输出用到的 Tensor 索引",
    "997": "用户在自己的工作台中使用 TensorFlow API 构造 TensorFlow 模型",
    "998": "用来连接 DP 屏幕",
    "999": "由 schema.fbs 文件使用 FlatBuffers 定义",
    "1000": "由jupyter软件自动生成",
    "1001": "由常开触点、常闭触点组合而成",
    "1002": "电信号从低电平到高电平或从高电平到低电平状态的改变",
    "1003": "电阻",
    "1004": "登录 Jetson Nano",
    "1005": "目前有130个左右",
    "1006": "目前有130个左右，它与 TensorFlow 的核心算子库略有不同，并做了移动设备相关的优化",
    "1007": "直接串联3.3V电源会产生非常大的电流",
    "1008": "直接在 Objective-C 代码中使用 C API",
    "1009": "直接更新树莓派系统",
    "1010": "直流桶式插座",
    "1011": "相机模块",
    "1012": "相比 Protocol Buffer 有更高的性能和更小的大小",
    "1013": "硬件加速代理(Hardware accelerator delegate)",
    "1014": "确认 CUDA 已经被正常安装",
    "1015": "神经元权重计算中的偏置量",
    "1016": "移动端及IoT设备端的深度学习技术",
    "1017": "第一个卷积层",
    "1018": "第二、三卷积层",
    "1019": "简化了算子集，缩小了运行库",
    "1020": "简化图像预处理和模型输出处理",
    "1021": "简单的线性回归的实验",
    "1022": "算子实现",
    "1023": "算子库(Op kernels)",
    "1024": "算子融合、常数折叠或无用代码删除",
    "1025": "精度达到98%",
    "1026": "精确度不高的任务",
    "1027": "系统升级",
    "1028": "结果不太准确但在CPU上运行更快",
    "1029": "绘制混淆矩阵以可视化模型性能",
    "1030": "绘制骨架",
    "1031": "给定汽车的功率（Horsepower），预测汽车油耗（MPG）",
    "1032": "编译 OpenCV",
    "1033": "编译 Python 模块",
    "1034": "缩减模型的大小并提高其效率，同时最大限度地降低对准确率的影响",
    "1035": "缩放图像",
    "1036": "缩短开发周期",
    "1037": "网络环境较差时可以考虑更换源",
    "1038": "能够利用各种硬件加速",
    "1039": "花卉数据集中的图片",
    "1040": "花卉识别 app",
    "1041": "花卉识别模型",
    "1042": "获取score中的最大值索引",
    "1043": "获取图像数据、创建位图、调用estimateSinglePose()、绘制骨架",
    "1044": "获取图像数据并转换格式",
    "1045": "获取张量的指针",
    "1046": "获取张量的数据",
    "1047": "观测开关去抖效果",
    "1048": "解决JavaScript内存回收问题",
    "1049": "解决跨域问题",
    "1050": "解释器和转换器",
    "1051": "解释输出",
    "1052": "计算图",
    "1053": "计算已知人脸和未知人脸特征向量的距离",
    "1054": "计算并显示每个类别的准确度",
    "1055": "计算机视觉应用",
    "1056": "计算能力不高，勉强可以使用一些小规模、并且优化过的网络进行推理，训练的话还是不够用的",
    "1057": "计算预测结果的softmax概率",
    "1058": "让输入输出映射到0-1之间，保证后期更有效地训练",
    "1059": "训练分类器",
    "1060": "训练后量化",
    "1061": "训练数据",
    "1062": "训练数据和测试数据",
    "1063": "训练曲线",
    "1064": "训练期间将不更新预训练网络的权重，只在 MobileNet V2基础模型上训练了几层",
    "1065": "训练模型",
    "1066": "训练模型，并监视其性能",
    "1067": "训练集",
    "1068": "训练集和验证集的值由不同的颜色符号显示",
    "1069": "记录训练日志",
    "1070": "记录训练过程中的指标和计算图",
    "1071": "许多 CPU 和硬件加速器实现提供 SIMD 指令功能，对量化特别有益",
    "1072": "设置 OpenCV 的内容、位置和方式",
    "1073": "设置 model.trainable = False",
    "1074": "设置GPIO引脚模式",
    "1075": "设置GPIO管脚",
    "1076": "设置上拉电阻",
    "1077": "设置为32，表示一次采样32条训练数据",
    "1078": "设置为50，表示遍历所有样本50次",
    "1079": "设置为true，表示打乱数据集",
    "1080": "设置前100层为不可训练",
    "1081": "设置引脚为输入或输出模式",
    "1082": "设置输入张量值",
    "1083": "评估准确性",
    "1084": "评估分类器的准确性",
    "1085": "评估指标accuracy",
    "1086": "评估模型",
    "1087": "评估模型的泛化能力",
    "1088": "评估训练有素的模型的性能",
    "1089": "识别图像空间模式",
    "1090": "识别图像里的空间模式，例如线条和物体局部",
    "1091": "识别手写数字",
    "1092": "识别花卉图片",
    "1093": "识别输入图像",
    "1094": "试运行应用",
    "1095": "读写GPIO管脚",
    "1096": "读取传感器数据，控制 LED 等外部设备",
    "1097": "读取输出张量值",
    "1098": "调整图像大小",
    "1099": "调整数据集形状",
    "1100": "调整输入图像大小以匹配模型输入要求",
    "1101": "调整预训练模型的顶层权重",
    "1102": "调整预训练模型的顶层权重，以便模型学习特定于数据集的高级特征",
    "1103": "调用 tf.lite.TFLiteConverter.from_saved_model() 或 TFLiteConverter.from_keras_model()",
    "1104": "调用CSI摄像头和USB摄像头",
    "1105": "调用estimateSinglePose()函数",
    "1106": "调用model.fit方法进行训练",
    "1107": "调用不同的硬件加速器比如 GPU 进行执行",
    "1108": "调用解释器的方式：try (Interpreter interpreter = new Interpreter(tensorflow_lite_model_file)) { interpreter.run(input, output); }",
    "1109": "资源有限，不能训练网络",
    "1110": "跨平台运行，包括 Android、iOS 以及基于 Linux 的 IoT 设备和微控制器",
    "1111": "转换 SavedModel 格式模型",
    "1112": "转换 concrete functions",
    "1113": "转换 tf.keras 模型",
    "1114": "转换Keras模型到TensorFlow Lite模型",
    "1115": "转换、运行 TensorFlow 模型所需的所有工具",
    "1116": "转换为 TensorFlow Lite 模型的代码示例",
    "1117": "转换后得到的TFLite格式模型",
    "1118": "转换后的原模型",
    "1119": "转换数据",
    "1120": "转换模型",
    "1121": "软件PWM库",
    "1122": "软件源配置文件",
    "1123": "软链接",
    "1124": "轻量化",
    "1125": "轻量级",
    "1126": "轻量级、快速启动、内存高效",
    "1127": "轻量，二进制文件的大小约为 1 MB（针对 32 位 ARM build）",
    "1128": "输入层",
    "1129": "输入层和输出层",
    "1130": "输入状态变化",
    "1131": "输出宽度和高度会收缩",
    "1132": "输出层",
    "1133": "输出是一个三维的张量，其形状描述了 (height, width, channels)",
    "1134": "输出模式",
    "1135": "输出油耗",
    "1136": "输出电压约为3.3V",
    "1137": "输出的通道数量取决于声明层时的 filters 参数",
    "1138": "输出的通道数量取决于声明层时的filters参数",
    "1139": "输出通道数为32",
    "1140": "输出通道数为64",
    "1141": "边做边学的理想工具",
    "1142": "边缘",
    "1143": "边缘操作",
    "1144": "边缘计算",
    "1145": "边缘计算设备",
    "1146": "迁移学习",
    "1147": "运行 TensorFlow Lite 模型",
    "1148": "运行Sync Gradle",
    "1149": "运行功率仅为 5 瓦",
    "1150": "运行各种深度学习模型",
    "1151": "运行在 Node.js 或浏览器环境中",
    "1152": "运行已有的 Python 版 TensorFlow 模型",
    "1153": "运行服务监听的IP地址、端口、notebooks内核目录、浏览器开关设置",
    "1154": "运行模型推理",
    "1155": "返回图像中每张人脸的128维人脸编码",
    "1156": "返回张量数据的副本",
    "1157": "返回的是数据的副本而非引用",
    "1158": "进行内存清理工作，防止内存泄露",
    "1159": "进行大量的张量操作时使用可能会很麻烦",
    "1160": "进行迁移学习，实现识别花卉模型",
    "1161": "远程桌面访问Jetson Nano",
    "1162": "连接LED灯和限流电阻到GPIO21和GND",
    "1163": "连接其他电子设备",
    "1164": "连接显示器、键盘和鼠标或通过SSH/VNC远程访问",
    "1165": "适用于多个平台",
    "1166": "适用于多个平台，提供了一个简单的 API",
    "1167": "选择模型",
    "1168": "选择模型、转换模型、部署到设备、优化模型",
    "1169": "选择模型、转换模型、部署到设备和优化模型",
    "1170": "逐步加载单个数据集的图像",
    "1171": "通过 pip 安装",
    "1172": "通过浏览器访问进行交互式计算",
    "1173": "通过许多正负样例中训练得到cascade方程，然后将其应用于其他图片",
    "1174": "通过跳线线连接到其他电路板或设备",
    "1175": "通过限流电阻串联到GPIO21，负极连接到GND",
    "1176": "部署到Jetson Nano开发板",
    "1177": "部署到设备",
    "1178": "部署在 Raspberry Pi 和基于 Arm64 的主板的嵌入式 Linux 系统上",
    "1179": "配置Jupyter lab的运行参数",
    "1180": "配置build.gradle",
    "1181": "配置proxy或使用国内镜像",
    "1182": "配置文件jupyter_notebook_config.py",
    "1183": "配置项目依赖",
    "1184": "采用更小的模型格式",
    "1185": "采用更小的模型格式，并提供了方便的模型转换器",
    "1186": "采用更小的模型格式，并提供了方便的模型转换器，可将 TensorFlow 模型转换为方便解释器使用的格式",
    "1187": "释放张量的GPU内存",
    "1188": "量化",
    "1189": "错误的连接和编程可能会导致设备损坏或故障",
    "1190": "防止Android在生成应用程序二进制文件时压缩TensorFlow Lite模型文件",
    "1191": "防止应用程序中的内存泄漏",
    "1192": "防止过拟合",
    "1193": "阻塞函数，会阻塞程序执行直到检测到一个边沿",
    "1194": "降低卷积层对位置的敏感",
    "1195": "降低存储器访问成本",
    "1196": "降低权重的精确表示，并且可选的降低存储和计算的激活值",
    "1197": "降低权重的精确表示，降低存储和计算的激活值",
    "1198": "降低用于读取和存储中间激活值的存储器访问成本",
    "1199": "随着层越来越高，这些功能越来越多地针对训练模型的数据集",
    "1200": "需要root权限",
    "1201": "需要使用GStreamer读取视频流",
    "1202": "需要使用能够在开发者套件的 Micro-USB 接口处提供 5V⎓2A 的高品质电源",
    "1203": "需要大容量快速存储卡，建议最小采用 64 GB UHS-1 卡",
    "1204": "需要大约两个半小时",
    "1205": "需要成功配置好 CUDA",
    "1206": "需要更多灵活性和控制时使用",
    "1207": "需要更多的内存",
    "1208": "需要注意版本变化",
    "1209": "需要高准确率的任务",
    "1210": "面包板、杜邦线公对母、LED灯、330欧姆电阻",
    "1211": "页面的基本结构，包含div标签、UI元素和JavaScript代码",
    "1212": "项目完整代码",
    "1213": "项目模板",
    "1214": "项目的清单文件",
    "1215": "预加载了ImageNet训练权重的深度学习模型",
    "1216": "预处理模型输入和后处理模型输出",
    "1217": "预测",
    "1218": "预测不受图像顺序影响",
    "1219": "预测汽车油耗效率",
    "1220": "预测汽车的油耗效率 MPG",
    "1221": "预训练模型",
    "1222": "预训练模型和全连接的分类器",
    "1223": "验证损失高于训练损失",
    "1224": "验证集",
    "1225": "高阶 tf.keras.* API（Keras 模型）或低阶 tf.* API（用于生成具体函数）",
    "1226": "默认分类到1000类",
    "1227": "默认包含最后的神经网络层（分类到1000类）",
    "1228": "默认安装 JetPack 安装了对应的 OpenCV 不支持 CUDA 且版本是固定搭配的"
  }
}