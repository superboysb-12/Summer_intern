{
  "entity_to_id": {
    "--enable_v1_converter": 0,
    "--keras_model_file": 1,
    "--output_file": 2,
    "--output_file, --saved_model_dir, --keras_model_file, --enable_v1_converter 参数": 3,
    "--saved_model_dir": 4,
    "-D WITH_QT=OFF 禁用了 Qt5 支持": 5,
    ".tflite": 6,
    "/etc/apt/sources.list": 7,
    "/opt/python 目录": 8,
    "/sys/class/gpio目录下的端口文件": 9,
    "/usr/bin/python": 10,
    "0.2s的响应时间": 11,
    "100": 12,
    "1080p30, 720p60以及640 × 480p90视频录像": 13,
    "128核NVIDIA Maxwell架构的GPU": 14,
    "14个引脚用于其他功能": 15,
    "155层": 16,
    "155层网络结构": 17,
    "2.0.0": 18,
    "2.3.0": 19,
    "26个引脚可以用作数字输入或输出": 20,
    "3个 Conv2D 和 2个 MaxPooling2D 层": 21,
    "40个 GPIO 引脚": 22,
    "40个GPIO引脚": 23,
    "5个节点": 24,
    "5个节点的输出层": 25,
    "6": 26,
    "800万像素、感光芯片为索尼IMX219": 27,
    "9": 28,
    "<!DOCTYPE html> <html> <head> <meta charset=\"utf-8\"> <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\"> <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">": 29,
    "<html> <body> <h4>TFJS example<hr/></h4> <div id=\"micro-out-div\">TensorFlow.js Test</div> <script src=\"./index.js\"> </script> </body> </html>": 30,
    "@tensorflow/tfjs": 31,
    "@tensorflow/tfjs-vis": 32,
    "Adam": 33,
    "Adam优化器": 34,
    "Airbnb": 35,
    "Android Studio": 36,
    "Android 应用": 37,
    "Android 开发人员": 38,
    "Android 开发人员使用": 39,
    "Android 开发者使用 JCenter Bintray 的 TFLite AAR": 40,
    "Android、iOS 和 Linux": 41,
    "Android、iOS、嵌入式设备、以及极小的 MCU 设备": 42,
    "Android环境部署": 43,
    "Android部署": 44,
    "B02版本有两路": 45,
    "BCM编号": 46,
    "BCM编号方式": 47,
    "Broadcom针脚号，即通常称的GPIO": 48,
    "C": 49,
    "C#": 50,
    "C++": 51,
    "CNN": 52,
    "CNN层": 53,
    "CSI 相机接口": 54,
    "CSI摄像头": 55,
    "CSI端口": 56,
    "ClassifierFloatMobileNet类": 57,
    "Classifier类": 58,
    "CocoaPods": 59,
    "Conv2D": 60,
    "Core API": 61,
    "DeepLearning.js": 62,
    "Dense": 63,
    "Display Port 接口": 64,
    "Etcher": 65,
    "Face Recognition": 66,
    "FlatBuffers": 67,
    "FlatBuffers 格式": 68,
    "GPIO.cleanup()方法": 69,
    "GPIO.setmod()": 70,
    "GPIO.setup()": 71,
    "GPIO.setup()方法": 72,
    "GPIO.setup(18, GPIO.IN, pull_up_down=GPIO.PUD_UP)": 73,
    "GPIO21": 74,
    "GPIO口": 75,
    "GPIO库": 76,
    "GPIO引脚": 77,
    "GPU": 78,
    "GPU 委托": 79,
    "GPU代理": 80,
    "GStreamer": 81,
    "GStreamer管道": 82,
    "GlobalAveragePooling2D": 83,
    "Google": 84,
    "Google Arts & Culture": 85,
    "Google Assistant": 86,
    "Google Assistant，Google Photos，Uber，Airbnb，网易，爱奇艺，WPS 等": 87,
    "Google Photos": 88,
    "Google Pixel 4": 89,
    "Google 内部用于计算机视觉场景的解决方案": 90,
    "Google 推荐使用": 91,
    "Gradle 同步": 92,
    "HDMI 接口": 93,
    "HIGH电平": 94,
    "HTML 文件": 95,
    "HTML文件、JS文件和配置文件": 96,
    "Haar特征分类器": 97,
    "Haar特征的cascade分类器": 98,
    "Hello AI World": 99,
    "I2C库": 100,
    "I2C接口(SCL、SDA)": 101,
    "IMAGE_WIDTH, IMAGE_HEIGHT, testData, testxs, labels, preds": 102,
    "ImageDataGenerator": 103,
    "ImageProcessor": 104,
    "IoT领域": 105,
    "Java": 106,
    "JavaScript": 107,
    "JavaScript 语言版本的扩展": 108,
    "JetBot": 109,
    "JetPack SDK": 110,
    "Jetson": 111,
    "Jetson Nano": 112,
    "Jetson Nano 开发板": 113,
    "Jetson 项目社区": 114,
    "Jupyter Notebook": 115,
    "Jupyter Notebook 的全面升级": 116,
    "Jupyter Notebook、文本编辑器、终端以及各种个性化组件": 117,
    "Jupyter lab": 118,
    "JupyterLab": 119,
    "Keras": 120,
    "Keras H5": 121,
    "Keras Model 和 SavedModel": 122,
    "Keras模型": 123,
    "Keras的模型定义方式": 124,
    "LED": 125,
    "LED灯": 126,
    "LED的控制引脚": 127,
    "LOW电平": 128,
    "Laurence Moroney提供的剪刀、石头、布手势图像": 129,
    "Layers API": 130,
    "Layers API和Core API": 131,
    "Linux": 132,
    "Linux开发环境": 133,
    "MCU": 134,
    "MIPI": 135,
    "MIPI CSI-2 摄像头接口": 136,
    "MIPI的相机串行接口（CSI）端口": 137,
    "MIPI联盟发起的为移动应用处理器制定的开放标准": 138,
    "MNIST分类器": 139,
    "MNIST数据集": 140,
    "MaxPooling2D": 141,
    "Micro-USB 接口": 142,
    "MnistData": 143,
    "MobileNet": 144,
    "MobileNet V2": 145,
    "MobileNet 图像分类": 146,
    "MobileNetV2": 147,
    "MobileNets_v2": 148,
    "NVIDIA Jetson Nano": 149,
    "NVIDIA Jetson 开发者专区": 150,
    "NVIDIA Jetson 论坛": 151,
    "Object C": 152,
    "OpenCV": 153,
    "OpenCV 安装": 154,
    "OpenCV 编译": 155,
    "OperatorCode": 156,
    "PCB板上针脚的物理位置对应的编号（1~40）": 157,
    "PWM接口": 158,
    "Parcel": 159,
    "Pi 4B与Pi v3+安装包是不同的": 160,
    "PoseNet模型": 161,
    "PoseNet示例应用程序": 162,
    "Post training quantization": 163,
    "Python": 164,
    "Python 2.7": 165,
    "Python API": 166,
    "Python API 或命令行": 167,
    "Python GPIO": 168,
    "Python 代码片段": 169,
    "Python 官方集成的工具": 170,
    "Python 模块": 171,
    "Python 源码包": 172,
    "Python 相关程序模块": 173,
    "Python 相关程序模块会拷贝到/opt/python": 174,
    "Python, C++, Java, Swift, Javascript等多种语言": 175,
    "Python安装": 176,
    "Python开发环境": 177,
    "Python程序": 178,
    "Quantization-aware training": 179,
    "RPI.GPIO库": 180,
    "RandomFlip": 181,
    "RandomRotation": 182,
    "Raspberry Camera V2": 183,
    "Raspberry Pi、Arducam等常见的相机模块": 184,
    "Raspbian软件仓库镜像": 185,
    "SD Memory Card Formatter": 186,
    "SPI库": 187,
    "SPI接口（MISO、MOSI、CLK、CS片选信号SPICE0_N）": 188,
    "SavedModel": 189,
    "SavedModel 和 Keras Sequential 两种模型导出方法和格式": 190,
    "Script Tag": 191,
    "Sequential模型": 192,
    "SparseCategoricalCrossentropy": 193,
    "Swift": 194,
    "TF Mobile": 195,
    "TFLite": 196,
    "TFLite model": 197,
    "TFLite 文件格式": 198,
    "TFLite 模型": 199,
    "TFLite 模型文件": 200,
    "TFLite 模型文件格式": 201,
    "TFLite 模型转换器": 202,
    "TFLite 模型转换过程": 203,
    "TFLite 解释器": 204,
    "TFLite 解释执行器": 205,
    "TFLite 转换器": 206,
    "TFLiteConverter": 207,
    "TFLiteConverter.from_concrete_functions()": 208,
    "TFLiteConverter.from_keras_model()": 209,
    "TFLiteConverter.from_saved_model()": 210,
    "TFLite模型": 211,
    "TFLite解释器": 212,
    "TFLite解释器和GPU代理": 213,
    "TFLite转换": 214,
    "TFMini": 215,
    "TensorBoard": 216,
    "TensorFLow-vis": 217,
    "TensorFlow": 218,
    "TensorFlow 2.x 模型": 219,
    "TensorFlow GPU 版本": 220,
    "TensorFlow Hub": 221,
    "TensorFlow Hub 上可搜索到的模型": 222,
    "TensorFlow Lite": 223,
    "TensorFlow Lite AAR": 224,
    "TensorFlow Lite API": 225,
    "TensorFlow Lite 工作流程": 226,
    "TensorFlow Lite 开发工作流程": 227,
    "TensorFlow Lite 推理": 228,
    "TensorFlow Lite 支持库": 229,
    "TensorFlow Lite 模型": 230,
    "TensorFlow Lite 模型文件": 231,
    "TensorFlow Lite 的工作流程": 232,
    "TensorFlow Lite 算子库": 233,
    "TensorFlow Lite 解释器": 234,
    "TensorFlow Lite 解释器(Interpreter)": 235,
    "TensorFlow Lite 解释器(Interpreter)、TensorFlow Lite 转换器(Converter)、算子库(Op kernels)、硬件加速代理(Hardware accelerator delegate)": 236,
    "TensorFlow Lite 解释执行器": 237,
    "TensorFlow Lite 转换器": 238,
    "TensorFlow Lite 转换器(Converter)": 239,
    "TensorFlow Lite 转换器命令行工具": 240,
    "TensorFlow Lite开发智能质检一体机和智能读码机": 241,
    "TensorFlow Lite支持库": 242,
    "TensorFlow Lite模型": 243,
    "TensorFlow Lite模型转换器": 244,
    "TensorFlow Lite解释器": 245,
    "TensorFlow Lite进行Live Caption功能": 246,
    "TensorFlow Lite进行OCR处理": 247,
    "TensorFlow Lite进行图像处理": 248,
    "TensorFlow Lite进行文字处理": 249,
    "TensorFlow Lite进行热词唤醒": 250,
    "TensorFlow Lite进行视频中的AR效果": 251,
    "TensorFlow Lite进行语音识别": 252,
    "TensorFlow Lite避开障碍物": 253,
    "TensorFlow 模型": 254,
    "TensorFlow 模型导出": 255,
    "TensorFlow 的 JavaScript 版本": 256,
    "TensorFlow.js": 257,
    "TensorFlow.js 的 JavaScript 库": 258,
    "TensorFlow.js 进行浏览器可视化的一组实用工具库": 259,
    "TensorFlow.js中的中心数据单元，是一维或多维数组": 260,
    "TensorFlow.js预训练模型": 261,
    "TensorFlow团队开发的产品": 262,
    "TensorFlow的轻量级版本": 263,
    "TensorLabel": 264,
    "Tensorflow Lite post-training quantization": 265,
    "Tensorflow.js": 266,
    "Text, Image, Video 和 Publishers 等类别": 267,
    "Total params: 2,626,821, Trainable params: 368,837, Non-trainable params: 2,257,984": 268,
    "UART串口接口（TXD、RXD）": 269,
    "UART库": 270,
    "USB摄像头": 271,
    "Uber": 272,
    "VNC Viewer": 273,
    "VNC 服务": 274,
    "VNC 服务器": 275,
    "WPS": 276,
    "Web Server for Chrome": 277,
    "Web 开发新手": 278,
    "Webpack": 279,
    "Wiring Pi": 280,
    "Wiring Pi编号": 281,
    "Wiring Pi编号模式": 282,
    "XML文件": 283,
    "['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips']": 284,
    "[null, 10]": 285,
    "[null, 28, 28, 1]": 286,
    "aaptOptions": 287,
    "accuracy评估指标": 288,
    "add_event_detect()函数": 289,
    "allocate_tensors()": 290,
    "android/start 目录下的项目模板和 finish 目录下的完整代码": 291,
    "base_model, Conv2D, Dropout, GlobalAveragePooling2D, Dense": 292,
    "batchSize": 293,
    "batchSize设置为512": 294,
    "batch_size": 295,
    "build.gradle": 296,
    "buildscript": 297,
    "callback": 298,
    "callbacks设置为fitCallbacks": 299,
    "categorical_crossentropy": 300,
    "categorical_crossentropy损失函数": 301,
    "cdn.jsdelivr.net": 302,
    "class_names": 303,
    "compare_faces": 304,
    "conv2d, dropout, global_average_pooling2d, dense": 305,
    "convertToTensor函数": 306,
    "converter.convert": 307,
    "createModel()": 308,
    "data.js": 309,
    "data.js文件": 310,
    "dense层": 311,
    "dependencies": 312,
    "dispose": 313,
    "dispose和tf.tidy两种内存管理方法": 314,
    "dlib图形库": 315,
    "doPrediction函数": 316,
    "epochs": 317,
    "epochs设置为10": 318,
    "examples/lite/codelabs": 319,
    "face_cascade.detectMultiScale": 320,
    "face_distance": 321,
    "face_encodings": 322,
    "face_locations": 323,
    "face_recognition": 324,
    "fine_tune_at": 325,
    "flatten层": 326,
    "flow_from_directory": 327,
    "flower_classification": 328,
    "flower_classification/android/finish": 329,
    "from_saved_model(), from_keras_model(), from_concrete_functions() 类方法": 330,
    "functional模型": 331,
    "get_input_details()": 332,
    "get_output_details()": 333,
    "get_tensor()": 334,
    "gpio readall命令": 335,
    "hub.KerasLayer": 336,
    "iOS 开发人员": 337,
    "iOS 开发人员使用": 338,
    "iOS 开发者通过 CocoaPods 获取": 339,
    "images和labels": 340,
    "import * as tf from '@tensorflow/tfjs' console.log(tf.version.tfjs) const shape = [2, 3]; // 2 rows, 3 columns const a = tf.tensor": 341,
    "include_top=False": 342,
    "index.html": 343,
    "index.js": 344,
    "inputShape: [28, 28, 1], kernelSize: 5, filters: 8, strides: 1, activation: 'relu', kernelInitializer: 'VarianceScaling'": 345,
    "inputShape为[1]，units为1，useBias为true": 346,
    "interpreter": 347,
    "interpreter.get_tensor()": 348,
    "invoke()": 349,
    "jtop 命令": 350,
    "jupyter": 351,
    "jupyter_notebook_config.py": 352,
    "jupyter的配置文件": 353,
    "label.txt": 354,
    "lambda函数": 355,
    "layers.Flatten()": 356,
    "make install": 357,
    "make 命令": 358,
    "microSD 卡": 359,
    "microSD 卡插槽": 360,
    "minNeighbors": 361,
    "mnist项目": 362,
    "mobilenetv2_1.00_224": 363,
    "model": 364,
    "model.compile": 365,
    "model.fit": 366,
    "model.fit()": 367,
    "model.predict()": 368,
    "model.tflite": 369,
    "model.tflite和label.txt": 370,
    "model.tflite文件": 371,
    "model.trainable = False": 372,
    "nano或vi": 373,
    "nextTestBatch": 374,
    "nextTrainBatch": 375,
    "nextTrainBatch和nextTestBatch两个public方法": 376,
    "noCompress \"tflite\"": 377,
    "normalization_layer": 378,
    "number_of_times_to_upsample参数设置对图像进行多少次上采样以查找人脸": 379,
    "optimizer, loss, metrics": 380,
    "package.json": 381,
    "pip": 382,
    "pip install": 383,
    "pip3": 384,
    "poolSize: [2, 2], strides: [2, 2]": 385,
    "predict()": 386,
    "print(\"button pressed!\")": 387,
    "python": 388,
    "recognizeImage方法": 389,
    "repositories和dependencies": 390,
    "saved_model_dir": 391,
    "saved_model_keras_dir": 392,
    "schema.fbs 文件": 393,
    "script 标签": 394,
    "sequential模型": 395,
    "sequential模型和functional模型": 396,
    "set_tensor()": 397,
    "showAccuracy函数": 398,
    "showConfusion函数": 399,
    "shuffle": 400,
    "shuffle设置为true": 401,
    "softmax": 402,
    "target_size": 403,
    "target_size 参数": 404,
    "target_size和batch_size": 405,
    "tensor()": 406,
    "tensorflow-lite": 407,
    "tf.image.resize": 408,
    "tf.keras model": 409,
    "tf.keras.Sequential": 410,
    "tf.keras.callbacks.TensorBoard": 411,
    "tf.keras.layers.Dense(units=1)": 412,
    "tf.keras.layers.Dense(units=1, input_shape=[1])": 413,
    "tf.keras.layers.Dense(units=16, activation='relu')": 414,
    "tf.keras.losses.SparseCategoricalCrossentropy": 415,
    "tf.keras.models.Sequential": 416,
    "tf.keras.optimizers.Adam": 417,
    "tf.linspace()": 418,
    "tf.lite.TFLiteConverter": 419,
    "tf.lite.TFLiteConverter.from_keras_model": 420,
    "tf.lite.TFLiteConverter.from_saved_model": 421,
    "tf.losses.meanSquaredError": 422,
    "tf.model()": 423,
    "tf.nn.softmax": 424,
    "tf.nn.softmax()": 425,
    "tf.saved_model.save": 426,
    "tf.sequential()": 427,
    "tf.sequential()和tf.model()两种创建模型的方式": 428,
    "tf.sequential()对象、输入层和输出层": 429,
    "tf.tensor": 430,
    "tf.tensor2d": 431,
    "tf.tidy": 432,
    "tf.tidy()": 433,
    "tf.train.adam()": 434,
    "tfjs-examples/mnist": 435,
    "tfjs-vis": 436,
    "tflite_convert": 437,
    "tflite_convert --keras_model_file=/tmp/mobilenet_keras_model.h5 --output_file=/tmp/mobilenet.tflite": 438,
    "tflite_convert --saved_model_dir=/tmp/mobilenet_saved_model --output_file=/tmp/mobilenet.tflite": 439,
    "tfvis.render.scatterplot": 440,
    "tfvis.show.modelSummary": 441,
    "time.sleep()方法": 442,
    "train_ds, validation_data=val_ds, epochs=NUM_EPOCHS, callbacks=tensorboard_callback": 443,
    "train_generator": 444,
    "train_generator, steps_per_epoch=len(train_generator), epochs=epochs, validation_data=val_generator, validation_steps=len(val_generator)": 445,
    "ui.js": 446,
    "units为1，useBias为true": 447,
    "useBias": 448,
    "val_ds": 449,
    "val_generator": 450,
    "validationData设置为[testXs, testYs]": 451,
    "vino": 452,
    "wait_for_edge()函数": 453,
    "yarn": 454,
    "一个使用数据流图进行数值计算的开源软件库": 455,
    "一个强大、简单、易上手的人脸识别开源项目": 456,
    "一个端到端的机器学习开源框架": 457,
    "一个缩减版的 TensorFlow，简化了算子集，也缩小了运行库": 458,
    "一个轻量、快速、兼容度高的专门针对移动式应用场景的深度学习工具": 459,
    "一种数据结构，包含了在解决特定问题时训练得到的机器学习网络的逻辑和知识": 460,
    "一种特定的矩阵用来呈现算法性能的可视化效果": 461,
    "一系列的函数，这些函数以一个或多个张量作为输入，并输出另一个张量": 462,
    "一系列的计算节点": 463,
    "一组工具，可帮助开发者在移动设备、嵌入式设备和 IoT 设备上运行 TensorFlow 模型": 464,
    "一组数字引脚，可用于将树莓派连接到其他电子设备": 465,
    "上拉电阻": 466,
    "下载 OpenCV": 467,
    "下载源代码使用GIT工具下载代码，然后编译安装": 468,
    "下载特定版本的Python": 469,
    "下载解压": 470,
    "不支持 CUDA": 471,
    "不改变基础模型的各项参数变量": 472,
    "不清除内部函数的返回值": 473,
    "不需要原始模型构建代码就可以运行": 474,
    "不需要原有模型中最后的神经网络层": 475,
    "不需要序列化或可以创造自己的序列化方法": 476,
    "不需要改变模型": 477,
    "与 TensorFlow 一起安装": 478,
    "与 TensorFlow 的核心算子库略有不同": 479,
    "与树莓派结合可以将项目与现实世界轻松的联系起来": 480,
    "与许多流行的配件兼容": 481,
    "串联在LED和电源之间限制电流": 482,
    "为 TensorFlow.js 提供浏览器可视化功能": 483,
    "为了高性能场景创建的序列化库": 484,
    "主要应用于游戏场景": 485,
    "了解模型效率、调试超参数": 486,
    "二维卷积层": 487,
    "二维卷积层、最大池化层、flatten层和dense层": 488,
    "二进制文件很小": 489,
    "二进制文件的大小约为 1 MB（针对 32 位 ARM build）": 490,
    "交叉熵（categoricalCrossentropy）": 491,
    "人体姿势估计": 492,
    "人脸检测": 493,
    "人脸检测、检测面部特征点、给脸部编码、从编码中找出人的名字": 494,
    "人脸识别": 495,
    "仅用于开发的程序包": 496,
    "仅适用于卷积神经网络的一个子集": 497,
    "从 Keras Model 转换模型": 498,
    "从 Python 官网下载": 499,
    "从 SavedModel 转换模型": 500,
    "从MNIST数据集中随机批量提取MNIST图像": 501,
    "从头编译": 502,
    "从指定目录生成训练数据批次": 503,
    "从指定目录生成验证数据批次": 504,
    "从测试集中返回一批图像及其标签": 505,
    "从训练集中返回一批随机图像及其标签": 506,
    "以最小精度下降来训练网络": 507,
    "优化分类任务": 508,
    "优化器": 509,
    "优化器、损失函数和评估指标": 510,
    "优化模型": 511,
    "优化模型大小和性能": 512,
    "优化的 FlatBuffer 格式": 513,
    "优化的 FlatBuffer 格式，以 .tflite 为文件扩展名": 514,
    "优化的算子内核": 515,
    "会导致多次输出": 516,
    "估计图像或视频中的人体姿势": 517,
    "位于/home/pi/.jupyter/目录下": 518,
    "作为判断训练结果的参数": 519,
    "作为损失函数用于模型训练": 520,
    "作为最终的Dense层激活函数进行分类": 521,
    "作为模型优化算法": 522,
    "作为迁移学习的基础模型": 523,
    "使权重和激活值的 Post training 更简单": 524,
    "使用 CocoaPods for Swift or Objective-C": 525,
    "使用 FlatBuffers 定义了 TFLite 模型文件格式": 526,
    "使用 GPU 加速模型的运算，提高运算效率": 527,
    "使用 JavaScript，降低前端工程师入门门槛": 528,
    "使用 Python API 进行转换": 529,
    "使用 SavedModel 格式存储": 530,
    "使用 TFLite 模型转换器转换成 TFLite 文件格式(FlatBuffers 格式)": 531,
    "使用 TFLite 转换器转换模型": 532,
    "使用 TensorFlow Lite AAR": 533,
    "使用 TensorFlow Lite 支持库预处理模型输入和后处理模型输出": 534,
    "使用 TensorFlow Lite 解释器（提供多种语言的 API）在设备端运行模型": 535,
    "使用 TensorFlow Lite 转换器将模型转换为 TensorFlow Lite 格式": 536,
    "使用3×3的卷积核，并在输出上使用 Relu 激活函数": 537,
    "使用BCM编号、物理引脚Broad编号": 538,
    "使用C、C++开发并且可以被其他语言包使用，例如python、ruby或者PHP等": 539,
    "使用GPU来加速数学运算": 540,
    "使用TFLiteConverter转换模型": 541,
    "使用tf.lite.TFLiteConverter.from_saved_model方法": 542,
    "使用tf.saved_model.save保存模型": 543,
    "使用优化器'sgd'和损失函数'mean_squared_error'": 544,
    "使用低学习率编译模型": 545,
    "使用低学习率重新编译模型": 546,
    "使用层构建模型": 547,
    "使用已编译好的库": 548,
    "使用带有 JetPack SDK 和 NVIDIA TensorRT 的 Jetson 开发工具包上的预训练模型进行实时图像分类和对象检测": 549,
    "使用构建工具进行探索": 550,
    "使用模型从未见过的测试数据评估分类器准确性": 551,
    "使用模型优化工具包缩减模型的大小并提高其效率，同时最大限度地降低对准确率的影响": 552,
    "使用测试数据集评估模型": 553,
    "使用深度可分离的卷积": 554,
    "使用滑动滤波器窗口学习空间不变的变换": 555,
    "使用脚本代码": 556,
    "使用计算机视觉相关的模型": 557,
    "使用预训练量化进行模型转换": 558,
    "保存模型": 559,
    "保护LED和GPIO引脚": 560,
    "保持了很多通用性": 561,
    "保留原来大规模训练的优势": 562,
    "修改build.gradle配置": 563,
    "借助低级运算构建模型": 564,
    "做了移动设备相关的优化": 565,
    "允许解释器在设备的 GPU 上运行适当的运算符": 566,
    "全能 IDE": 567,
    "全连接 (Full Connected) 层": 568,
    "全连接层": 569,
    "全连接网络": 570,
    "关闭LED灯": 571,
    "具有shape属性定义数组形状": 572,
    "内存回收问题突出": 573,
    "内存高效": 574,
    "内存高效，支持将文件映射到内存中，然后直接进行读取和解释": 575,
    "内置的算子": 576,
    "写一段简单的测试代码": 577,
    "冻结前100层": 578,
    "冻结预训练模型并更新分类器权重": 579,
    "准备训练集和验证集": 580,
    "准确度": 581,
    "减少了内存碎片化": 582,
    "减少服务器的运算，提高服务器资源利用，增强客户端响应速度": 583,
    "出门问问智能音箱": 584,
    "分类结果的概率": 585,
    "创建 Python 包的软链接": 586,
    "创建0~1之间平均分配的100个值": 587,
    "创建TFLiteConverter实例并加载Keras模型": 588,
    "创建Tensor实例的主要构造函数": 589,
    "创建、训练和导出自定义 TensorFlow Lite 模型": 590,
    "创建实例并加载模型": 591,
    "创建模型的高级API示例": 592,
    "创建解释器、分配张量": 593,
    "创新奇智": 594,
    "初始化TensorFlow Lite解释器": 595,
    "利用 Android 神经网络 API（Android NN API)": 596,
    "利用在同一域中的较大数据集上训练的模型所学习的特征": 597,
    "利用手机上的加速器，比如 GPU 或者 DSP": 598,
    "前几层学习通用特征": 599,
    "功能强大的编程语言，易于使用，易于阅读和编写": 600,
    "功能强大的边缘计算设备": 601,
    "功能强大，交互式、富文本，还有丰富的插件、主题修改、多语言支持": 602,
    "功能接线的引脚号（如TXD、PWM0等）": 603,
    "加载 @tensorflow/tfjs 和 @tensorflow/tfjs-vis 的代码": 604,
    "加载 TensorFlow Hub 上的模型": 605,
    "加载和运行TFLite模型": 606,
    "加载数据、定义模型、训练循环并指定UI元素": 607,
    "加载数据并准备进行训练, 定义模型结构, 训练模型并监视其性能, 评估模型": 608,
    "加载模型": 609,
    "加载模型、转换数据、运行模型推理、解释输出": 610,
    "加速模型推理": 611,
    "动态显示训练的过程": 612,
    "包含一个完整的TensorFlow程序，不仅包含权重值，还包含计算": 613,
    "包含命令行工具gpio，可以用来设置GPIO管脚，读写GPIO管脚，甚至在Shell脚本中使用来控制GPIO管脚": 614,
    "包含完整的TensorFlow程序": 615,
    "包含完整的TensorFlow程序，包括权重和计算": 616,
    "单一芯片的小型计算机": 617,
    "单个图像的维度为[28,28,1]": 618,
    "占用更少的磁盘和内存，更快更高效": 619,
    "卷积基": 620,
    "卷积层": 621,
    "卷积层与全连接层": 622,
    "卷积层输入": 623,
    "取消冻结模型的顶层": 624,
    "取消冻结模型的顶层，设置 base_model.trainable = True": 625,
    "受限于GPU内存的大小": 626,
    "只使用在C语言中": 627,
    "只提供了基本的转化功能": 628,
    "只有 Ethernet 有线网络，不包括无线网卡": 629,
    "可从 github 下载源码": 630,
    "可以从同一网络上的另一台计算机控制 Jetson Nano 开发板": 631,
    "可以使用 C++ 和 Python 提供的 API": 632,
    "可以使用 Java 或 C++ API": 633,
    "可以使用树莓派摄像头，IMX219模组800万像素": 634,
    "可以使用自己的 TensorFlow 模型、在线查找模型，或者从的 TensorFlow 预训练模型中选择一个模型直接使用或重新训练": 635,
    "可以创建任何非闭环的计算图": 636,
    "可以添加--no-cache-dir参数来避免缓存问题": 637,
    "可以直接使用cv2.videocapture(2)打开": 638,
    "可以直接在 Objective-C 代码中使用 C API": 639,
    "可以直接在浏览器中运行，无需安装或借助后端": 640,
    "可以通过软件编程进行控制": 641,
    "可以配置为输入或输出": 642,
    "可在手机、嵌入式 Linux 设备和微控制器等很多不同类型的硬件上运行": 643,
    "可能导致模型过拟合": 644,
    "可视化模型训练的过程和结果": 645,
    "可视化模型预测结果和原始数据": 646,
    "名字": 647,
    "后端处理任务": 648,
    "启动图标": 649,
    "命令行 TensorFlow Lite 转换器命令行工具": 650,
    "命令行与 Python API": 651,
    "命令行工具": 652,
    "命令行工具和 Python API": 653,
    "四引脚按键": 654,
    "四脚按键开关": 655,
    "回调函数": 656,
    "图像分类、对象检测、姿势估计、文本恶意检测": 657,
    "图像分类、物体检测、分割和语音处理": 658,
    "图像分类任务": 659,
    "图像和视频处理": 660,
    "在 Android 与 iOS 平台上使用": 661,
    "在 Android 应用中使用 TFLite 解释器运行它": 662,
    "在 Android 应用中运行模型": 663,
    "在 Android 设备上运行图像识别模型": 664,
    "在 HTML 中直接引用 TensorFlow.js 发布的 NPM 包中已经打包安装好的 JavaScript 代码": 665,
    "在 Jetson Nano 开发板上手动编译与安装": 666,
    "在 MCU 上甚至可以小于 100KB": 667,
    "在 Node 环境进行运算的速度与 Python 速度不相上下": 668,
    "在18号引脚处设置": 669,
    "在不同设备上使用硬件加速": 670,
    "在内存有限的移动环境中使用": 671,
    "在图像中检测面部": 672,
    "在大规模数据处理上不如Python高效": 673,
    "在安卓应用只需 1 兆左右的运行环境": 674,
    "在官网下载安装包后安装": 675,
    "在小型数据集上训练模型": 676,
    "在把模型嵌入到实际的二进制文件中时兼容性存在较大挑战": 677,
    "在有 GPU 加速的手机上运行 MobileNet 图像分类，模型运行速度可以提高": 678,
    "在有 GPU 加速的手机上运行，模型运行速度可以提高 5.5 倍": 679,
    "在每一个训练周期显示训练情况": 680,
    "在浏览器上开发模型或运行已训练的模型": 681,
    "在浏览器中加载": 682,
    "在浏览器中训练模型": 683,
    "在浏览器环境中实现深度学习的功能": 684,
    "在生产环境中不需要": 685,
    "在移动端、嵌入式和物联网设备上运行 TensorFlow 模型": 686,
    "在移动设备、嵌入式设备和 IoT 设备上运行 TensorFlow 模型": 687,
    "在移动设备、嵌入式设备和IoT设备上运行TensorFlow模型": 688,
    "在移动设备上实现性能大幅度提升": 689,
    "在移动设备上执行模型推理": 690,
    "在给定设备上实现性能、模型大小和准确性的理想平衡": 691,
    "在训练过程中不能用于训练": 692,
    "在设备端运行 TFLite 模型": 693,
    "在资源限制严重的移动和嵌入式设备上执行": 694,
    "在边缘设备上运行 TensorFlow 模型推理": 695,
    "在边缘设备上运行 TensorFlow 模型推理的官方框架": 696,
    "基于 TF Mobile 的经验，也继承了 TFMini 和内部其他类似项目的很多优秀工作": 697,
    "基于 TF Mobile 的经验，继承了 TFMini 和内部其他类似项目的优秀工作": 698,
    "基于 WebGL 加速的开放源代码 JavaScript 机器学习库": 699,
    "基于一个流线型的架构，使用深度可分离的卷积": 700,
    "基于流线型架构的轻量级深层神经网络": 701,
    "基于现有模型构建 Interpreter": 702,
    "基于现有的模型进行继续训练": 703,
    "增加一个事件的检测函数": 704,
    "处理媒体应用程序": 705,
    "处理简单的数据": 706,
    "多个张量": 707,
    "多媒体框架": 708,
    "多种级别的量化支持": 709,
    "大大降低移动端及IoT设备端的深度学习技术门槛": 710,
    "大电流可能损坏LED和供电设备": 711,
    "大而复杂的模型": 712,
    "头信息和脚本加载部分": 713,
    "如果仅使用支持常见图像分类模型（InceptionV3 和 MobileNet）所需的运算符，二进制文件的大小不到 300 KB": 714,
    "委托（Delegates）": 715,
    "子图": 716,
    "子图本身的输入和输出": 717,
    "存储已安装软件包的名称和版本": 718,
    "存储模型权重": 719,
    "存储镜像": 720,
    "存放训练好的模型供开发人员复用": 721,
    "学习 AI 和构建有趣应用程序": 722,
    "安装 OpenCV 项目": 723,
    "安装 Python 包依赖项": 724,
    "安装 TensorFlow": 725,
    "安装Python": 726,
    "安装Python包": 727,
    "安装依赖": 728,
    "安装依赖项": 729,
    "安装和升级 pip3": 730,
    "安装所需的系统包": 731,
    "安装的 TensorFlow 版本必须与正在使用的 JetPack 版本一致": 732,
    "安装相应的依赖包": 733,
    "完全基于 JavaScript 从头开发、训练和部署模型": 734,
    "完成分类": 735,
    "完成分类任务": 736,
    "官方已经停止维护": 737,
    "官方推荐使用的 AC8265": 738,
    "定义的神经元网络层与层之间的关系较为随意": 739,
    "定义需要监视的指标": 740,
    "定位图像中的人脸位置": 741,
    "实现 TensorFlow Lite 功能": 742,
    "实现 headless 远程桌面访问 Jetson Nano": 743,
    "实现下载和访问mnist数据集": 744,
    "实现了一组优化的算子内核": 745,
    "实现花卉识别 app": 746,
    "对 SIMD 指令功能特别有益": 747,
    "对images进行归一化处理": 748,
    "对手写数字的图像进行分类": 749,
    "对数据降维": 750,
    "对模型的权重产生更一致且变化较小的渐变更新": 751,
    "对现有 CPU 平台的支持": 752,
    "对训练图像随机变换引入多样性": 753,
    "对训练图像随机变换的方法来人为引入样本多样性": 754,
    "对随机目标函数执行一阶梯度优化的算法，基于适应性低阶矩估计": 755,
    "导致每个时期的梯度更新数量较少": 756,
    "将 NPM 模块转换为在线可以引用的免费服务": 757,
    "将 TensorFlow 模型格式转换为 TensorFlow Lite 的 Python API": 758,
    "将 TensorFlow 模型转换为 TFLite 文件格式(FlatBuffers 格式)": 759,
    "将 TensorFlow 模型转换为 TFLite 格式": 760,
    "将 TensorFlow 模型转换为 TensorFlow Lite 格式": 761,
    "将 TensorFlow 模型转换为方便解释器使用的格式": 762,
    "将Keras模型转换为TensorFlow Lite模型": 763,
    "将SavedModel转换为TensorFlow Lite格式": 764,
    "将TensorFlow模型转换为轻量级格式": 765,
    "将images和labels映射为归一化后的images和原始labels": 766,
    "将maven源google()和jcenter()替换为国内镜像": 767,
    "将三维张量展开到1维": 768,
    "将人脸编码列表与候选编码进行比较，以查看它们是否匹配": 769,
    "将前一层的输出平铺到一个向量中": 770,
    "将图片分类到1000类": 771,
    "将外设操作视为文件读写": 772,
    "将彩色图像转换为灰度图像，检测人脸并在边界周围绘制矩形": 773,
    "将所有图像加载到一个模型需要的特定的大小": 774,
    "将数据转换为TensorFlow可读的张量格式": 775,
    "将模型保存为TFLite兼容格式": 776,
    "将模型加载到内存中": 777,
    "将模型嵌入到二进制文件中，这样就可以在设备上运行和部署模型": 778,
    "将模型文件拷贝到assets目录": 779,
    "将模型显示在浏览器中": 780,
    "将模型输出与类别标签关联": 781,
    "将模型输出转换为概率分布": 782,
    "将特征转换为每个图像对应一个1280元素向量": 783,
    "将网络的每一层简单的叠在一起": 784,
    "将输入数据转换成模型接收的形式或排布，如resize原始图像到模型输入大小": 785,
    "将输入的 TensorFlow 模型生成 TensorFlow Lite 模型": 786,
    "将镜像写入 microSD 卡": 787,
    "将需要的层按顺序写在一个列表里，然后将列表作为sequential()函数的输入": 788,
    "小一点的模型": 789,
    "尝试简化 TensorFlow 并在移动设备上运行": 790,
    "工业物联智能设备开发": 791,
    "已经训练好的分类器": 792,
    "常开触点": 793,
    "常开触点和常闭触点": 794,
    "常见的移动/嵌入式平台": 795,
    "常闭触点": 796,
    "幅度过大会导致预训练模型遗忘已学特征": 797,
    "应对快速变化需求的软件开发模式": 798,
    "应用于树莓派的GPIO控制库函数": 799,
    "底层 Core API 和最高级的 Layers API": 800,
    "廉价且周边设备多": 801,
    "延迟一秒钟": 802,
    "延迟较低": 803,
    "建议最小采用 64 GB UHS-1 卡": 804,
    "开关去抖": 805,
    "开关抖动": 806,
    "开发依赖": 807,
    "开箱即用的开发库，无需编写基础复杂的数学问题": 808,
    "引入 index.js": 809,
    "引入优化以减小二进制文件的大小和提高性能": 810,
    "引用 Model 的内存缓冲区的一片区域，提高内存效率": 811,
    "张量": 812,
    "张量(Tensor)": 813,
    "张量形状是 (image_height, image_width, color_channels)": 814,
    "归一化操作": 815,
    "当压力撤销时电路恢复": 816,
    "当压力施压时电路接通": 817,
    "形状是 (224,224, 3)": 818,
    "微控制器(MCU)支持": 819,
    "微调": 820,
    "微调过程": 821,
    "必须在开机前先装上去，系统才能识别": 822,
    "快速启动": 823,
    "快速的启动并运行一组深度学习推理演示": 824,
    "忽略由于开关抖动引起的小于": 825,
    "恢复训练": 826,
    "成功配置好 CUDA": 827,
    "手写数字识别": 828,
    "手势识别数据集": 829,
    "手势识别项目": 830,
    "打乱数据顺序，创建特征向量和标签向量，转换为张量格式，进行归一化操作": 831,
    "打开现有 Android Studio 项目": 832,
    "打开项目图标": 833,
    "执行TensorFlow Lite模型的推理": 834,
    "执行一个函数并清除所有创建的中间张量": 835,
    "执行推理": 836,
    "执行最终的分类": 837,
    "执行模型推理": 838,
    "执行模型文件在输入数据上定义的运算符，输出推理结果": 839,
    "执行模型转换过程": 840,
    "批次大小": 841,
    "指定从哪个层开始进行微调": 842,
    "指定含有 TensorFlow 1.x 或者 2.0 使用 SavedModel 生成文件的绝对路径目录": 843,
    "指定含有 TensorFlow 1.x 或者 2.0 使用 tf.keras model 生成 HDF5 文件的绝对路径目录": 844,
    "指定引脚编号系统": 845,
    "指定输出文件的绝对路径": 846,
    "损失函数": 847,
    "接受 TFLite 模型": 848,
    "控制GPIO引脚": 849,
    "控制LED灯的亮暗": 850,
    "控制外部硬件设备": 851,
    "控制树莓派的GPIO": 852,
    "推理过程": 853,
    "描述人体各个部位的Haar特征值": 854,
    "描述构建和运行示例所需的依赖项": 855,
    "提供 5V⎓2A 的高品质电源为开发者套件供电": 856,
    "提供DOM根并调用JS脚本": 857,
    "提供了一个简单的 API，用于从 Java、Swift、Objective-C、C++ 和 Python 运行 TensorFlow Lite 模型": 858,
    "提供了一些转换工具压缩模型，进行算子融合并生成代码": 859,
    "提供了大量方便的工具，例如权重初始化，模型序列化，训练监测，可迁移性和安全检查": 860,
    "提供低级的机器学习构建模块和高级的类似Keras的API": 861,
    "提供多种语言的 API": 862,
    "提供开箱即用的预训练模型": 863,
    "提供经过充分认证的模型": 864,
    "提供训练好的模型，开发人员可以复用这些已经训练好且经过充分认证的模型": 865,
    "提取图像特征": 866,
    "提问或分享项目": 867,
    "提高性能的方法是训练预训练模型的顶层的权重以及刚添加的分类器的训练": 868,
    "提高模型性能": 869,
    "搭建本地服务器解决跨域问题": 870,
    "摄像头预捕获的图像宽度、高度、窗口显示的图像宽度、高度、捕获帧率、是否旋转图像": 871,
    "支持 GPU 硬件加速": 872,
    "支持BCM编号模式和物理引脚Broad编号模式": 873,
    "支持像素缩放和数据增强": 874,
    "支持图像大小调整和批次划分": 875,
    "支持大规模的模型训练和各种环境的部署": 876,
    "支持将文件映射到内存中，然后直接进行读取和解释，不需要额外解析": 877,
    "支持自定义输入形状和是否包含顶层分类器": 878,
    "支持设备端机器学习推断": 879,
    "支持量化": 880,
    "支持预装驱动程序的RPi相机": 881,
    "敏捷开发": 882,
    "教育": 883,
    "数据规范化和转换为张量类型": 884,
    "数据转换": 885,
    "数据采集、模型训练、参数调整、模型文件生成、网页端部署、摄像头检测": 886,
    "数据集": 887,
    "数据预处理": 888,
    "文字处理": 889,
    "无需使用ByteBuffer来处理图像，提供了方便的支持库来简化图像预处理": 890,
    "易于设置和使用": 891,
    "是一个层次的结构": 892,
    "显示每个类别的准确度": 893,
    "显示混淆矩阵": 894,
    "普通GPIO口": 895,
    "更换国内源如阿里、清华": 896,
    "更新可视化元素": 897,
    "更注重考虑实时性，内存高效": 898,
    "更谨慎地控制内存何时回收": 899,
    "更适合于边缘设备部署": 900,
    "最大池化层": 901,
    "有130个左右": 902,
    "有助于避免因错误的样本而改向错误的方向": 903,
    "有效的物品检测方法": 904,
    "有经验的开发者": 905,
    "服务器和移动端的部署": 906,
    "未满足的对等依赖 seedrandom@~": 907,
    "机器学习和计算机视觉应用": 908,
    "权重值和计算": 909,
    "构建CNN模型": 910,
    "构建Tensorflow.js模型来识别手写数字": 911,
    "构建和运行mnist代码": 912,
    "构建和运行机器学习模型": 913,
    "构建小型移动机器人、人脸签到打卡、口罩识别、智能门锁、智能音箱等复杂 AI 系统": 914,
    "构建工具": 915,
    "构建神经网络": 916,
    "构成检测目标的相邻矩形的最小个数(默认为3个)": 917,
    "查看开发板系统信息": 918,
    "查看树莓派的GPIO引脚信息": 919,
    "标准算子": 920,
    "树莓派": 921,
    "树莓派4B的18号引脚": 922,
    "树莓派GPIO": 923,
    "树莓派接口": 924,
    "树莓派的21号引脚": 925,
    "树莓派的官方编程语言": 926,
    "树莓派系统": 927,
    "树莓派通用输入/输出接口（GPIO）": 928,
    "核心运行时": 929,
    "格式修改、显示驱动程序协调和数据处理": 930,
    "格式化 microSD 卡": 931,
    "梯度更新": 932,
    "检测人脸": 933,
    "检测关键身体部位的位置": 934,
    "模型": 935,
    "模型优化": 936,
    "模型优化工具": 937,
    "模型优化工具包": 938,
    "模型可以跟 Python 等其他语言模型进行互转": 939,
    "模型执行流图": 940,
    "模型推理": 941,
    "模型文件和标签文件": 942,
    "模型文件已拷贝到 assets 目录": 943,
    "模型的计算节点": 944,
    "模型精度提高到98%": 945,
    "模型编译": 946,
    "模型训练": 947,
    "模型评估": 948,
    "模型转换": 949,
    "模型预测": 950,
    "比 CPU 执行更快的浮点矩阵运算": 951,
    "池化层": 952,
    "汽车油耗（MPG）": 953,
    "汽车的功率（Horsepower）": 954,
    "没有操作系统，只有内存": 955,
    "测试Python开发环境和查看当前Python版本": 956,
    "测试数据": 957,
    "测试的软件包、webpack或Babel": 958,
    "浏览器可以很好可视化机器训练过程": 959,
    "浏览器可调用设备的摄像头、麦克风等增加机器学习的应用场景": 960,
    "混淆矩阵": 961,
    "清华源": 962,
    "清理GPIO引脚的设置": 963,
    "清除张量或变量并释放其GPU内存": 964,
    "清除所有创建的中间张量并释放它们的GPU内存": 965,
    "激活->设置为输出状态->写入1点亮LED": 966,
    "激活函数": 967,
    "灵活的架构可以将模型部署到桌面、服务器或移动设备中的 CPU 或 GPU 上": 968,
    "点亮LED灯": 969,
    "爱奇艺": 970,
    "版本变化后 API 函数会改变": 971,
    "版本变化后API函数会改变": 972,
    "物体检测、人脸识别、图像分割等视觉任务": 973,
    "物理引脚Broad编号": 974,
    "特别为各种端侧设备优化的算子库": 975,
    "瓶颈层": 976,
    "生成 HDF5 文件的绝对路径目录": 977,
    "生成 SavedModel": 978,
    "生成SavedModel": 979,
    "生成一个批次一个批次的图片，以生成器的形式给模型训练": 980,
    "生成一个批次的图片，以生成器的形式给模型训练": 981,
    "生成批次的图片数据": 982,
    "生成配置文件": 983,
    "用于 5V 电源输入": 984,
    "用于处理数据": 985,
    "用于工具的配置中心": 986,
    "用于构建网页界面": 987,
    "用于特征提取": 988,
    "用于编写JavaScript代码": 989,
    "用于训练模型": 990,
    "用于预测": 991,
    "用作即插即用外围设备": 992,
    "用作启动设备和主存储器": 993,
    "用到的算子索引": 994,
    "用户在自己的工作台中使用 TensorFlow API 构造 TensorFlow 模型": 995,
    "用来转换 SavedModel 格式模型": 996,
    "用来转换 concrete functions": 997,
    "用来转换 tf.keras 模型": 998,
    "用来连接 DP 屏幕": 999,
    "电信号从低电平到高电平或从高电平到低电平状态的改变": 1000,
    "电阻": 1001,
    "登录 Jetson Nano": 1002,
    "监督学习中展示多个类别是否有混淆": 1003,
    "直接串联3.3V电源会产生大电流": 1004,
    "直接更新树莓派系统": 1005,
    "直接部署或用于迁移学习": 1006,
    "直流桶式插座": 1007,
    "相机模块": 1008,
    "相比 Protocol Buffer 有更高的性能和更小的大小": 1009,
    "硬件加速代理(Hardware accelerator delegate)": 1010,
    "确认 CUDA 已经被正常安装": 1011,
    "神经元权重计算中的偏置量": 1012,
    "离线语音识别": 1013,
    "科沃斯扫地机器人": 1014,
    "移动应用中的OCR处理": 1015,
    "移动端及IoT设备端的深度学习技术": 1016,
    "移除原有模型中最后的神经网络层（分类到1000类）": 1017,
    "端侧机器学习": 1018,
    "第一个卷积层": 1019,
    "第二、三卷积层": 1020,
    "简化图像预处理和输出处理": 1021,
    "简单的线性回归的实验": 1022,
    "算子优化": 1023,
    "算子优化和常见的编译优化": 1024,
    "算子优化和常见的编译优化，比如算子融合、常数折叠或无用代码删除等": 1025,
    "算子实现": 1026,
    "算子库(Op kernels)": 1027,
    "算子库和共享的内存缓冲区": 1028,
    "算子融合、常数折叠、无用代码删除": 1029,
    "类型: bool. (default False) Enables the converter and flags used in TF 1.x instead of TF 2.x": 1030,
    "类型: string. Full path of the output file": 1031,
    "类型: string. Full path to the Keras H5 model file": 1032,
    "类型: string. Full path to the SavedModel directory": 1033,
    "系统升级": 1034,
    "线性堆叠layers的模型": 1035,
    "编程语言": 1036,
    "编译 OpenCV": 1037,
    "编译 Python 模块": 1038,
    "缩减模型的大小并提高其效率，同时最大限度地降低对准确率的影响": 1039,
    "缩短开发周期": 1040,
    "网易": 1041,
    "网络环境较差时可以考虑更换源": 1042,
    "能够利用各种硬件加速": 1043,
    "能够执行完整全面的反向传播": 1044,
    "自定制算子": 1045,
    "节省训练时间和计算资源": 1046,
    "花卉数据集中的图片": 1047,
    "花卉识别 app": 1048,
    "花卉识别模型": 1049,
    "获取一些非常有意思的项目": 1050,
    "获取图像数据、处理图像、调用模型、绘制关键点": 1051,
    "获取张量数据": 1052,
    "获取指向张量的指针": 1053,
    "获取更多的 Jetson 平台信息": 1054,
    "观测开关去抖效果": 1055,
    "视频中的AR效果": 1056,
    "解决JavaScript内存回收问题": 1057,
    "解释器和转换器": 1058,
    "解释输出": 1059,
    "计算已知人脸和未知人脸特征向量的距离，距离越小表示两张人脸为同一个人的可能性越大": 1060,
    "计算机视觉应用": 1061,
    "计算节点的输入和输出": 1062,
    "计算预测结果的概率分布": 1063,
    "让输入输出映射到0-1之间，保证后期更有效地训练": 1064,
    "训练分类器查看数千个图像及其标签": 1065,
    "训练后量化": 1066,
    "训练数据": 1067,
    "训练数据和测试数据": 1068,
    "训练数据集很大且类似于预训练模型的数据集": 1069,
    "训练时从数据集中的不同类中随机选出一定数量的图像": 1070,
    "训练期间将不更新预训练网络的权重，只在 MobileNet V2基础模型上训练了几层": 1071,
    "训练模型": 1072,
    "训练模型时使用": 1073,
    "训练集": 1074,
    "训练预训练模型的顶层权重": 1075,
    "记录训练日志": 1076,
    "记录训练过程中的指标和计算图": 1077,
    "许多 CPU 和硬件加速器实现提供 SIMD 指令功能，对量化特别有益": 1078,
    "设置 OpenCV 的内容、位置和方式": 1079,
    "设置 converter.optimizations=[tf.lite.Optimize.DEFAULT]": 1080,
    "设置 model.trainable = False": 1081,
    "设置GPIO引脚模式": 1082,
    "设置GPIO引脚编号模式": 1083,
    "设置model.trainable = False": 1084,
    "设置上拉电阻": 1085,
    "设置为32，表示一次采样32条训练数据": 1086,
    "设置为50，表示遍历所有样本50次": 1087,
    "设置为true，表示打乱数据集": 1088,
    "设置前100层为不可训练": 1089,
    "设置图像加载的特定大小": 1090,
    "设置在训练中基础模型的参数不会被新的训练修改": 1091,
    "设置在训练中基础模型的各项参数变量不会被新的训练修改数据": 1092,
    "设置引脚为输入或输出模式": 1093,
    "设置训练时随机选出的图像数量": 1094,
    "设置访问密码": 1095,
    "设置输入张量值": 1096,
    "访问密码可以为空但建议设置": 1097,
    "评估指标": 1098,
    "评估模型时使用以检查模型对新数据的泛化情况": 1099,
    "评估训练有素的模型的性能": 1100,
    "识别剪刀、石头、布手势": 1101,
    "识别图像里的空间模式，例如线条和物体局部": 1102,
    "识别照相机所拍摄的花卉": 1103,
    "识别花卉图片": 1104,
    "识别输入图像": 1105,
    "请注意版本": 1106,
    "读取传感器数据，控制 LED 等外部设备": 1107,
    "读取传感器数据，控制外部设备": 1108,
    "读取输出张量值": 1109,
    "调整大小、裁剪、旋转和归一化图像": 1110,
    "调整数据集形状": 1111,
    "调整输入图像大小以匹配模型输入要求": 1112,
    "调用CSI摄像头和USB摄像头": 1113,
    "调用model.fit方法进行训练": 1114,
    "调用不同的硬件加速器比如 GPU 进行执行": 1115,
    "调用解释器的方式：try (Interpreter interpreter = new Interpreter(tensorflow_lite_model_file)) { interpreter.run(input, output); }": 1116,
    "负极连接到GND": 1117,
    "资源有限，不能训练网络，可能出现OOM错误": 1118,
    "跨平台运行，包括 Android、iOS 以及基于 Linux 的 IoT 设备和微控制器": 1119,
    "转换 SavedModel 格式模型": 1120,
    "转换 concrete functions": 1121,
    "转换 tf.keras 模型": 1122,
    "转换和运行 TensorFlow 模型所需的工具": 1123,
    "转换数据": 1124,
    "转换模型": 1125,
    "软件PWM库": 1126,
    "软件源配置文件": 1127,
    "软链接": 1128,
    "轻量、快速、兼容度高": 1129,
    "轻量级": 1130,
    "轻量级、快速启动、内存高效": 1131,
    "输入层": 1132,
    "输入输出用到的 Tensor 索引": 1133,
    "输入（Xs）和标签（Ys）": 1134,
    "输出宽度和高度会收缩": 1135,
    "输出层": 1136,
    "输出是一个三维的张量，其形状描述了 (height, width, channels)": 1137,
    "输出模式": 1138,
    "输出电压约为3.3V": 1139,
    "输出的通道数量取决于声明层时的 filters 参数": 1140,
    "输出的通道数量取决于声明层时的filters参数": 1141,
    "输出通道数为32": 1142,
    "输出通道数为64": 1143,
    "边缘": 1144,
    "边缘操作": 1145,
    "迁移学习": 1146,
    "运行 TensorFlow Lite 模型": 1147,
    "运行Sync Gradle": 1148,
    "运行TFLite推理并获取输出概率": 1149,
    "运行各种深度学习模型": 1150,
    "运行在 Node.js 或浏览器环境中": 1151,
    "运行已有的 Python 版 TensorFlow 模型": 1152,
    "运行服务监听的IP地址、端口、notebooks内核的目录、是否打开浏览器等配置项": 1153,
    "运行模型推理": 1154,
    "返回图像中每张人脸的128维人脸编码": 1155,
    "返回张量数据的副本": 1156,
    "返回的是数据的副本而非引用": 1157,
    "进行 VNC 连接": 1158,
    "进行内存清理工作，防止内存泄露": 1159,
    "进行大量张量操作时使用可能会很麻烦": 1160,
    "进行迁移学习，实现识别花卉模型": 1161,
    "连接LED灯的正极": 1162,
    "连接按键的一个引脚": 1163,
    "连接显示器、键盘和鼠标": 1164,
    "适用于以 Swift 和 Objective-C 编写的原生 iOS 库": 1165,
    "适用于多个平台": 1166,
    "适用于多个平台，提供了一个简单的 API": 1167,
    "适用于移动和嵌入式设备的轻量级推理框架": 1168,
    "选择模型": 1169,
    "选择模型、转换模型、部署到设备、优化模型": 1170,
    "选择模型、转换模型、部署到设备和优化模型": 1171,
    "逐步加载单个数据集的图像": 1172,
    "逐步加载数据集图像": 1173,
    "通过 SSH 或 VNC 服务远程访问": 1174,
    "通过 pip 安装": 1175,
    "通过 script 标签引入 index.js": 1176,
    "通过GPIO控制展示基础硬件控制能力": 1177,
    "通过tf.model()来创建LayersModel": 1178,
    "通过利用 WebGL 在 GPU 上执行计算大幅提高速度": 1179,
    "通过命令行转换模型": 1180,
    "通过浏览器访问": 1181,
    "通过脚本标签（script tags）或从 yarn（或者 NPM）安装并使用 Parcel，WebPack 或 Rollup 等工具构建工程": 1182,
    "通过计算每个滑动窗口的最大值来缩减卷积结果的大小": 1183,
    "通过许多正负样例中训练得到cascade方程，然后将其应用于其他图片": 1184,
    "通过跳线线连接到其他电路板或设备": 1185,
    "通过迁移学习实现花卉识别模型": 1186,
    "通过限流电阻串联到GPIO21": 1187,
    "郁金香(tulips)、玫瑰(roses)、浦公英(dandelion)、向日葵(sunflowers)、雏菊(daisy)": 1188,
    "部署TensorFlow Lite模型": 1189,
    "部署到设备": 1190,
    "部署在 Raspberry Pi 和基于 Arm64 的主板的嵌入式 Linux 系统上": 1191,
    "配置Jupyter lab的运行参数": 1192,
    "配置proxy或使用国内镜像": 1193,
    "配置文件": 1194,
    "配置模型的优化器和损失函数": 1195,
    "配置项目依赖": 1196,
    "采用 FlatBuffers 格式，支持将文件映射到内存中，然后直接进行读取和解释": 1197,
    "采用更小的模型格式": 1198,
    "释放张量占用的GPU内存": 1199,
    "重启树莓派后启动": 1200,
    "量化": 1201,
    "针对移动设备做了很多优化": 1202,
    "错误的连接和编程可能会导致设备损坏或故障": 1203,
    "防止Android在生成应用程序二进制文件时压缩TensorFlow Lite模型文件": 1204,
    "防止应用程序中的内存泄漏": 1205,
    "阻塞函数，会阻塞程序执行直到检测到一个边沿": 1206,
    "降低卷积层对位置的敏感": 1207,
    "降低存储器访问成本": 1208,
    "降低权重的精确表示，并且可选的降低存储和计算的激活值": 1209,
    "降低权重的精确表示，降低存储和计算的激活值": 1210,
    "降低用于读取和存储中间激活值的存储器访问成本": 1211,
    "随机打乱数据集以避免模型预测受图像顺序影响": 1212,
    "需先训练顶层分类器并设置预训练模型为不可训练": 1213,
    "需要root权限": 1214,
    "需要使用GStreamer读取视频流": 1215,
    "需要大约两个半小时": 1216,
    "需要更多灵活性和控制时使用": 1217,
    "需要更多的内存": 1218,
    "需要确认版本": 1219,
    "需要高准确率": 1220,
    "静态图片分辨率为3280 × 2464": 1221,
    "面包板、杜邦线公对母、LED灯、330欧姆电阻": 1222,
    "面部，眼睛，微笑等": 1223,
    "项目的清单文件": 1224,
    "预加载了ImageNet训练权重的深度学习模型": 1225,
    "预处理模型输入和后处理模型输出": 1226,
    "预处理输入图像": 1227,
    "预测": 1228,
    "预测汽车油耗效率": 1229,
    "预测汽车油耗（MPG）": 1230,
    "预测汽车的油耗效率 MPG": 1231,
    "预训练模型和全连接的分类器": 1232,
    "预训练模型将忘记它学到的东西": 1233,
    "验证集": 1234,
    "高层学习针对特定数据集的专用特征": 1235,
    "高性能": 1236,
    "高阶 tf.keras.* API（Keras 模型）或低阶 tf.* API（用于生成具体函数）": 1237,
    "默认包含1000类的分类层": 1238,
    "默认安装 JetPack 安装了对应的 OpenCV 不支持 CUDA 且版本是固定搭配的": 1239
  },
  "id_to_entity": {
    "0": "--enable_v1_converter",
    "1": "--keras_model_file",
    "2": "--output_file",
    "3": "--output_file, --saved_model_dir, --keras_model_file, --enable_v1_converter 参数",
    "4": "--saved_model_dir",
    "5": "-D WITH_QT=OFF 禁用了 Qt5 支持",
    "6": ".tflite",
    "7": "/etc/apt/sources.list",
    "8": "/opt/python 目录",
    "9": "/sys/class/gpio目录下的端口文件",
    "10": "/usr/bin/python",
    "11": "0.2s的响应时间",
    "12": "100",
    "13": "1080p30, 720p60以及640 × 480p90视频录像",
    "14": "128核NVIDIA Maxwell架构的GPU",
    "15": "14个引脚用于其他功能",
    "16": "155层",
    "17": "155层网络结构",
    "18": "2.0.0",
    "19": "2.3.0",
    "20": "26个引脚可以用作数字输入或输出",
    "21": "3个 Conv2D 和 2个 MaxPooling2D 层",
    "22": "40个 GPIO 引脚",
    "23": "40个GPIO引脚",
    "24": "5个节点",
    "25": "5个节点的输出层",
    "26": "6",
    "27": "800万像素、感光芯片为索尼IMX219",
    "28": "9",
    "29": "<!DOCTYPE html> <html> <head> <meta charset=\"utf-8\"> <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\"> <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">",
    "30": "<html> <body> <h4>TFJS example<hr/></h4> <div id=\"micro-out-div\">TensorFlow.js Test</div> <script src=\"./index.js\"> </script> </body> </html>",
    "31": "@tensorflow/tfjs",
    "32": "@tensorflow/tfjs-vis",
    "33": "Adam",
    "34": "Adam优化器",
    "35": "Airbnb",
    "36": "Android Studio",
    "37": "Android 应用",
    "38": "Android 开发人员",
    "39": "Android 开发人员使用",
    "40": "Android 开发者使用 JCenter Bintray 的 TFLite AAR",
    "41": "Android、iOS 和 Linux",
    "42": "Android、iOS、嵌入式设备、以及极小的 MCU 设备",
    "43": "Android环境部署",
    "44": "Android部署",
    "45": "B02版本有两路",
    "46": "BCM编号",
    "47": "BCM编号方式",
    "48": "Broadcom针脚号，即通常称的GPIO",
    "49": "C",
    "50": "C#",
    "51": "C++",
    "52": "CNN",
    "53": "CNN层",
    "54": "CSI 相机接口",
    "55": "CSI摄像头",
    "56": "CSI端口",
    "57": "ClassifierFloatMobileNet类",
    "58": "Classifier类",
    "59": "CocoaPods",
    "60": "Conv2D",
    "61": "Core API",
    "62": "DeepLearning.js",
    "63": "Dense",
    "64": "Display Port 接口",
    "65": "Etcher",
    "66": "Face Recognition",
    "67": "FlatBuffers",
    "68": "FlatBuffers 格式",
    "69": "GPIO.cleanup()方法",
    "70": "GPIO.setmod()",
    "71": "GPIO.setup()",
    "72": "GPIO.setup()方法",
    "73": "GPIO.setup(18, GPIO.IN, pull_up_down=GPIO.PUD_UP)",
    "74": "GPIO21",
    "75": "GPIO口",
    "76": "GPIO库",
    "77": "GPIO引脚",
    "78": "GPU",
    "79": "GPU 委托",
    "80": "GPU代理",
    "81": "GStreamer",
    "82": "GStreamer管道",
    "83": "GlobalAveragePooling2D",
    "84": "Google",
    "85": "Google Arts & Culture",
    "86": "Google Assistant",
    "87": "Google Assistant，Google Photos，Uber，Airbnb，网易，爱奇艺，WPS 等",
    "88": "Google Photos",
    "89": "Google Pixel 4",
    "90": "Google 内部用于计算机视觉场景的解决方案",
    "91": "Google 推荐使用",
    "92": "Gradle 同步",
    "93": "HDMI 接口",
    "94": "HIGH电平",
    "95": "HTML 文件",
    "96": "HTML文件、JS文件和配置文件",
    "97": "Haar特征分类器",
    "98": "Haar特征的cascade分类器",
    "99": "Hello AI World",
    "100": "I2C库",
    "101": "I2C接口(SCL、SDA)",
    "102": "IMAGE_WIDTH, IMAGE_HEIGHT, testData, testxs, labels, preds",
    "103": "ImageDataGenerator",
    "104": "ImageProcessor",
    "105": "IoT领域",
    "106": "Java",
    "107": "JavaScript",
    "108": "JavaScript 语言版本的扩展",
    "109": "JetBot",
    "110": "JetPack SDK",
    "111": "Jetson",
    "112": "Jetson Nano",
    "113": "Jetson Nano 开发板",
    "114": "Jetson 项目社区",
    "115": "Jupyter Notebook",
    "116": "Jupyter Notebook 的全面升级",
    "117": "Jupyter Notebook、文本编辑器、终端以及各种个性化组件",
    "118": "Jupyter lab",
    "119": "JupyterLab",
    "120": "Keras",
    "121": "Keras H5",
    "122": "Keras Model 和 SavedModel",
    "123": "Keras模型",
    "124": "Keras的模型定义方式",
    "125": "LED",
    "126": "LED灯",
    "127": "LED的控制引脚",
    "128": "LOW电平",
    "129": "Laurence Moroney提供的剪刀、石头、布手势图像",
    "130": "Layers API",
    "131": "Layers API和Core API",
    "132": "Linux",
    "133": "Linux开发环境",
    "134": "MCU",
    "135": "MIPI",
    "136": "MIPI CSI-2 摄像头接口",
    "137": "MIPI的相机串行接口（CSI）端口",
    "138": "MIPI联盟发起的为移动应用处理器制定的开放标准",
    "139": "MNIST分类器",
    "140": "MNIST数据集",
    "141": "MaxPooling2D",
    "142": "Micro-USB 接口",
    "143": "MnistData",
    "144": "MobileNet",
    "145": "MobileNet V2",
    "146": "MobileNet 图像分类",
    "147": "MobileNetV2",
    "148": "MobileNets_v2",
    "149": "NVIDIA Jetson Nano",
    "150": "NVIDIA Jetson 开发者专区",
    "151": "NVIDIA Jetson 论坛",
    "152": "Object C",
    "153": "OpenCV",
    "154": "OpenCV 安装",
    "155": "OpenCV 编译",
    "156": "OperatorCode",
    "157": "PCB板上针脚的物理位置对应的编号（1~40）",
    "158": "PWM接口",
    "159": "Parcel",
    "160": "Pi 4B与Pi v3+安装包是不同的",
    "161": "PoseNet模型",
    "162": "PoseNet示例应用程序",
    "163": "Post training quantization",
    "164": "Python",
    "165": "Python 2.7",
    "166": "Python API",
    "167": "Python API 或命令行",
    "168": "Python GPIO",
    "169": "Python 代码片段",
    "170": "Python 官方集成的工具",
    "171": "Python 模块",
    "172": "Python 源码包",
    "173": "Python 相关程序模块",
    "174": "Python 相关程序模块会拷贝到/opt/python",
    "175": "Python, C++, Java, Swift, Javascript等多种语言",
    "176": "Python安装",
    "177": "Python开发环境",
    "178": "Python程序",
    "179": "Quantization-aware training",
    "180": "RPI.GPIO库",
    "181": "RandomFlip",
    "182": "RandomRotation",
    "183": "Raspberry Camera V2",
    "184": "Raspberry Pi、Arducam等常见的相机模块",
    "185": "Raspbian软件仓库镜像",
    "186": "SD Memory Card Formatter",
    "187": "SPI库",
    "188": "SPI接口（MISO、MOSI、CLK、CS片选信号SPICE0_N）",
    "189": "SavedModel",
    "190": "SavedModel 和 Keras Sequential 两种模型导出方法和格式",
    "191": "Script Tag",
    "192": "Sequential模型",
    "193": "SparseCategoricalCrossentropy",
    "194": "Swift",
    "195": "TF Mobile",
    "196": "TFLite",
    "197": "TFLite model",
    "198": "TFLite 文件格式",
    "199": "TFLite 模型",
    "200": "TFLite 模型文件",
    "201": "TFLite 模型文件格式",
    "202": "TFLite 模型转换器",
    "203": "TFLite 模型转换过程",
    "204": "TFLite 解释器",
    "205": "TFLite 解释执行器",
    "206": "TFLite 转换器",
    "207": "TFLiteConverter",
    "208": "TFLiteConverter.from_concrete_functions()",
    "209": "TFLiteConverter.from_keras_model()",
    "210": "TFLiteConverter.from_saved_model()",
    "211": "TFLite模型",
    "212": "TFLite解释器",
    "213": "TFLite解释器和GPU代理",
    "214": "TFLite转换",
    "215": "TFMini",
    "216": "TensorBoard",
    "217": "TensorFLow-vis",
    "218": "TensorFlow",
    "219": "TensorFlow 2.x 模型",
    "220": "TensorFlow GPU 版本",
    "221": "TensorFlow Hub",
    "222": "TensorFlow Hub 上可搜索到的模型",
    "223": "TensorFlow Lite",
    "224": "TensorFlow Lite AAR",
    "225": "TensorFlow Lite API",
    "226": "TensorFlow Lite 工作流程",
    "227": "TensorFlow Lite 开发工作流程",
    "228": "TensorFlow Lite 推理",
    "229": "TensorFlow Lite 支持库",
    "230": "TensorFlow Lite 模型",
    "231": "TensorFlow Lite 模型文件",
    "232": "TensorFlow Lite 的工作流程",
    "233": "TensorFlow Lite 算子库",
    "234": "TensorFlow Lite 解释器",
    "235": "TensorFlow Lite 解释器(Interpreter)",
    "236": "TensorFlow Lite 解释器(Interpreter)、TensorFlow Lite 转换器(Converter)、算子库(Op kernels)、硬件加速代理(Hardware accelerator delegate)",
    "237": "TensorFlow Lite 解释执行器",
    "238": "TensorFlow Lite 转换器",
    "239": "TensorFlow Lite 转换器(Converter)",
    "240": "TensorFlow Lite 转换器命令行工具",
    "241": "TensorFlow Lite开发智能质检一体机和智能读码机",
    "242": "TensorFlow Lite支持库",
    "243": "TensorFlow Lite模型",
    "244": "TensorFlow Lite模型转换器",
    "245": "TensorFlow Lite解释器",
    "246": "TensorFlow Lite进行Live Caption功能",
    "247": "TensorFlow Lite进行OCR处理",
    "248": "TensorFlow Lite进行图像处理",
    "249": "TensorFlow Lite进行文字处理",
    "250": "TensorFlow Lite进行热词唤醒",
    "251": "TensorFlow Lite进行视频中的AR效果",
    "252": "TensorFlow Lite进行语音识别",
    "253": "TensorFlow Lite避开障碍物",
    "254": "TensorFlow 模型",
    "255": "TensorFlow 模型导出",
    "256": "TensorFlow 的 JavaScript 版本",
    "257": "TensorFlow.js",
    "258": "TensorFlow.js 的 JavaScript 库",
    "259": "TensorFlow.js 进行浏览器可视化的一组实用工具库",
    "260": "TensorFlow.js中的中心数据单元，是一维或多维数组",
    "261": "TensorFlow.js预训练模型",
    "262": "TensorFlow团队开发的产品",
    "263": "TensorFlow的轻量级版本",
    "264": "TensorLabel",
    "265": "Tensorflow Lite post-training quantization",
    "266": "Tensorflow.js",
    "267": "Text, Image, Video 和 Publishers 等类别",
    "268": "Total params: 2,626,821, Trainable params: 368,837, Non-trainable params: 2,257,984",
    "269": "UART串口接口（TXD、RXD）",
    "270": "UART库",
    "271": "USB摄像头",
    "272": "Uber",
    "273": "VNC Viewer",
    "274": "VNC 服务",
    "275": "VNC 服务器",
    "276": "WPS",
    "277": "Web Server for Chrome",
    "278": "Web 开发新手",
    "279": "Webpack",
    "280": "Wiring Pi",
    "281": "Wiring Pi编号",
    "282": "Wiring Pi编号模式",
    "283": "XML文件",
    "284": "['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips']",
    "285": "[null, 10]",
    "286": "[null, 28, 28, 1]",
    "287": "aaptOptions",
    "288": "accuracy评估指标",
    "289": "add_event_detect()函数",
    "290": "allocate_tensors()",
    "291": "android/start 目录下的项目模板和 finish 目录下的完整代码",
    "292": "base_model, Conv2D, Dropout, GlobalAveragePooling2D, Dense",
    "293": "batchSize",
    "294": "batchSize设置为512",
    "295": "batch_size",
    "296": "build.gradle",
    "297": "buildscript",
    "298": "callback",
    "299": "callbacks设置为fitCallbacks",
    "300": "categorical_crossentropy",
    "301": "categorical_crossentropy损失函数",
    "302": "cdn.jsdelivr.net",
    "303": "class_names",
    "304": "compare_faces",
    "305": "conv2d, dropout, global_average_pooling2d, dense",
    "306": "convertToTensor函数",
    "307": "converter.convert",
    "308": "createModel()",
    "309": "data.js",
    "310": "data.js文件",
    "311": "dense层",
    "312": "dependencies",
    "313": "dispose",
    "314": "dispose和tf.tidy两种内存管理方法",
    "315": "dlib图形库",
    "316": "doPrediction函数",
    "317": "epochs",
    "318": "epochs设置为10",
    "319": "examples/lite/codelabs",
    "320": "face_cascade.detectMultiScale",
    "321": "face_distance",
    "322": "face_encodings",
    "323": "face_locations",
    "324": "face_recognition",
    "325": "fine_tune_at",
    "326": "flatten层",
    "327": "flow_from_directory",
    "328": "flower_classification",
    "329": "flower_classification/android/finish",
    "330": "from_saved_model(), from_keras_model(), from_concrete_functions() 类方法",
    "331": "functional模型",
    "332": "get_input_details()",
    "333": "get_output_details()",
    "334": "get_tensor()",
    "335": "gpio readall命令",
    "336": "hub.KerasLayer",
    "337": "iOS 开发人员",
    "338": "iOS 开发人员使用",
    "339": "iOS 开发者通过 CocoaPods 获取",
    "340": "images和labels",
    "341": "import * as tf from '@tensorflow/tfjs' console.log(tf.version.tfjs) const shape = [2, 3]; // 2 rows, 3 columns const a = tf.tensor",
    "342": "include_top=False",
    "343": "index.html",
    "344": "index.js",
    "345": "inputShape: [28, 28, 1], kernelSize: 5, filters: 8, strides: 1, activation: 'relu', kernelInitializer: 'VarianceScaling'",
    "346": "inputShape为[1]，units为1，useBias为true",
    "347": "interpreter",
    "348": "interpreter.get_tensor()",
    "349": "invoke()",
    "350": "jtop 命令",
    "351": "jupyter",
    "352": "jupyter_notebook_config.py",
    "353": "jupyter的配置文件",
    "354": "label.txt",
    "355": "lambda函数",
    "356": "layers.Flatten()",
    "357": "make install",
    "358": "make 命令",
    "359": "microSD 卡",
    "360": "microSD 卡插槽",
    "361": "minNeighbors",
    "362": "mnist项目",
    "363": "mobilenetv2_1.00_224",
    "364": "model",
    "365": "model.compile",
    "366": "model.fit",
    "367": "model.fit()",
    "368": "model.predict()",
    "369": "model.tflite",
    "370": "model.tflite和label.txt",
    "371": "model.tflite文件",
    "372": "model.trainable = False",
    "373": "nano或vi",
    "374": "nextTestBatch",
    "375": "nextTrainBatch",
    "376": "nextTrainBatch和nextTestBatch两个public方法",
    "377": "noCompress \"tflite\"",
    "378": "normalization_layer",
    "379": "number_of_times_to_upsample参数设置对图像进行多少次上采样以查找人脸",
    "380": "optimizer, loss, metrics",
    "381": "package.json",
    "382": "pip",
    "383": "pip install",
    "384": "pip3",
    "385": "poolSize: [2, 2], strides: [2, 2]",
    "386": "predict()",
    "387": "print(\"button pressed!\")",
    "388": "python",
    "389": "recognizeImage方法",
    "390": "repositories和dependencies",
    "391": "saved_model_dir",
    "392": "saved_model_keras_dir",
    "393": "schema.fbs 文件",
    "394": "script 标签",
    "395": "sequential模型",
    "396": "sequential模型和functional模型",
    "397": "set_tensor()",
    "398": "showAccuracy函数",
    "399": "showConfusion函数",
    "400": "shuffle",
    "401": "shuffle设置为true",
    "402": "softmax",
    "403": "target_size",
    "404": "target_size 参数",
    "405": "target_size和batch_size",
    "406": "tensor()",
    "407": "tensorflow-lite",
    "408": "tf.image.resize",
    "409": "tf.keras model",
    "410": "tf.keras.Sequential",
    "411": "tf.keras.callbacks.TensorBoard",
    "412": "tf.keras.layers.Dense(units=1)",
    "413": "tf.keras.layers.Dense(units=1, input_shape=[1])",
    "414": "tf.keras.layers.Dense(units=16, activation='relu')",
    "415": "tf.keras.losses.SparseCategoricalCrossentropy",
    "416": "tf.keras.models.Sequential",
    "417": "tf.keras.optimizers.Adam",
    "418": "tf.linspace()",
    "419": "tf.lite.TFLiteConverter",
    "420": "tf.lite.TFLiteConverter.from_keras_model",
    "421": "tf.lite.TFLiteConverter.from_saved_model",
    "422": "tf.losses.meanSquaredError",
    "423": "tf.model()",
    "424": "tf.nn.softmax",
    "425": "tf.nn.softmax()",
    "426": "tf.saved_model.save",
    "427": "tf.sequential()",
    "428": "tf.sequential()和tf.model()两种创建模型的方式",
    "429": "tf.sequential()对象、输入层和输出层",
    "430": "tf.tensor",
    "431": "tf.tensor2d",
    "432": "tf.tidy",
    "433": "tf.tidy()",
    "434": "tf.train.adam()",
    "435": "tfjs-examples/mnist",
    "436": "tfjs-vis",
    "437": "tflite_convert",
    "438": "tflite_convert --keras_model_file=/tmp/mobilenet_keras_model.h5 --output_file=/tmp/mobilenet.tflite",
    "439": "tflite_convert --saved_model_dir=/tmp/mobilenet_saved_model --output_file=/tmp/mobilenet.tflite",
    "440": "tfvis.render.scatterplot",
    "441": "tfvis.show.modelSummary",
    "442": "time.sleep()方法",
    "443": "train_ds, validation_data=val_ds, epochs=NUM_EPOCHS, callbacks=tensorboard_callback",
    "444": "train_generator",
    "445": "train_generator, steps_per_epoch=len(train_generator), epochs=epochs, validation_data=val_generator, validation_steps=len(val_generator)",
    "446": "ui.js",
    "447": "units为1，useBias为true",
    "448": "useBias",
    "449": "val_ds",
    "450": "val_generator",
    "451": "validationData设置为[testXs, testYs]",
    "452": "vino",
    "453": "wait_for_edge()函数",
    "454": "yarn",
    "455": "一个使用数据流图进行数值计算的开源软件库",
    "456": "一个强大、简单、易上手的人脸识别开源项目",
    "457": "一个端到端的机器学习开源框架",
    "458": "一个缩减版的 TensorFlow，简化了算子集，也缩小了运行库",
    "459": "一个轻量、快速、兼容度高的专门针对移动式应用场景的深度学习工具",
    "460": "一种数据结构，包含了在解决特定问题时训练得到的机器学习网络的逻辑和知识",
    "461": "一种特定的矩阵用来呈现算法性能的可视化效果",
    "462": "一系列的函数，这些函数以一个或多个张量作为输入，并输出另一个张量",
    "463": "一系列的计算节点",
    "464": "一组工具，可帮助开发者在移动设备、嵌入式设备和 IoT 设备上运行 TensorFlow 模型",
    "465": "一组数字引脚，可用于将树莓派连接到其他电子设备",
    "466": "上拉电阻",
    "467": "下载 OpenCV",
    "468": "下载源代码使用GIT工具下载代码，然后编译安装",
    "469": "下载特定版本的Python",
    "470": "下载解压",
    "471": "不支持 CUDA",
    "472": "不改变基础模型的各项参数变量",
    "473": "不清除内部函数的返回值",
    "474": "不需要原始模型构建代码就可以运行",
    "475": "不需要原有模型中最后的神经网络层",
    "476": "不需要序列化或可以创造自己的序列化方法",
    "477": "不需要改变模型",
    "478": "与 TensorFlow 一起安装",
    "479": "与 TensorFlow 的核心算子库略有不同",
    "480": "与树莓派结合可以将项目与现实世界轻松的联系起来",
    "481": "与许多流行的配件兼容",
    "482": "串联在LED和电源之间限制电流",
    "483": "为 TensorFlow.js 提供浏览器可视化功能",
    "484": "为了高性能场景创建的序列化库",
    "485": "主要应用于游戏场景",
    "486": "了解模型效率、调试超参数",
    "487": "二维卷积层",
    "488": "二维卷积层、最大池化层、flatten层和dense层",
    "489": "二进制文件很小",
    "490": "二进制文件的大小约为 1 MB（针对 32 位 ARM build）",
    "491": "交叉熵（categoricalCrossentropy）",
    "492": "人体姿势估计",
    "493": "人脸检测",
    "494": "人脸检测、检测面部特征点、给脸部编码、从编码中找出人的名字",
    "495": "人脸识别",
    "496": "仅用于开发的程序包",
    "497": "仅适用于卷积神经网络的一个子集",
    "498": "从 Keras Model 转换模型",
    "499": "从 Python 官网下载",
    "500": "从 SavedModel 转换模型",
    "501": "从MNIST数据集中随机批量提取MNIST图像",
    "502": "从头编译",
    "503": "从指定目录生成训练数据批次",
    "504": "从指定目录生成验证数据批次",
    "505": "从测试集中返回一批图像及其标签",
    "506": "从训练集中返回一批随机图像及其标签",
    "507": "以最小精度下降来训练网络",
    "508": "优化分类任务",
    "509": "优化器",
    "510": "优化器、损失函数和评估指标",
    "511": "优化模型",
    "512": "优化模型大小和性能",
    "513": "优化的 FlatBuffer 格式",
    "514": "优化的 FlatBuffer 格式，以 .tflite 为文件扩展名",
    "515": "优化的算子内核",
    "516": "会导致多次输出",
    "517": "估计图像或视频中的人体姿势",
    "518": "位于/home/pi/.jupyter/目录下",
    "519": "作为判断训练结果的参数",
    "520": "作为损失函数用于模型训练",
    "521": "作为最终的Dense层激活函数进行分类",
    "522": "作为模型优化算法",
    "523": "作为迁移学习的基础模型",
    "524": "使权重和激活值的 Post training 更简单",
    "525": "使用 CocoaPods for Swift or Objective-C",
    "526": "使用 FlatBuffers 定义了 TFLite 模型文件格式",
    "527": "使用 GPU 加速模型的运算，提高运算效率",
    "528": "使用 JavaScript，降低前端工程师入门门槛",
    "529": "使用 Python API 进行转换",
    "530": "使用 SavedModel 格式存储",
    "531": "使用 TFLite 模型转换器转换成 TFLite 文件格式(FlatBuffers 格式)",
    "532": "使用 TFLite 转换器转换模型",
    "533": "使用 TensorFlow Lite AAR",
    "534": "使用 TensorFlow Lite 支持库预处理模型输入和后处理模型输出",
    "535": "使用 TensorFlow Lite 解释器（提供多种语言的 API）在设备端运行模型",
    "536": "使用 TensorFlow Lite 转换器将模型转换为 TensorFlow Lite 格式",
    "537": "使用3×3的卷积核，并在输出上使用 Relu 激活函数",
    "538": "使用BCM编号、物理引脚Broad编号",
    "539": "使用C、C++开发并且可以被其他语言包使用，例如python、ruby或者PHP等",
    "540": "使用GPU来加速数学运算",
    "541": "使用TFLiteConverter转换模型",
    "542": "使用tf.lite.TFLiteConverter.from_saved_model方法",
    "543": "使用tf.saved_model.save保存模型",
    "544": "使用优化器'sgd'和损失函数'mean_squared_error'",
    "545": "使用低学习率编译模型",
    "546": "使用低学习率重新编译模型",
    "547": "使用层构建模型",
    "548": "使用已编译好的库",
    "549": "使用带有 JetPack SDK 和 NVIDIA TensorRT 的 Jetson 开发工具包上的预训练模型进行实时图像分类和对象检测",
    "550": "使用构建工具进行探索",
    "551": "使用模型从未见过的测试数据评估分类器准确性",
    "552": "使用模型优化工具包缩减模型的大小并提高其效率，同时最大限度地降低对准确率的影响",
    "553": "使用测试数据集评估模型",
    "554": "使用深度可分离的卷积",
    "555": "使用滑动滤波器窗口学习空间不变的变换",
    "556": "使用脚本代码",
    "557": "使用计算机视觉相关的模型",
    "558": "使用预训练量化进行模型转换",
    "559": "保存模型",
    "560": "保护LED和GPIO引脚",
    "561": "保持了很多通用性",
    "562": "保留原来大规模训练的优势",
    "563": "修改build.gradle配置",
    "564": "借助低级运算构建模型",
    "565": "做了移动设备相关的优化",
    "566": "允许解释器在设备的 GPU 上运行适当的运算符",
    "567": "全能 IDE",
    "568": "全连接 (Full Connected) 层",
    "569": "全连接层",
    "570": "全连接网络",
    "571": "关闭LED灯",
    "572": "具有shape属性定义数组形状",
    "573": "内存回收问题突出",
    "574": "内存高效",
    "575": "内存高效，支持将文件映射到内存中，然后直接进行读取和解释",
    "576": "内置的算子",
    "577": "写一段简单的测试代码",
    "578": "冻结前100层",
    "579": "冻结预训练模型并更新分类器权重",
    "580": "准备训练集和验证集",
    "581": "准确度",
    "582": "减少了内存碎片化",
    "583": "减少服务器的运算，提高服务器资源利用，增强客户端响应速度",
    "584": "出门问问智能音箱",
    "585": "分类结果的概率",
    "586": "创建 Python 包的软链接",
    "587": "创建0~1之间平均分配的100个值",
    "588": "创建TFLiteConverter实例并加载Keras模型",
    "589": "创建Tensor实例的主要构造函数",
    "590": "创建、训练和导出自定义 TensorFlow Lite 模型",
    "591": "创建实例并加载模型",
    "592": "创建模型的高级API示例",
    "593": "创建解释器、分配张量",
    "594": "创新奇智",
    "595": "初始化TensorFlow Lite解释器",
    "596": "利用 Android 神经网络 API（Android NN API)",
    "597": "利用在同一域中的较大数据集上训练的模型所学习的特征",
    "598": "利用手机上的加速器，比如 GPU 或者 DSP",
    "599": "前几层学习通用特征",
    "600": "功能强大的编程语言，易于使用，易于阅读和编写",
    "601": "功能强大的边缘计算设备",
    "602": "功能强大，交互式、富文本，还有丰富的插件、主题修改、多语言支持",
    "603": "功能接线的引脚号（如TXD、PWM0等）",
    "604": "加载 @tensorflow/tfjs 和 @tensorflow/tfjs-vis 的代码",
    "605": "加载 TensorFlow Hub 上的模型",
    "606": "加载和运行TFLite模型",
    "607": "加载数据、定义模型、训练循环并指定UI元素",
    "608": "加载数据并准备进行训练, 定义模型结构, 训练模型并监视其性能, 评估模型",
    "609": "加载模型",
    "610": "加载模型、转换数据、运行模型推理、解释输出",
    "611": "加速模型推理",
    "612": "动态显示训练的过程",
    "613": "包含一个完整的TensorFlow程序，不仅包含权重值，还包含计算",
    "614": "包含命令行工具gpio，可以用来设置GPIO管脚，读写GPIO管脚，甚至在Shell脚本中使用来控制GPIO管脚",
    "615": "包含完整的TensorFlow程序",
    "616": "包含完整的TensorFlow程序，包括权重和计算",
    "617": "单一芯片的小型计算机",
    "618": "单个图像的维度为[28,28,1]",
    "619": "占用更少的磁盘和内存，更快更高效",
    "620": "卷积基",
    "621": "卷积层",
    "622": "卷积层与全连接层",
    "623": "卷积层输入",
    "624": "取消冻结模型的顶层",
    "625": "取消冻结模型的顶层，设置 base_model.trainable = True",
    "626": "受限于GPU内存的大小",
    "627": "只使用在C语言中",
    "628": "只提供了基本的转化功能",
    "629": "只有 Ethernet 有线网络，不包括无线网卡",
    "630": "可从 github 下载源码",
    "631": "可以从同一网络上的另一台计算机控制 Jetson Nano 开发板",
    "632": "可以使用 C++ 和 Python 提供的 API",
    "633": "可以使用 Java 或 C++ API",
    "634": "可以使用树莓派摄像头，IMX219模组800万像素",
    "635": "可以使用自己的 TensorFlow 模型、在线查找模型，或者从的 TensorFlow 预训练模型中选择一个模型直接使用或重新训练",
    "636": "可以创建任何非闭环的计算图",
    "637": "可以添加--no-cache-dir参数来避免缓存问题",
    "638": "可以直接使用cv2.videocapture(2)打开",
    "639": "可以直接在 Objective-C 代码中使用 C API",
    "640": "可以直接在浏览器中运行，无需安装或借助后端",
    "641": "可以通过软件编程进行控制",
    "642": "可以配置为输入或输出",
    "643": "可在手机、嵌入式 Linux 设备和微控制器等很多不同类型的硬件上运行",
    "644": "可能导致模型过拟合",
    "645": "可视化模型训练的过程和结果",
    "646": "可视化模型预测结果和原始数据",
    "647": "名字",
    "648": "后端处理任务",
    "649": "启动图标",
    "650": "命令行 TensorFlow Lite 转换器命令行工具",
    "651": "命令行与 Python API",
    "652": "命令行工具",
    "653": "命令行工具和 Python API",
    "654": "四引脚按键",
    "655": "四脚按键开关",
    "656": "回调函数",
    "657": "图像分类、对象检测、姿势估计、文本恶意检测",
    "658": "图像分类、物体检测、分割和语音处理",
    "659": "图像分类任务",
    "660": "图像和视频处理",
    "661": "在 Android 与 iOS 平台上使用",
    "662": "在 Android 应用中使用 TFLite 解释器运行它",
    "663": "在 Android 应用中运行模型",
    "664": "在 Android 设备上运行图像识别模型",
    "665": "在 HTML 中直接引用 TensorFlow.js 发布的 NPM 包中已经打包安装好的 JavaScript 代码",
    "666": "在 Jetson Nano 开发板上手动编译与安装",
    "667": "在 MCU 上甚至可以小于 100KB",
    "668": "在 Node 环境进行运算的速度与 Python 速度不相上下",
    "669": "在18号引脚处设置",
    "670": "在不同设备上使用硬件加速",
    "671": "在内存有限的移动环境中使用",
    "672": "在图像中检测面部",
    "673": "在大规模数据处理上不如Python高效",
    "674": "在安卓应用只需 1 兆左右的运行环境",
    "675": "在官网下载安装包后安装",
    "676": "在小型数据集上训练模型",
    "677": "在把模型嵌入到实际的二进制文件中时兼容性存在较大挑战",
    "678": "在有 GPU 加速的手机上运行 MobileNet 图像分类，模型运行速度可以提高",
    "679": "在有 GPU 加速的手机上运行，模型运行速度可以提高 5.5 倍",
    "680": "在每一个训练周期显示训练情况",
    "681": "在浏览器上开发模型或运行已训练的模型",
    "682": "在浏览器中加载",
    "683": "在浏览器中训练模型",
    "684": "在浏览器环境中实现深度学习的功能",
    "685": "在生产环境中不需要",
    "686": "在移动端、嵌入式和物联网设备上运行 TensorFlow 模型",
    "687": "在移动设备、嵌入式设备和 IoT 设备上运行 TensorFlow 模型",
    "688": "在移动设备、嵌入式设备和IoT设备上运行TensorFlow模型",
    "689": "在移动设备上实现性能大幅度提升",
    "690": "在移动设备上执行模型推理",
    "691": "在给定设备上实现性能、模型大小和准确性的理想平衡",
    "692": "在训练过程中不能用于训练",
    "693": "在设备端运行 TFLite 模型",
    "694": "在资源限制严重的移动和嵌入式设备上执行",
    "695": "在边缘设备上运行 TensorFlow 模型推理",
    "696": "在边缘设备上运行 TensorFlow 模型推理的官方框架",
    "697": "基于 TF Mobile 的经验，也继承了 TFMini 和内部其他类似项目的很多优秀工作",
    "698": "基于 TF Mobile 的经验，继承了 TFMini 和内部其他类似项目的优秀工作",
    "699": "基于 WebGL 加速的开放源代码 JavaScript 机器学习库",
    "700": "基于一个流线型的架构，使用深度可分离的卷积",
    "701": "基于流线型架构的轻量级深层神经网络",
    "702": "基于现有模型构建 Interpreter",
    "703": "基于现有的模型进行继续训练",
    "704": "增加一个事件的检测函数",
    "705": "处理媒体应用程序",
    "706": "处理简单的数据",
    "707": "多个张量",
    "708": "多媒体框架",
    "709": "多种级别的量化支持",
    "710": "大大降低移动端及IoT设备端的深度学习技术门槛",
    "711": "大电流可能损坏LED和供电设备",
    "712": "大而复杂的模型",
    "713": "头信息和脚本加载部分",
    "714": "如果仅使用支持常见图像分类模型（InceptionV3 和 MobileNet）所需的运算符，二进制文件的大小不到 300 KB",
    "715": "委托（Delegates）",
    "716": "子图",
    "717": "子图本身的输入和输出",
    "718": "存储已安装软件包的名称和版本",
    "719": "存储模型权重",
    "720": "存储镜像",
    "721": "存放训练好的模型供开发人员复用",
    "722": "学习 AI 和构建有趣应用程序",
    "723": "安装 OpenCV 项目",
    "724": "安装 Python 包依赖项",
    "725": "安装 TensorFlow",
    "726": "安装Python",
    "727": "安装Python包",
    "728": "安装依赖",
    "729": "安装依赖项",
    "730": "安装和升级 pip3",
    "731": "安装所需的系统包",
    "732": "安装的 TensorFlow 版本必须与正在使用的 JetPack 版本一致",
    "733": "安装相应的依赖包",
    "734": "完全基于 JavaScript 从头开发、训练和部署模型",
    "735": "完成分类",
    "736": "完成分类任务",
    "737": "官方已经停止维护",
    "738": "官方推荐使用的 AC8265",
    "739": "定义的神经元网络层与层之间的关系较为随意",
    "740": "定义需要监视的指标",
    "741": "定位图像中的人脸位置",
    "742": "实现 TensorFlow Lite 功能",
    "743": "实现 headless 远程桌面访问 Jetson Nano",
    "744": "实现下载和访问mnist数据集",
    "745": "实现了一组优化的算子内核",
    "746": "实现花卉识别 app",
    "747": "对 SIMD 指令功能特别有益",
    "748": "对images进行归一化处理",
    "749": "对手写数字的图像进行分类",
    "750": "对数据降维",
    "751": "对模型的权重产生更一致且变化较小的渐变更新",
    "752": "对现有 CPU 平台的支持",
    "753": "对训练图像随机变换引入多样性",
    "754": "对训练图像随机变换的方法来人为引入样本多样性",
    "755": "对随机目标函数执行一阶梯度优化的算法，基于适应性低阶矩估计",
    "756": "导致每个时期的梯度更新数量较少",
    "757": "将 NPM 模块转换为在线可以引用的免费服务",
    "758": "将 TensorFlow 模型格式转换为 TensorFlow Lite 的 Python API",
    "759": "将 TensorFlow 模型转换为 TFLite 文件格式(FlatBuffers 格式)",
    "760": "将 TensorFlow 模型转换为 TFLite 格式",
    "761": "将 TensorFlow 模型转换为 TensorFlow Lite 格式",
    "762": "将 TensorFlow 模型转换为方便解释器使用的格式",
    "763": "将Keras模型转换为TensorFlow Lite模型",
    "764": "将SavedModel转换为TensorFlow Lite格式",
    "765": "将TensorFlow模型转换为轻量级格式",
    "766": "将images和labels映射为归一化后的images和原始labels",
    "767": "将maven源google()和jcenter()替换为国内镜像",
    "768": "将三维张量展开到1维",
    "769": "将人脸编码列表与候选编码进行比较，以查看它们是否匹配",
    "770": "将前一层的输出平铺到一个向量中",
    "771": "将图片分类到1000类",
    "772": "将外设操作视为文件读写",
    "773": "将彩色图像转换为灰度图像，检测人脸并在边界周围绘制矩形",
    "774": "将所有图像加载到一个模型需要的特定的大小",
    "775": "将数据转换为TensorFlow可读的张量格式",
    "776": "将模型保存为TFLite兼容格式",
    "777": "将模型加载到内存中",
    "778": "将模型嵌入到二进制文件中，这样就可以在设备上运行和部署模型",
    "779": "将模型文件拷贝到assets目录",
    "780": "将模型显示在浏览器中",
    "781": "将模型输出与类别标签关联",
    "782": "将模型输出转换为概率分布",
    "783": "将特征转换为每个图像对应一个1280元素向量",
    "784": "将网络的每一层简单的叠在一起",
    "785": "将输入数据转换成模型接收的形式或排布，如resize原始图像到模型输入大小",
    "786": "将输入的 TensorFlow 模型生成 TensorFlow Lite 模型",
    "787": "将镜像写入 microSD 卡",
    "788": "将需要的层按顺序写在一个列表里，然后将列表作为sequential()函数的输入",
    "789": "小一点的模型",
    "790": "尝试简化 TensorFlow 并在移动设备上运行",
    "791": "工业物联智能设备开发",
    "792": "已经训练好的分类器",
    "793": "常开触点",
    "794": "常开触点和常闭触点",
    "795": "常见的移动/嵌入式平台",
    "796": "常闭触点",
    "797": "幅度过大会导致预训练模型遗忘已学特征",
    "798": "应对快速变化需求的软件开发模式",
    "799": "应用于树莓派的GPIO控制库函数",
    "800": "底层 Core API 和最高级的 Layers API",
    "801": "廉价且周边设备多",
    "802": "延迟一秒钟",
    "803": "延迟较低",
    "804": "建议最小采用 64 GB UHS-1 卡",
    "805": "开关去抖",
    "806": "开关抖动",
    "807": "开发依赖",
    "808": "开箱即用的开发库，无需编写基础复杂的数学问题",
    "809": "引入 index.js",
    "810": "引入优化以减小二进制文件的大小和提高性能",
    "811": "引用 Model 的内存缓冲区的一片区域，提高内存效率",
    "812": "张量",
    "813": "张量(Tensor)",
    "814": "张量形状是 (image_height, image_width, color_channels)",
    "815": "归一化操作",
    "816": "当压力撤销时电路恢复",
    "817": "当压力施压时电路接通",
    "818": "形状是 (224,224, 3)",
    "819": "微控制器(MCU)支持",
    "820": "微调",
    "821": "微调过程",
    "822": "必须在开机前先装上去，系统才能识别",
    "823": "快速启动",
    "824": "快速的启动并运行一组深度学习推理演示",
    "825": "忽略由于开关抖动引起的小于",
    "826": "恢复训练",
    "827": "成功配置好 CUDA",
    "828": "手写数字识别",
    "829": "手势识别数据集",
    "830": "手势识别项目",
    "831": "打乱数据顺序，创建特征向量和标签向量，转换为张量格式，进行归一化操作",
    "832": "打开现有 Android Studio 项目",
    "833": "打开项目图标",
    "834": "执行TensorFlow Lite模型的推理",
    "835": "执行一个函数并清除所有创建的中间张量",
    "836": "执行推理",
    "837": "执行最终的分类",
    "838": "执行模型推理",
    "839": "执行模型文件在输入数据上定义的运算符，输出推理结果",
    "840": "执行模型转换过程",
    "841": "批次大小",
    "842": "指定从哪个层开始进行微调",
    "843": "指定含有 TensorFlow 1.x 或者 2.0 使用 SavedModel 生成文件的绝对路径目录",
    "844": "指定含有 TensorFlow 1.x 或者 2.0 使用 tf.keras model 生成 HDF5 文件的绝对路径目录",
    "845": "指定引脚编号系统",
    "846": "指定输出文件的绝对路径",
    "847": "损失函数",
    "848": "接受 TFLite 模型",
    "849": "控制GPIO引脚",
    "850": "控制LED灯的亮暗",
    "851": "控制外部硬件设备",
    "852": "控制树莓派的GPIO",
    "853": "推理过程",
    "854": "描述人体各个部位的Haar特征值",
    "855": "描述构建和运行示例所需的依赖项",
    "856": "提供 5V⎓2A 的高品质电源为开发者套件供电",
    "857": "提供DOM根并调用JS脚本",
    "858": "提供了一个简单的 API，用于从 Java、Swift、Objective-C、C++ 和 Python 运行 TensorFlow Lite 模型",
    "859": "提供了一些转换工具压缩模型，进行算子融合并生成代码",
    "860": "提供了大量方便的工具，例如权重初始化，模型序列化，训练监测，可迁移性和安全检查",
    "861": "提供低级的机器学习构建模块和高级的类似Keras的API",
    "862": "提供多种语言的 API",
    "863": "提供开箱即用的预训练模型",
    "864": "提供经过充分认证的模型",
    "865": "提供训练好的模型，开发人员可以复用这些已经训练好且经过充分认证的模型",
    "866": "提取图像特征",
    "867": "提问或分享项目",
    "868": "提高性能的方法是训练预训练模型的顶层的权重以及刚添加的分类器的训练",
    "869": "提高模型性能",
    "870": "搭建本地服务器解决跨域问题",
    "871": "摄像头预捕获的图像宽度、高度、窗口显示的图像宽度、高度、捕获帧率、是否旋转图像",
    "872": "支持 GPU 硬件加速",
    "873": "支持BCM编号模式和物理引脚Broad编号模式",
    "874": "支持像素缩放和数据增强",
    "875": "支持图像大小调整和批次划分",
    "876": "支持大规模的模型训练和各种环境的部署",
    "877": "支持将文件映射到内存中，然后直接进行读取和解释，不需要额外解析",
    "878": "支持自定义输入形状和是否包含顶层分类器",
    "879": "支持设备端机器学习推断",
    "880": "支持量化",
    "881": "支持预装驱动程序的RPi相机",
    "882": "敏捷开发",
    "883": "教育",
    "884": "数据规范化和转换为张量类型",
    "885": "数据转换",
    "886": "数据采集、模型训练、参数调整、模型文件生成、网页端部署、摄像头检测",
    "887": "数据集",
    "888": "数据预处理",
    "889": "文字处理",
    "890": "无需使用ByteBuffer来处理图像，提供了方便的支持库来简化图像预处理",
    "891": "易于设置和使用",
    "892": "是一个层次的结构",
    "893": "显示每个类别的准确度",
    "894": "显示混淆矩阵",
    "895": "普通GPIO口",
    "896": "更换国内源如阿里、清华",
    "897": "更新可视化元素",
    "898": "更注重考虑实时性，内存高效",
    "899": "更谨慎地控制内存何时回收",
    "900": "更适合于边缘设备部署",
    "901": "最大池化层",
    "902": "有130个左右",
    "903": "有助于避免因错误的样本而改向错误的方向",
    "904": "有效的物品检测方法",
    "905": "有经验的开发者",
    "906": "服务器和移动端的部署",
    "907": "未满足的对等依赖 seedrandom@~",
    "908": "机器学习和计算机视觉应用",
    "909": "权重值和计算",
    "910": "构建CNN模型",
    "911": "构建Tensorflow.js模型来识别手写数字",
    "912": "构建和运行mnist代码",
    "913": "构建和运行机器学习模型",
    "914": "构建小型移动机器人、人脸签到打卡、口罩识别、智能门锁、智能音箱等复杂 AI 系统",
    "915": "构建工具",
    "916": "构建神经网络",
    "917": "构成检测目标的相邻矩形的最小个数(默认为3个)",
    "918": "查看开发板系统信息",
    "919": "查看树莓派的GPIO引脚信息",
    "920": "标准算子",
    "921": "树莓派",
    "922": "树莓派4B的18号引脚",
    "923": "树莓派GPIO",
    "924": "树莓派接口",
    "925": "树莓派的21号引脚",
    "926": "树莓派的官方编程语言",
    "927": "树莓派系统",
    "928": "树莓派通用输入/输出接口（GPIO）",
    "929": "核心运行时",
    "930": "格式修改、显示驱动程序协调和数据处理",
    "931": "格式化 microSD 卡",
    "932": "梯度更新",
    "933": "检测人脸",
    "934": "检测关键身体部位的位置",
    "935": "模型",
    "936": "模型优化",
    "937": "模型优化工具",
    "938": "模型优化工具包",
    "939": "模型可以跟 Python 等其他语言模型进行互转",
    "940": "模型执行流图",
    "941": "模型推理",
    "942": "模型文件和标签文件",
    "943": "模型文件已拷贝到 assets 目录",
    "944": "模型的计算节点",
    "945": "模型精度提高到98%",
    "946": "模型编译",
    "947": "模型训练",
    "948": "模型评估",
    "949": "模型转换",
    "950": "模型预测",
    "951": "比 CPU 执行更快的浮点矩阵运算",
    "952": "池化层",
    "953": "汽车油耗（MPG）",
    "954": "汽车的功率（Horsepower）",
    "955": "没有操作系统，只有内存",
    "956": "测试Python开发环境和查看当前Python版本",
    "957": "测试数据",
    "958": "测试的软件包、webpack或Babel",
    "959": "浏览器可以很好可视化机器训练过程",
    "960": "浏览器可调用设备的摄像头、麦克风等增加机器学习的应用场景",
    "961": "混淆矩阵",
    "962": "清华源",
    "963": "清理GPIO引脚的设置",
    "964": "清除张量或变量并释放其GPU内存",
    "965": "清除所有创建的中间张量并释放它们的GPU内存",
    "966": "激活->设置为输出状态->写入1点亮LED",
    "967": "激活函数",
    "968": "灵活的架构可以将模型部署到桌面、服务器或移动设备中的 CPU 或 GPU 上",
    "969": "点亮LED灯",
    "970": "爱奇艺",
    "971": "版本变化后 API 函数会改变",
    "972": "版本变化后API函数会改变",
    "973": "物体检测、人脸识别、图像分割等视觉任务",
    "974": "物理引脚Broad编号",
    "975": "特别为各种端侧设备优化的算子库",
    "976": "瓶颈层",
    "977": "生成 HDF5 文件的绝对路径目录",
    "978": "生成 SavedModel",
    "979": "生成SavedModel",
    "980": "生成一个批次一个批次的图片，以生成器的形式给模型训练",
    "981": "生成一个批次的图片，以生成器的形式给模型训练",
    "982": "生成批次的图片数据",
    "983": "生成配置文件",
    "984": "用于 5V 电源输入",
    "985": "用于处理数据",
    "986": "用于工具的配置中心",
    "987": "用于构建网页界面",
    "988": "用于特征提取",
    "989": "用于编写JavaScript代码",
    "990": "用于训练模型",
    "991": "用于预测",
    "992": "用作即插即用外围设备",
    "993": "用作启动设备和主存储器",
    "994": "用到的算子索引",
    "995": "用户在自己的工作台中使用 TensorFlow API 构造 TensorFlow 模型",
    "996": "用来转换 SavedModel 格式模型",
    "997": "用来转换 concrete functions",
    "998": "用来转换 tf.keras 模型",
    "999": "用来连接 DP 屏幕",
    "1000": "电信号从低电平到高电平或从高电平到低电平状态的改变",
    "1001": "电阻",
    "1002": "登录 Jetson Nano",
    "1003": "监督学习中展示多个类别是否有混淆",
    "1004": "直接串联3.3V电源会产生大电流",
    "1005": "直接更新树莓派系统",
    "1006": "直接部署或用于迁移学习",
    "1007": "直流桶式插座",
    "1008": "相机模块",
    "1009": "相比 Protocol Buffer 有更高的性能和更小的大小",
    "1010": "硬件加速代理(Hardware accelerator delegate)",
    "1011": "确认 CUDA 已经被正常安装",
    "1012": "神经元权重计算中的偏置量",
    "1013": "离线语音识别",
    "1014": "科沃斯扫地机器人",
    "1015": "移动应用中的OCR处理",
    "1016": "移动端及IoT设备端的深度学习技术",
    "1017": "移除原有模型中最后的神经网络层（分类到1000类）",
    "1018": "端侧机器学习",
    "1019": "第一个卷积层",
    "1020": "第二、三卷积层",
    "1021": "简化图像预处理和输出处理",
    "1022": "简单的线性回归的实验",
    "1023": "算子优化",
    "1024": "算子优化和常见的编译优化",
    "1025": "算子优化和常见的编译优化，比如算子融合、常数折叠或无用代码删除等",
    "1026": "算子实现",
    "1027": "算子库(Op kernels)",
    "1028": "算子库和共享的内存缓冲区",
    "1029": "算子融合、常数折叠、无用代码删除",
    "1030": "类型: bool. (default False) Enables the converter and flags used in TF 1.x instead of TF 2.x",
    "1031": "类型: string. Full path of the output file",
    "1032": "类型: string. Full path to the Keras H5 model file",
    "1033": "类型: string. Full path to the SavedModel directory",
    "1034": "系统升级",
    "1035": "线性堆叠layers的模型",
    "1036": "编程语言",
    "1037": "编译 OpenCV",
    "1038": "编译 Python 模块",
    "1039": "缩减模型的大小并提高其效率，同时最大限度地降低对准确率的影响",
    "1040": "缩短开发周期",
    "1041": "网易",
    "1042": "网络环境较差时可以考虑更换源",
    "1043": "能够利用各种硬件加速",
    "1044": "能够执行完整全面的反向传播",
    "1045": "自定制算子",
    "1046": "节省训练时间和计算资源",
    "1047": "花卉数据集中的图片",
    "1048": "花卉识别 app",
    "1049": "花卉识别模型",
    "1050": "获取一些非常有意思的项目",
    "1051": "获取图像数据、处理图像、调用模型、绘制关键点",
    "1052": "获取张量数据",
    "1053": "获取指向张量的指针",
    "1054": "获取更多的 Jetson 平台信息",
    "1055": "观测开关去抖效果",
    "1056": "视频中的AR效果",
    "1057": "解决JavaScript内存回收问题",
    "1058": "解释器和转换器",
    "1059": "解释输出",
    "1060": "计算已知人脸和未知人脸特征向量的距离，距离越小表示两张人脸为同一个人的可能性越大",
    "1061": "计算机视觉应用",
    "1062": "计算节点的输入和输出",
    "1063": "计算预测结果的概率分布",
    "1064": "让输入输出映射到0-1之间，保证后期更有效地训练",
    "1065": "训练分类器查看数千个图像及其标签",
    "1066": "训练后量化",
    "1067": "训练数据",
    "1068": "训练数据和测试数据",
    "1069": "训练数据集很大且类似于预训练模型的数据集",
    "1070": "训练时从数据集中的不同类中随机选出一定数量的图像",
    "1071": "训练期间将不更新预训练网络的权重，只在 MobileNet V2基础模型上训练了几层",
    "1072": "训练模型",
    "1073": "训练模型时使用",
    "1074": "训练集",
    "1075": "训练预训练模型的顶层权重",
    "1076": "记录训练日志",
    "1077": "记录训练过程中的指标和计算图",
    "1078": "许多 CPU 和硬件加速器实现提供 SIMD 指令功能，对量化特别有益",
    "1079": "设置 OpenCV 的内容、位置和方式",
    "1080": "设置 converter.optimizations=[tf.lite.Optimize.DEFAULT]",
    "1081": "设置 model.trainable = False",
    "1082": "设置GPIO引脚模式",
    "1083": "设置GPIO引脚编号模式",
    "1084": "设置model.trainable = False",
    "1085": "设置上拉电阻",
    "1086": "设置为32，表示一次采样32条训练数据",
    "1087": "设置为50，表示遍历所有样本50次",
    "1088": "设置为true，表示打乱数据集",
    "1089": "设置前100层为不可训练",
    "1090": "设置图像加载的特定大小",
    "1091": "设置在训练中基础模型的参数不会被新的训练修改",
    "1092": "设置在训练中基础模型的各项参数变量不会被新的训练修改数据",
    "1093": "设置引脚为输入或输出模式",
    "1094": "设置训练时随机选出的图像数量",
    "1095": "设置访问密码",
    "1096": "设置输入张量值",
    "1097": "访问密码可以为空但建议设置",
    "1098": "评估指标",
    "1099": "评估模型时使用以检查模型对新数据的泛化情况",
    "1100": "评估训练有素的模型的性能",
    "1101": "识别剪刀、石头、布手势",
    "1102": "识别图像里的空间模式，例如线条和物体局部",
    "1103": "识别照相机所拍摄的花卉",
    "1104": "识别花卉图片",
    "1105": "识别输入图像",
    "1106": "请注意版本",
    "1107": "读取传感器数据，控制 LED 等外部设备",
    "1108": "读取传感器数据，控制外部设备",
    "1109": "读取输出张量值",
    "1110": "调整大小、裁剪、旋转和归一化图像",
    "1111": "调整数据集形状",
    "1112": "调整输入图像大小以匹配模型输入要求",
    "1113": "调用CSI摄像头和USB摄像头",
    "1114": "调用model.fit方法进行训练",
    "1115": "调用不同的硬件加速器比如 GPU 进行执行",
    "1116": "调用解释器的方式：try (Interpreter interpreter = new Interpreter(tensorflow_lite_model_file)) { interpreter.run(input, output); }",
    "1117": "负极连接到GND",
    "1118": "资源有限，不能训练网络，可能出现OOM错误",
    "1119": "跨平台运行，包括 Android、iOS 以及基于 Linux 的 IoT 设备和微控制器",
    "1120": "转换 SavedModel 格式模型",
    "1121": "转换 concrete functions",
    "1122": "转换 tf.keras 模型",
    "1123": "转换和运行 TensorFlow 模型所需的工具",
    "1124": "转换数据",
    "1125": "转换模型",
    "1126": "软件PWM库",
    "1127": "软件源配置文件",
    "1128": "软链接",
    "1129": "轻量、快速、兼容度高",
    "1130": "轻量级",
    "1131": "轻量级、快速启动、内存高效",
    "1132": "输入层",
    "1133": "输入输出用到的 Tensor 索引",
    "1134": "输入（Xs）和标签（Ys）",
    "1135": "输出宽度和高度会收缩",
    "1136": "输出层",
    "1137": "输出是一个三维的张量，其形状描述了 (height, width, channels)",
    "1138": "输出模式",
    "1139": "输出电压约为3.3V",
    "1140": "输出的通道数量取决于声明层时的 filters 参数",
    "1141": "输出的通道数量取决于声明层时的filters参数",
    "1142": "输出通道数为32",
    "1143": "输出通道数为64",
    "1144": "边缘",
    "1145": "边缘操作",
    "1146": "迁移学习",
    "1147": "运行 TensorFlow Lite 模型",
    "1148": "运行Sync Gradle",
    "1149": "运行TFLite推理并获取输出概率",
    "1150": "运行各种深度学习模型",
    "1151": "运行在 Node.js 或浏览器环境中",
    "1152": "运行已有的 Python 版 TensorFlow 模型",
    "1153": "运行服务监听的IP地址、端口、notebooks内核的目录、是否打开浏览器等配置项",
    "1154": "运行模型推理",
    "1155": "返回图像中每张人脸的128维人脸编码",
    "1156": "返回张量数据的副本",
    "1157": "返回的是数据的副本而非引用",
    "1158": "进行 VNC 连接",
    "1159": "进行内存清理工作，防止内存泄露",
    "1160": "进行大量张量操作时使用可能会很麻烦",
    "1161": "进行迁移学习，实现识别花卉模型",
    "1162": "连接LED灯的正极",
    "1163": "连接按键的一个引脚",
    "1164": "连接显示器、键盘和鼠标",
    "1165": "适用于以 Swift 和 Objective-C 编写的原生 iOS 库",
    "1166": "适用于多个平台",
    "1167": "适用于多个平台，提供了一个简单的 API",
    "1168": "适用于移动和嵌入式设备的轻量级推理框架",
    "1169": "选择模型",
    "1170": "选择模型、转换模型、部署到设备、优化模型",
    "1171": "选择模型、转换模型、部署到设备和优化模型",
    "1172": "逐步加载单个数据集的图像",
    "1173": "逐步加载数据集图像",
    "1174": "通过 SSH 或 VNC 服务远程访问",
    "1175": "通过 pip 安装",
    "1176": "通过 script 标签引入 index.js",
    "1177": "通过GPIO控制展示基础硬件控制能力",
    "1178": "通过tf.model()来创建LayersModel",
    "1179": "通过利用 WebGL 在 GPU 上执行计算大幅提高速度",
    "1180": "通过命令行转换模型",
    "1181": "通过浏览器访问",
    "1182": "通过脚本标签（script tags）或从 yarn（或者 NPM）安装并使用 Parcel，WebPack 或 Rollup 等工具构建工程",
    "1183": "通过计算每个滑动窗口的最大值来缩减卷积结果的大小",
    "1184": "通过许多正负样例中训练得到cascade方程，然后将其应用于其他图片",
    "1185": "通过跳线线连接到其他电路板或设备",
    "1186": "通过迁移学习实现花卉识别模型",
    "1187": "通过限流电阻串联到GPIO21",
    "1188": "郁金香(tulips)、玫瑰(roses)、浦公英(dandelion)、向日葵(sunflowers)、雏菊(daisy)",
    "1189": "部署TensorFlow Lite模型",
    "1190": "部署到设备",
    "1191": "部署在 Raspberry Pi 和基于 Arm64 的主板的嵌入式 Linux 系统上",
    "1192": "配置Jupyter lab的运行参数",
    "1193": "配置proxy或使用国内镜像",
    "1194": "配置文件",
    "1195": "配置模型的优化器和损失函数",
    "1196": "配置项目依赖",
    "1197": "采用 FlatBuffers 格式，支持将文件映射到内存中，然后直接进行读取和解释",
    "1198": "采用更小的模型格式",
    "1199": "释放张量占用的GPU内存",
    "1200": "重启树莓派后启动",
    "1201": "量化",
    "1202": "针对移动设备做了很多优化",
    "1203": "错误的连接和编程可能会导致设备损坏或故障",
    "1204": "防止Android在生成应用程序二进制文件时压缩TensorFlow Lite模型文件",
    "1205": "防止应用程序中的内存泄漏",
    "1206": "阻塞函数，会阻塞程序执行直到检测到一个边沿",
    "1207": "降低卷积层对位置的敏感",
    "1208": "降低存储器访问成本",
    "1209": "降低权重的精确表示，并且可选的降低存储和计算的激活值",
    "1210": "降低权重的精确表示，降低存储和计算的激活值",
    "1211": "降低用于读取和存储中间激活值的存储器访问成本",
    "1212": "随机打乱数据集以避免模型预测受图像顺序影响",
    "1213": "需先训练顶层分类器并设置预训练模型为不可训练",
    "1214": "需要root权限",
    "1215": "需要使用GStreamer读取视频流",
    "1216": "需要大约两个半小时",
    "1217": "需要更多灵活性和控制时使用",
    "1218": "需要更多的内存",
    "1219": "需要确认版本",
    "1220": "需要高准确率",
    "1221": "静态图片分辨率为3280 × 2464",
    "1222": "面包板、杜邦线公对母、LED灯、330欧姆电阻",
    "1223": "面部，眼睛，微笑等",
    "1224": "项目的清单文件",
    "1225": "预加载了ImageNet训练权重的深度学习模型",
    "1226": "预处理模型输入和后处理模型输出",
    "1227": "预处理输入图像",
    "1228": "预测",
    "1229": "预测汽车油耗效率",
    "1230": "预测汽车油耗（MPG）",
    "1231": "预测汽车的油耗效率 MPG",
    "1232": "预训练模型和全连接的分类器",
    "1233": "预训练模型将忘记它学到的东西",
    "1234": "验证集",
    "1235": "高层学习针对特定数据集的专用特征",
    "1236": "高性能",
    "1237": "高阶 tf.keras.* API（Keras 模型）或低阶 tf.* API（用于生成具体函数）",
    "1238": "默认包含1000类的分类层",
    "1239": "默认安装 JetPack 安装了对应的 OpenCV 不支持 CUDA 且版本是固定搭配的"
  }
}